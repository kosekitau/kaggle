{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45b2c75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:09:56.344216Z",
     "iopub.status.busy": "2023-08-10T15:09:56.343471Z",
     "iopub.status.idle": "2023-08-10T15:10:28.437341Z",
     "shell.execute_reply": "2023-08-10T15:10:28.435983Z"
    },
    "papermill": {
     "duration": 32.112961,
     "end_time": "2023-08-10T15:10:28.440123",
     "exception": false,
     "start_time": "2023-08-10T15:09:56.327162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q /kaggle/input/tabpfn-019-whl/tabpfn-0.1.9-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61815ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:28.468466Z",
     "iopub.status.busy": "2023-08-10T15:10:28.468132Z",
     "iopub.status.idle": "2023-08-10T15:10:31.465793Z",
     "shell.execute_reply": "2023-08-10T15:10:31.464321Z"
    },
    "papermill": {
     "duration": 3.014813,
     "end_time": "2023-08-10T15:10:31.468660",
     "exception": false,
     "start_time": "2023-08-10T15:10:28.453847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n",
    "!cp /kaggle/input/tabpfn-019-whl/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b2feaa",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:31.497405Z",
     "iopub.status.busy": "2023-08-10T15:10:31.497064Z",
     "iopub.status.idle": "2023-08-10T15:10:38.121577Z",
     "shell.execute_reply": "2023-08-10T15:10:38.120598Z"
    },
    "papermill": {
     "duration": 6.641595,
     "end_time": "2023-08-10T15:10:38.124035",
     "exception": false,
     "start_time": "2023-08-10T15:10:31.482440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timedelta, date\n",
    "import json\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, train_test_split, RepeatedStratifiedKFold, LeaveOneOut, StratifiedKFold\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f842cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:38.152622Z",
     "iopub.status.busy": "2023-08-10T15:10:38.151934Z",
     "iopub.status.idle": "2023-08-10T15:10:38.194858Z",
     "shell.execute_reply": "2023-08-10T15:10:38.193983Z"
    },
    "papermill": {
     "duration": 0.05934,
     "end_time": "2023-08-10T15:10:38.196925",
     "exception": false,
     "start_time": "2023-08-10T15:10:38.137585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n",
    "df_sub = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "df_greeks = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/greeks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b44d3bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:38.224878Z",
     "iopub.status.busy": "2023-08-10T15:10:38.224610Z",
     "iopub.status.idle": "2023-08-10T15:10:38.230904Z",
     "shell.execute_reply": "2023-08-10T15:10:38.230031Z"
    },
    "papermill": {
     "duration": 0.022928,
     "end_time": "2023-08-10T15:10:38.232937",
     "exception": false,
     "start_time": "2023-08-10T15:10:38.210009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance_logloss(y_true, y_pred):\n",
    "    y_pred = np.stack([1-y_pred, y_pred]).T\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "    #y_pred / np.sum(y_pred, axis=1)[:, None]\n",
    "    nc = np.bincount(y_true)\n",
    "    logloss = (-1/nc[0]*(np.sum(np.where(y_true==0,1,0) * np.log(y_pred[:,0]))) - 1/nc[1]*(np.sum(np.where(y_true!=0,1,0) * np.log(y_pred[:,1])))) / 2\n",
    "\n",
    "    return logloss\n",
    "\n",
    "metric = balance_logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd4a460a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:38.260660Z",
     "iopub.status.busy": "2023-08-10T15:10:38.259867Z",
     "iopub.status.idle": "2023-08-10T15:10:38.265315Z",
     "shell.execute_reply": "2023-08-10T15:10:38.264415Z"
    },
    "papermill": {
     "duration": 0.02137,
     "end_time": "2023-08-10T15:10:38.267311",
     "exception": false,
     "start_time": "2023-08-10T15:10:38.245941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_state = 13\n",
    "learning_rate = 0.1\n",
    "\n",
    "base_params_dict = {\n",
    "    \"LightGBM\" : {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"random_state\": random_state,\n",
    "        \"verbose\": -1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        },\n",
    "    \"XGBoost\" : {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"random_state\": random_state,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        'eval_metric': 'logloss',\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33792c",
   "metadata": {
    "papermill": {
     "duration": 0.0129,
     "end_time": "2023-08-10T15:10:38.293258",
     "exception": false,
     "start_time": "2023-08-10T15:10:38.280358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LightGBM + HardPointCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aea7968e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:38.320187Z",
     "iopub.status.busy": "2023-08-10T15:10:38.319928Z",
     "iopub.status.idle": "2023-08-10T15:10:38.385963Z",
     "shell.execute_reply": "2023-08-10T15:10:38.384906Z"
    },
    "papermill": {
     "duration": 0.083864,
     "end_time": "2023-08-10T15:10:38.390051",
     "exception": false,
     "start_time": "2023-08-10T15:10:38.306187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>BN</th>\n",
       "      <th>BP</th>\n",
       "      <th>BQ</th>\n",
       "      <th>BR</th>\n",
       "      <th>BZ</th>\n",
       "      <th>CB</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>CF</th>\n",
       "      <th>CH</th>\n",
       "      <th>CL</th>\n",
       "      <th>CR</th>\n",
       "      <th>CS</th>\n",
       "      <th>CU</th>\n",
       "      <th>CW</th>\n",
       "      <th>DA</th>\n",
       "      <th>DE</th>\n",
       "      <th>DF</th>\n",
       "      <th>DH</th>\n",
       "      <th>DI</th>\n",
       "      <th>DL</th>\n",
       "      <th>DN</th>\n",
       "      <th>DU</th>\n",
       "      <th>DV</th>\n",
       "      <th>DY</th>\n",
       "      <th>EB</th>\n",
       "      <th>EE</th>\n",
       "      <th>EG</th>\n",
       "      <th>EH</th>\n",
       "      <th>EJ</th>\n",
       "      <th>EL</th>\n",
       "      <th>EP</th>\n",
       "      <th>EU</th>\n",
       "      <th>FC</th>\n",
       "      <th>FD</th>\n",
       "      <th>FE</th>\n",
       "      <th>FI</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>Class</th>\n",
       "      <th>HardPoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000ff2bfdfe9</td>\n",
       "      <td>0.209377</td>\n",
       "      <td>3109.03329</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.812214</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>4126.58731</td>\n",
       "      <td>22.5984</td>\n",
       "      <td>175.638726</td>\n",
       "      <td>152.707705</td>\n",
       "      <td>823.928241</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>47.223358</td>\n",
       "      <td>0.563481</td>\n",
       "      <td>23.387600</td>\n",
       "      <td>4.851915</td>\n",
       "      <td>0.023482</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.069225</td>\n",
       "      <td>13.784111</td>\n",
       "      <td>1.302012</td>\n",
       "      <td>36.205956</td>\n",
       "      <td>69.08340</td>\n",
       "      <td>295.570575</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.284232</td>\n",
       "      <td>89.245560</td>\n",
       "      <td>84.31664</td>\n",
       "      <td>29.657104</td>\n",
       "      <td>5.310690</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>23.187704</td>\n",
       "      <td>7.294176</td>\n",
       "      <td>1.987283</td>\n",
       "      <td>1433.166750</td>\n",
       "      <td>0.949104</td>\n",
       "      <td>1</td>\n",
       "      <td>30.879420</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>3.828384</td>\n",
       "      <td>13.394640</td>\n",
       "      <td>10.265073</td>\n",
       "      <td>9028.291921</td>\n",
       "      <td>3.583450</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>11.339138</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>22.136229</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007255e47698</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>978.76416</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.632190</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>13.517790</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5496.92824</td>\n",
       "      <td>19.4205</td>\n",
       "      <td>155.868030</td>\n",
       "      <td>14.754720</td>\n",
       "      <td>51.216883</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>30.284345</td>\n",
       "      <td>0.484710</td>\n",
       "      <td>50.628208</td>\n",
       "      <td>6.085041</td>\n",
       "      <td>0.031442</td>\n",
       "      <td>1.113875</td>\n",
       "      <td>1.117800</td>\n",
       "      <td>28.310953</td>\n",
       "      <td>1.357182</td>\n",
       "      <td>37.476568</td>\n",
       "      <td>70.79836</td>\n",
       "      <td>178.553100</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.363489</td>\n",
       "      <td>110.581815</td>\n",
       "      <td>75.74548</td>\n",
       "      <td>37.532000</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>17.222328</td>\n",
       "      <td>4.926396</td>\n",
       "      <td>0.858603</td>\n",
       "      <td>1111.287150</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>95.415086</td>\n",
       "      <td>52.260480</td>\n",
       "      <td>17.175984</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>6785.003474</td>\n",
       "      <td>10.358927</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>9.292698</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>29.135430</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>013f2bd269f5</td>\n",
       "      <td>0.470030</td>\n",
       "      <td>2635.10654</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>6.732840</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.824570</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5135.78024</td>\n",
       "      <td>26.4825</td>\n",
       "      <td>128.988531</td>\n",
       "      <td>219.320160</td>\n",
       "      <td>482.141594</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>32.563713</td>\n",
       "      <td>0.495852</td>\n",
       "      <td>85.955376</td>\n",
       "      <td>5.376488</td>\n",
       "      <td>0.036218</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.700350</td>\n",
       "      <td>39.364743</td>\n",
       "      <td>1.009611</td>\n",
       "      <td>21.459644</td>\n",
       "      <td>70.81970</td>\n",
       "      <td>321.426625</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.210441</td>\n",
       "      <td>120.056438</td>\n",
       "      <td>65.46984</td>\n",
       "      <td>28.053464</td>\n",
       "      <td>1.289739</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>36.861352</td>\n",
       "      <td>7.813674</td>\n",
       "      <td>8.146651</td>\n",
       "      <td>1494.076488</td>\n",
       "      <td>0.377208</td>\n",
       "      <td>1</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>5.390628</td>\n",
       "      <td>224.207424</td>\n",
       "      <td>8.745201</td>\n",
       "      <td>8338.906181</td>\n",
       "      <td>11.626917</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>37.077772</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>28.022851</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>043ac50845d5</td>\n",
       "      <td>0.252107</td>\n",
       "      <td>3819.65177</td>\n",
       "      <td>120.201618</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.685344</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>11.053708</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4169.67738</td>\n",
       "      <td>23.6577</td>\n",
       "      <td>237.282264</td>\n",
       "      <td>11.050410</td>\n",
       "      <td>661.518640</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>15.201914</td>\n",
       "      <td>0.717882</td>\n",
       "      <td>88.159360</td>\n",
       "      <td>2.347652</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>1.400300</td>\n",
       "      <td>0.636075</td>\n",
       "      <td>41.116960</td>\n",
       "      <td>0.722727</td>\n",
       "      <td>21.530392</td>\n",
       "      <td>47.27586</td>\n",
       "      <td>196.607985</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.292431</td>\n",
       "      <td>139.824570</td>\n",
       "      <td>71.57120</td>\n",
       "      <td>24.354856</td>\n",
       "      <td>2.655345</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>52.003884</td>\n",
       "      <td>7.386060</td>\n",
       "      <td>3.813326</td>\n",
       "      <td>15691.552180</td>\n",
       "      <td>0.614484</td>\n",
       "      <td>1</td>\n",
       "      <td>31.674357</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>31.323372</td>\n",
       "      <td>59.301984</td>\n",
       "      <td>7.884336</td>\n",
       "      <td>10965.766040</td>\n",
       "      <td>14.852022</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>18.529584</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>39.948656</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>044fb8a146ec</td>\n",
       "      <td>0.380297</td>\n",
       "      <td>3733.04844</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.942255</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>5728.73412</td>\n",
       "      <td>24.0108</td>\n",
       "      <td>324.546318</td>\n",
       "      <td>149.717165</td>\n",
       "      <td>6074.859475</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>82.213495</td>\n",
       "      <td>0.536467</td>\n",
       "      <td>72.644264</td>\n",
       "      <td>30.537722</td>\n",
       "      <td>0.025472</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.693150</td>\n",
       "      <td>31.724726</td>\n",
       "      <td>0.827550</td>\n",
       "      <td>34.415360</td>\n",
       "      <td>74.06532</td>\n",
       "      <td>200.178160</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.207708</td>\n",
       "      <td>97.920120</td>\n",
       "      <td>52.83888</td>\n",
       "      <td>26.019912</td>\n",
       "      <td>1.144902</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>9.064856</td>\n",
       "      <td>7.350720</td>\n",
       "      <td>3.490846</td>\n",
       "      <td>1403.656300</td>\n",
       "      <td>0.164268</td>\n",
       "      <td>1</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>91.994825</td>\n",
       "      <td>51.141336</td>\n",
       "      <td>29.102640</td>\n",
       "      <td>4.274640</td>\n",
       "      <td>16198.049590</td>\n",
       "      <td>13.666727</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>16.408728</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>45.381316</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id        AB          AF          AH         AM        AR  \\\n",
       "0  000ff2bfdfe9  0.209377  3109.03329   85.200147  22.394407  8.138688   \n",
       "1  007255e47698  0.145282   978.76416   85.200147  36.968889  8.138688   \n",
       "2  013f2bd269f5  0.470030  2635.10654   85.200147  32.360553  8.138688   \n",
       "3  043ac50845d5  0.252107  3819.65177  120.201618  77.112203  8.138688   \n",
       "4  044fb8a146ec  0.380297  3733.04844   85.200147  14.103738  8.138688   \n",
       "\n",
       "         AX        AY         AZ          BC         BD        BN          BP  \\\n",
       "0  0.699861  0.025578   9.812214    5.555634  4126.58731  22.5984  175.638726   \n",
       "1  3.632190  0.025578  13.517790    1.229900  5496.92824  19.4205  155.868030   \n",
       "2  6.732840  0.025578  12.824570    1.229900  5135.78024  26.4825  128.988531   \n",
       "3  3.685344  0.025578  11.053708    1.229900  4169.67738  23.6577  237.282264   \n",
       "4  3.942255  0.054810   3.396778  102.151980  5728.73412  24.0108  324.546318   \n",
       "\n",
       "           BQ           BR          BZ         CB        CC        CD   \\\n",
       "0  152.707705   823.928241  257.432377  47.223358  0.563481  23.387600   \n",
       "1   14.754720    51.216883  257.432377  30.284345  0.484710  50.628208   \n",
       "2  219.320160   482.141594  257.432377  32.563713  0.495852  85.955376   \n",
       "3   11.050410   661.518640  257.432377  15.201914  0.717882  88.159360   \n",
       "4  149.717165  6074.859475  257.432377  82.213495  0.536467  72.644264   \n",
       "\n",
       "          CF        CH        CL        CR         CS        CU        CW   \\\n",
       "0   4.851915  0.023482  1.050225  0.069225  13.784111  1.302012  36.205956   \n",
       "1   6.085041  0.031442  1.113875  1.117800  28.310953  1.357182  37.476568   \n",
       "2   5.376488  0.036218  1.050225  0.700350  39.364743  1.009611  21.459644   \n",
       "3   2.347652  0.029054  1.400300  0.636075  41.116960  0.722727  21.530392   \n",
       "4  30.537722  0.025472  1.050225  0.693150  31.724726  0.827550  34.415360   \n",
       "\n",
       "         DA          DE       DF        DH          DI        DL         DN  \\\n",
       "0  69.08340  295.570575  0.23868  0.284232   89.245560  84.31664  29.657104   \n",
       "1  70.79836  178.553100  0.23868  0.363489  110.581815  75.74548  37.532000   \n",
       "2  70.81970  321.426625  0.23868  0.210441  120.056438  65.46984  28.053464   \n",
       "3  47.27586  196.607985  0.23868  0.292431  139.824570  71.57120  24.354856   \n",
       "4  74.06532  200.178160  0.23868  0.207708   97.920120  52.83888  26.019912   \n",
       "\n",
       "         DU       DV         DY        EB        EE            EG        EH  \\\n",
       "0  5.310690  1.74307  23.187704  7.294176  1.987283   1433.166750  0.949104   \n",
       "1  0.005518  1.74307  17.222328  4.926396  0.858603   1111.287150  0.003042   \n",
       "2  1.289739  1.74307  36.861352  7.813674  8.146651   1494.076488  0.377208   \n",
       "3  2.655345  1.74307  52.003884  7.386060  3.813326  15691.552180  0.614484   \n",
       "4  1.144902  1.74307   9.064856  7.350720  3.490846   1403.656300  0.164268   \n",
       "\n",
       "   EJ          EL         EP         EU          FC        FD             FE  \\\n",
       "0   1   30.879420  78.526968   3.828384   13.394640  10.265073   9028.291921   \n",
       "1   0  109.125159  95.415086  52.260480   17.175984   0.296850   6785.003474   \n",
       "2   1  109.125159  78.526968   5.390628  224.207424   8.745201   8338.906181   \n",
       "3   1   31.674357  78.526968  31.323372   59.301984   7.884336  10965.766040   \n",
       "4   1  109.125159  91.994825  51.141336   29.102640   4.274640  16198.049590   \n",
       "\n",
       "          FI        FL        FR        FS         GB          GE  \\\n",
       "0   3.583450  7.298162   1.73855  0.094822  11.339138   72.611063   \n",
       "1  10.358927  0.173229   0.49706  0.568932   9.292698   72.611063   \n",
       "2  11.626917  7.709560   0.97556  1.198821  37.077772   88.609437   \n",
       "3  14.852022  6.122162   0.49706  0.284466  18.529584   82.416803   \n",
       "4  13.666727  8.153058  48.50134  0.121914  16.408728  146.109943   \n",
       "\n",
       "             GF         GH         GI         GL  Class  HardPoint  \n",
       "0   2003.810319  22.136229  69.834944   0.120343      1          0  \n",
       "1  27981.562750  29.135430  32.131996  21.978000      0          0  \n",
       "2  13676.957810  28.022851  35.192676   0.196941      0          0  \n",
       "3   2094.262452  39.948656  90.493248   0.155829      0          0  \n",
       "4   8524.370502  45.381316  36.262628   0.096614      1          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN', 'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS', 'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY', 'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI', 'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL']\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "target = \"Class\"\n",
    "n_splits = 10\n",
    "n_repeats = 10\n",
    "num_boost_round = 1000\n",
    "stopping_rounds = 50\n",
    "learning_rate = 0.1\n",
    "n_trials = 300\n",
    "model_name = \"LightGBM\"\n",
    "\n",
    "# FE\n",
    "df_train[\"EJ\"] = df_train[\"EJ\"].replace({'A':0, 'B':1})\n",
    "df_test[\"EJ\"] = df_test[\"EJ\"].replace({'A':0, 'B':1})\n",
    "\n",
    "# HardPoint\n",
    "hp = [509, 313, 479, 267, 408, 193, 145, 229, 31, 434]\n",
    "hps = [1 if i in hp else 0 for i in range(len(df_train))]\n",
    "df_train[\"HardPoint\"] = hps\n",
    "\n",
    "# EX\n",
    "ex_columns = ['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n",
    "       'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n",
    "       'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
    "       'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n",
    "       'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL'] # All\n",
    "\n",
    "exclude_columns = []\n",
    "\n",
    "ex_columns = sorted(list(set(ex_columns) - set(exclude_columns)))\n",
    "use_columns = ex_columns + [target]\n",
    "X_sub = df_test[ex_columns]\n",
    "\n",
    "display(df_train.head())\n",
    "\n",
    "print(ex_columns)\n",
    "print(len(ex_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8989ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:38.421119Z",
     "iopub.status.busy": "2023-08-10T15:10:38.420831Z",
     "iopub.status.idle": "2023-08-10T15:10:38.428710Z",
     "shell.execute_reply": "2023-08-10T15:10:38.427425Z"
    },
    "papermill": {
     "duration": 0.025446,
     "end_time": "2023-08-10T15:10:38.430781",
     "exception": false,
     "start_time": "2023-08-10T15:10:38.405335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary', 'metric': 'binary_logloss', 'random_state': 13, 'verbose': -1, 'learning_rate': 0.1, 'max_depth': 4, 'num_leaves': 353, 'min_data_in_leaf': 16, 'min_gain_to_split': 0.07285502530384627, 'max_bin': 63, 'subsample': 0.5572107119770833, 'subsample_freq': 0, 'feature_fraction': 0.49006914678241065, 'reg_alpha': 1.1978900147063292e-05, 'reg_lambda': 7.80448230256196e-06, 'scale_pos_weight': 27.352771232270825}\n"
     ]
    }
   ],
   "source": [
    "best_params = {'objective': 'binary', 'metric': 'binary_logloss', 'random_state': 13, 'verbose': -1, 'learning_rate': 0.1, 'max_depth': 4, 'num_leaves': 353, 'min_data_in_leaf': 16, 'min_gain_to_split': 0.07285502530384627, 'max_bin': 63, 'subsample': 0.5572107119770833, 'subsample_freq': 0, 'feature_fraction': 0.49006914678241065, 'reg_alpha': 1.1978900147063292e-05, 'reg_lambda': 7.80448230256196e-06, 'scale_pos_weight': 27.352771232270825}\n",
    "params = {**base_params_dict[model_name], **best_params}\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b09bb56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:38.460354Z",
     "iopub.status.busy": "2023-08-10T15:10:38.459599Z",
     "iopub.status.idle": "2023-08-10T15:10:48.948263Z",
     "shell.execute_reply": "2023-08-10T15:10:48.947250Z"
    },
    "papermill": {
     "duration": 10.505872,
     "end_time": "2023-08-10T15:10:48.950592",
     "exception": false,
     "start_time": "2023-08-10T15:10:38.444720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== START FOLD 1 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[163]\ttraining's binary_logloss: 0.00267841\tvalid_1's binary_logloss: 0.109056\n",
      "==================== START FOLD 2 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[167]\ttraining's binary_logloss: 0.00234184\tvalid_1's binary_logloss: 0.110322\n",
      "==================== START FOLD 3 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[144]\ttraining's binary_logloss: 0.00391154\tvalid_1's binary_logloss: 0.120555\n",
      "==================== START FOLD 4 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[108]\ttraining's binary_logloss: 0.00959849\tvalid_1's binary_logloss: 0.231031\n",
      "==================== START FOLD 5 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[86]\ttraining's binary_logloss: 0.0277295\tvalid_1's binary_logloss: 0.127411\n",
      "==================== START FOLD 6 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[105]\ttraining's binary_logloss: 0.0154653\tvalid_1's binary_logloss: 0.246664\n",
      "==================== START FOLD 7 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[148]\ttraining's binary_logloss: 0.0032649\tvalid_1's binary_logloss: 0.0856257\n",
      "==================== START FOLD 8 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's binary_logloss: 0.00283794\tvalid_1's binary_logloss: 0.11298\n",
      "==================== START FOLD 9 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[150]\ttraining's binary_logloss: 0.00397233\tvalid_1's binary_logloss: 0.170792\n",
      "==================== START FOLD 10 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[140]\ttraining's binary_logloss: 0.00433108\tvalid_1's binary_logloss: 0.043649\n",
      "==================== START FOLD 11 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's binary_logloss: 0.00641697\tvalid_1's binary_logloss: 0.226956\n",
      "==================== START FOLD 12 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's binary_logloss: 0.277035\tvalid_1's binary_logloss: 0.417109\n",
      "==================== START FOLD 13 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[83]\ttraining's binary_logloss: 0.0280106\tvalid_1's binary_logloss: 0.298236\n",
      "==================== START FOLD 14 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\ttraining's binary_logloss: 0.00259778\tvalid_1's binary_logloss: 0.0527405\n",
      "==================== START FOLD 15 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[157]\ttraining's binary_logloss: 0.00287206\tvalid_1's binary_logloss: 0.149123\n",
      "==================== START FOLD 16 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[144]\ttraining's binary_logloss: 0.00391588\tvalid_1's binary_logloss: 0.0365381\n",
      "==================== START FOLD 17 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[142]\ttraining's binary_logloss: 0.0041809\tvalid_1's binary_logloss: 0.107529\n",
      "==================== START FOLD 18 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[172]\ttraining's binary_logloss: 0.00241453\tvalid_1's binary_logloss: 0.0671428\n",
      "==================== START FOLD 19 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[169]\ttraining's binary_logloss: 0.00228727\tvalid_1's binary_logloss: 0.11822\n",
      "==================== START FOLD 20 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[145]\ttraining's binary_logloss: 0.00364251\tvalid_1's binary_logloss: 0.123268\n",
      "==================== START FOLD 21 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's binary_logloss: 0.00218737\tvalid_1's binary_logloss: 0.0942107\n",
      "==================== START FOLD 22 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[147]\ttraining's binary_logloss: 0.00329701\tvalid_1's binary_logloss: 0.147903\n",
      "==================== START FOLD 23 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's binary_logloss: 0.00241122\tvalid_1's binary_logloss: 0.0908967\n",
      "==================== START FOLD 24 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[138]\ttraining's binary_logloss: 0.00452957\tvalid_1's binary_logloss: 0.163758\n",
      "==================== START FOLD 25 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[119]\ttraining's binary_logloss: 0.0102542\tvalid_1's binary_logloss: 0.244442\n",
      "==================== START FOLD 26 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[146]\ttraining's binary_logloss: 0.00372429\tvalid_1's binary_logloss: 0.177213\n",
      "==================== START FOLD 27 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\ttraining's binary_logloss: 0.00425532\tvalid_1's binary_logloss: 0.190214\n",
      "==================== START FOLD 28 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\ttraining's binary_logloss: 0.00251739\tvalid_1's binary_logloss: 0.0790823\n",
      "==================== START FOLD 29 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[83]\ttraining's binary_logloss: 0.0269258\tvalid_1's binary_logloss: 0.133537\n",
      "==================== START FOLD 30 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[144]\ttraining's binary_logloss: 0.00348201\tvalid_1's binary_logloss: 0.0802721\n",
      "==================== START FOLD 31 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[140]\ttraining's binary_logloss: 0.00447354\tvalid_1's binary_logloss: 0.117792\n",
      "==================== START FOLD 32 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[156]\ttraining's binary_logloss: 0.00287054\tvalid_1's binary_logloss: 0.0865804\n",
      "==================== START FOLD 33 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[167]\ttraining's binary_logloss: 0.00246347\tvalid_1's binary_logloss: 0.116058\n",
      "==================== START FOLD 34 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's binary_logloss: 0.0129985\tvalid_1's binary_logloss: 0.204436\n",
      "==================== START FOLD 35 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's binary_logloss: 0.00784816\tvalid_1's binary_logloss: 0.203085\n",
      "==================== START FOLD 36 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[167]\ttraining's binary_logloss: 0.00259887\tvalid_1's binary_logloss: 0.0515617\n",
      "==================== START FOLD 37 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[165]\ttraining's binary_logloss: 0.00236212\tvalid_1's binary_logloss: 0.100198\n",
      "==================== START FOLD 38 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[170]\ttraining's binary_logloss: 0.00217195\tvalid_1's binary_logloss: 0.0427366\n",
      "==================== START FOLD 39 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[162]\ttraining's binary_logloss: 0.00233702\tvalid_1's binary_logloss: 0.172599\n",
      "==================== START FOLD 40 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttraining's binary_logloss: 0.00936428\tvalid_1's binary_logloss: 0.194143\n",
      "==================== START FOLD 41 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[167]\ttraining's binary_logloss: 0.00231681\tvalid_1's binary_logloss: 0.0772401\n",
      "==================== START FOLD 42 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\ttraining's binary_logloss: 0.00435575\tvalid_1's binary_logloss: 0.118753\n",
      "==================== START FOLD 43 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[142]\ttraining's binary_logloss: 0.00422019\tvalid_1's binary_logloss: 0.0743544\n",
      "==================== START FOLD 44 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[134]\ttraining's binary_logloss: 0.00601111\tvalid_1's binary_logloss: 0.111822\n",
      "==================== START FOLD 45 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[150]\ttraining's binary_logloss: 0.0030772\tvalid_1's binary_logloss: 0.185345\n",
      "==================== START FOLD 46 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's binary_logloss: 0.00584647\tvalid_1's binary_logloss: 0.095958\n",
      "==================== START FOLD 47 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[119]\ttraining's binary_logloss: 0.00912132\tvalid_1's binary_logloss: 0.154416\n",
      "==================== START FOLD 48 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[102]\ttraining's binary_logloss: 0.0163792\tvalid_1's binary_logloss: 0.148733\n",
      "==================== START FOLD 49 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[170]\ttraining's binary_logloss: 0.00241006\tvalid_1's binary_logloss: 0.0974356\n",
      "==================== START FOLD 50 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[86]\ttraining's binary_logloss: 0.0257859\tvalid_1's binary_logloss: 0.237246\n",
      "==================== START FOLD 51 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[143]\ttraining's binary_logloss: 0.00400444\tvalid_1's binary_logloss: 0.129511\n",
      "==================== START FOLD 52 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[156]\ttraining's binary_logloss: 0.00231037\tvalid_1's binary_logloss: 0.124453\n",
      "==================== START FOLD 53 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttraining's binary_logloss: 0.00232605\tvalid_1's binary_logloss: 0.0525743\n",
      "==================== START FOLD 54 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[133]\ttraining's binary_logloss: 0.00463074\tvalid_1's binary_logloss: 0.246737\n",
      "==================== START FOLD 55 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's binary_logloss: 0.00833482\tvalid_1's binary_logloss: 0.089725\n",
      "==================== START FOLD 56 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[144]\ttraining's binary_logloss: 0.00503569\tvalid_1's binary_logloss: 0.0963321\n",
      "==================== START FOLD 57 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[139]\ttraining's binary_logloss: 0.00419086\tvalid_1's binary_logloss: 0.200681\n",
      "==================== START FOLD 58 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[152]\ttraining's binary_logloss: 0.00270614\tvalid_1's binary_logloss: 0.0996884\n",
      "==================== START FOLD 59 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's binary_logloss: 0.0192069\tvalid_1's binary_logloss: 0.136669\n",
      "==================== START FOLD 60 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[173]\ttraining's binary_logloss: 0.00231254\tvalid_1's binary_logloss: 0.136319\n",
      "==================== START FOLD 61 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[143]\ttraining's binary_logloss: 0.00400058\tvalid_1's binary_logloss: 0.0926589\n",
      "==================== START FOLD 62 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[73]\ttraining's binary_logloss: 0.0424322\tvalid_1's binary_logloss: 0.289174\n",
      "==================== START FOLD 63 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[177]\ttraining's binary_logloss: 0.00238802\tvalid_1's binary_logloss: 0.0580052\n",
      "==================== START FOLD 64 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[138]\ttraining's binary_logloss: 0.00471853\tvalid_1's binary_logloss: 0.187194\n",
      "==================== START FOLD 65 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's binary_logloss: 0.0119498\tvalid_1's binary_logloss: 0.0908058\n",
      "==================== START FOLD 66 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[160]\ttraining's binary_logloss: 0.00231177\tvalid_1's binary_logloss: 0.163853\n",
      "==================== START FOLD 67 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[164]\ttraining's binary_logloss: 0.00243149\tvalid_1's binary_logloss: 0.0886254\n",
      "==================== START FOLD 68 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[128]\ttraining's binary_logloss: 0.0075848\tvalid_1's binary_logloss: 0.0936275\n",
      "==================== START FOLD 69 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttraining's binary_logloss: 0.00229481\tvalid_1's binary_logloss: 0.100858\n",
      "==================== START FOLD 70 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\ttraining's binary_logloss: 0.00235225\tvalid_1's binary_logloss: 0.0849119\n",
      "==================== START FOLD 71 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[161]\ttraining's binary_logloss: 0.00270965\tvalid_1's binary_logloss: 0.124263\n",
      "==================== START FOLD 72 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[140]\ttraining's binary_logloss: 0.00447412\tvalid_1's binary_logloss: 0.136893\n",
      "==================== START FOLD 73 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[172]\ttraining's binary_logloss: 0.00239652\tvalid_1's binary_logloss: 0.0310642\n",
      "==================== START FOLD 74 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[114]\ttraining's binary_logloss: 0.0102156\tvalid_1's binary_logloss: 0.126492\n",
      "==================== START FOLD 75 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[115]\ttraining's binary_logloss: 0.011096\tvalid_1's binary_logloss: 0.141554\n",
      "==================== START FOLD 76 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttraining's binary_logloss: 0.00471888\tvalid_1's binary_logloss: 0.202188\n",
      "==================== START FOLD 77 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[80]\ttraining's binary_logloss: 0.0313528\tvalid_1's binary_logloss: 0.264482\n",
      "==================== START FOLD 78 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's binary_logloss: 0.00246717\tvalid_1's binary_logloss: 0.0791717\n",
      "==================== START FOLD 79 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[125]\ttraining's binary_logloss: 0.00702001\tvalid_1's binary_logloss: 0.142501\n",
      "==================== START FOLD 80 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[169]\ttraining's binary_logloss: 0.00240854\tvalid_1's binary_logloss: 0.0652304\n",
      "==================== START FOLD 81 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's binary_logloss: 0.00912214\tvalid_1's binary_logloss: 0.131305\n",
      "==================== START FOLD 82 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's binary_logloss: 0.00305553\tvalid_1's binary_logloss: 0.12468\n",
      "==================== START FOLD 83 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[163]\ttraining's binary_logloss: 0.00230705\tvalid_1's binary_logloss: 0.0846124\n",
      "==================== START FOLD 84 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[161]\ttraining's binary_logloss: 0.00253585\tvalid_1's binary_logloss: 0.0835104\n",
      "==================== START FOLD 85 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's binary_logloss: 0.00873547\tvalid_1's binary_logloss: 0.15005\n",
      "==================== START FOLD 86 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's binary_logloss: 0.381757\tvalid_1's binary_logloss: 0.407505\n",
      "==================== START FOLD 87 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttraining's binary_logloss: 0.0461546\tvalid_1's binary_logloss: 0.245946\n",
      "==================== START FOLD 88 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttraining's binary_logloss: 0.00243843\tvalid_1's binary_logloss: 0.0718036\n",
      "==================== START FOLD 89 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[148]\ttraining's binary_logloss: 0.00355924\tvalid_1's binary_logloss: 0.0945314\n",
      "==================== START FOLD 90 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's binary_logloss: 0.00803496\tvalid_1's binary_logloss: 0.0999955\n",
      "==================== START FOLD 91 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[118]\ttraining's binary_logloss: 0.00771728\tvalid_1's binary_logloss: 0.231052\n",
      "==================== START FOLD 92 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[154]\ttraining's binary_logloss: 0.00286074\tvalid_1's binary_logloss: 0.134557\n",
      "==================== START FOLD 93 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[145]\ttraining's binary_logloss: 0.00446181\tvalid_1's binary_logloss: 0.116784\n",
      "==================== START FOLD 94 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[127]\ttraining's binary_logloss: 0.00709657\tvalid_1's binary_logloss: 0.100978\n",
      "==================== START FOLD 95 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\ttraining's binary_logloss: 0.00491691\tvalid_1's binary_logloss: 0.105756\n",
      "==================== START FOLD 96 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[151]\ttraining's binary_logloss: 0.00266288\tvalid_1's binary_logloss: 0.108648\n",
      "==================== START FOLD 97 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[158]\ttraining's binary_logloss: 0.00271229\tvalid_1's binary_logloss: 0.134117\n",
      "==================== START FOLD 98 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[161]\ttraining's binary_logloss: 0.00313712\tvalid_1's binary_logloss: 0.110023\n",
      "==================== START FOLD 99 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[138]\ttraining's binary_logloss: 0.00458953\tvalid_1's binary_logloss: 0.0735927\n",
      "==================== START FOLD 100 ====================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttraining's binary_logloss: 0.0139458\tvalid_1's binary_logloss: 0.269979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/324956156.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  model1_preds[f\"pred_{i+1}\"] = pred\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(random_state)\n",
    "\n",
    "labels = df_train[[\"HardPoint\", \"Class\"]]\n",
    "cv = RepeatedMultilabelStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state+111)\n",
    "\n",
    "evals_results = []\n",
    "model1_preds = df_sub[[\"Id\"]].copy()\n",
    "scores = []\n",
    "\n",
    "y = df_train[target]\n",
    "X = df_train[ex_columns]\n",
    "model1_oof = np.zeros(len(y))\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv.split(X, labels)):\n",
    "    print(\"=\" * 20, f\"START FOLD {i+1}\", \"=\" * 20)\n",
    "    y_train = y.iloc[train_index].to_numpy()\n",
    "    X_train = X.iloc[train_index].to_numpy()\n",
    "    y_val = y.iloc[val_index].to_numpy()\n",
    "    X_val = X.iloc[val_index].to_numpy()\n",
    "\n",
    "    #########\n",
    "    # LightGBM\n",
    "    #########\n",
    "    if model_name==\"LightGBM\":\n",
    "        data_train = lgb.Dataset(X_train, y_train)\n",
    "        data_val = lgb.Dataset(X_val, y_val)\n",
    "        evals_result = {}\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_set=data_train,\n",
    "            valid_sets=[data_train, data_val],\n",
    "            num_boost_round=num_boost_round,\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n",
    "                lgb.record_evaluation(evals_result),\n",
    "            ],\n",
    "        )\n",
    "        # \n",
    "        y_pred = model.predict(X_val)\n",
    "        score = metric(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "        model1_oof[val_index] += y_pred / n_repeats\n",
    "        # \n",
    "        pred = model.predict(X_sub)\n",
    "        model1_preds[f\"pred_{i+1}\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a660ace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:48.994364Z",
     "iopub.status.busy": "2023-08-10T15:10:48.993568Z",
     "iopub.status.idle": "2023-08-10T15:10:49.000395Z",
     "shell.execute_reply": "2023-08-10T15:10:48.999503Z"
    },
    "papermill": {
     "duration": 0.03064,
     "end_time": "2023-08-10T15:10:49.002337",
     "exception": false,
     "start_time": "2023-08-10T15:10:48.971697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score mean: 0.19966716657458528\n",
      "oof Score mean: 0.1716920724856611\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score mean: {np.mean(scores)}\")\n",
    "print(f\"oof Score mean: {metric(y, model1_oof)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6b6de84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:49.045922Z",
     "iopub.status.busy": "2023-08-10T15:10:49.045082Z",
     "iopub.status.idle": "2023-08-10T15:10:49.138003Z",
     "shell.execute_reply": "2023-08-10T15:10:49.137157Z"
    },
    "papermill": {
     "duration": 0.117676,
     "end_time": "2023-08-10T15:10:49.140700",
     "exception": false,
     "start_time": "2023-08-10T15:10:49.023024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/2725093147.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  model1_preds[\"class_1\"] = model1_preds.drop(\"Id\", axis=1).mean(axis=1).tolist()\n",
      "/tmp/ipykernel_23/2725093147.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  model1_preds[\"class_0\"] = (1 - model1_preds[\"class_1\"]).tolist()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>pred_4</th>\n",
       "      <th>pred_5</th>\n",
       "      <th>pred_6</th>\n",
       "      <th>pred_7</th>\n",
       "      <th>pred_8</th>\n",
       "      <th>pred_9</th>\n",
       "      <th>pred_10</th>\n",
       "      <th>pred_11</th>\n",
       "      <th>pred_12</th>\n",
       "      <th>pred_13</th>\n",
       "      <th>pred_14</th>\n",
       "      <th>pred_15</th>\n",
       "      <th>pred_16</th>\n",
       "      <th>pred_17</th>\n",
       "      <th>pred_18</th>\n",
       "      <th>pred_19</th>\n",
       "      <th>pred_20</th>\n",
       "      <th>pred_21</th>\n",
       "      <th>pred_22</th>\n",
       "      <th>pred_23</th>\n",
       "      <th>pred_24</th>\n",
       "      <th>pred_25</th>\n",
       "      <th>pred_26</th>\n",
       "      <th>pred_27</th>\n",
       "      <th>pred_28</th>\n",
       "      <th>pred_29</th>\n",
       "      <th>pred_30</th>\n",
       "      <th>pred_31</th>\n",
       "      <th>pred_32</th>\n",
       "      <th>pred_33</th>\n",
       "      <th>pred_34</th>\n",
       "      <th>pred_35</th>\n",
       "      <th>pred_36</th>\n",
       "      <th>pred_37</th>\n",
       "      <th>pred_38</th>\n",
       "      <th>pred_39</th>\n",
       "      <th>pred_40</th>\n",
       "      <th>pred_41</th>\n",
       "      <th>pred_42</th>\n",
       "      <th>pred_43</th>\n",
       "      <th>pred_44</th>\n",
       "      <th>pred_45</th>\n",
       "      <th>pred_46</th>\n",
       "      <th>pred_47</th>\n",
       "      <th>pred_48</th>\n",
       "      <th>pred_49</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_53</th>\n",
       "      <th>pred_54</th>\n",
       "      <th>pred_55</th>\n",
       "      <th>pred_56</th>\n",
       "      <th>pred_57</th>\n",
       "      <th>pred_58</th>\n",
       "      <th>pred_59</th>\n",
       "      <th>pred_60</th>\n",
       "      <th>pred_61</th>\n",
       "      <th>pred_62</th>\n",
       "      <th>pred_63</th>\n",
       "      <th>pred_64</th>\n",
       "      <th>pred_65</th>\n",
       "      <th>pred_66</th>\n",
       "      <th>pred_67</th>\n",
       "      <th>pred_68</th>\n",
       "      <th>pred_69</th>\n",
       "      <th>pred_70</th>\n",
       "      <th>pred_71</th>\n",
       "      <th>pred_72</th>\n",
       "      <th>pred_73</th>\n",
       "      <th>pred_74</th>\n",
       "      <th>pred_75</th>\n",
       "      <th>pred_76</th>\n",
       "      <th>pred_77</th>\n",
       "      <th>pred_78</th>\n",
       "      <th>pred_79</th>\n",
       "      <th>pred_80</th>\n",
       "      <th>pred_81</th>\n",
       "      <th>pred_82</th>\n",
       "      <th>pred_83</th>\n",
       "      <th>pred_84</th>\n",
       "      <th>pred_85</th>\n",
       "      <th>pred_86</th>\n",
       "      <th>pred_87</th>\n",
       "      <th>pred_88</th>\n",
       "      <th>pred_89</th>\n",
       "      <th>pred_90</th>\n",
       "      <th>pred_91</th>\n",
       "      <th>pred_92</th>\n",
       "      <th>pred_93</th>\n",
       "      <th>pred_94</th>\n",
       "      <th>pred_95</th>\n",
       "      <th>pred_96</th>\n",
       "      <th>pred_97</th>\n",
       "      <th>pred_98</th>\n",
       "      <th>pred_99</th>\n",
       "      <th>pred_100</th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>0.025992</td>\n",
       "      <td>0.135041</td>\n",
       "      <td>0.08754</td>\n",
       "      <td>0.190641</td>\n",
       "      <td>0.021145</td>\n",
       "      <td>0.050264</td>\n",
       "      <td>0.030484</td>\n",
       "      <td>0.07442</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.140669</td>\n",
       "      <td>0.11606</td>\n",
       "      <td>0.200298</td>\n",
       "      <td>0.179447</td>\n",
       "      <td>0.09055</td>\n",
       "      <td>0.074146</td>\n",
       "      <td>0.057277</td>\n",
       "      <td>0.158453</td>\n",
       "      <td>0.07023</td>\n",
       "      <td>0.287781</td>\n",
       "      <td>0.043823</td>\n",
       "      <td>0.097248</td>\n",
       "      <td>0.014342</td>\n",
       "      <td>0.07335</td>\n",
       "      <td>0.122987</td>\n",
       "      <td>0.12438</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.080452</td>\n",
       "      <td>0.058202</td>\n",
       "      <td>0.379883</td>\n",
       "      <td>0.051994</td>\n",
       "      <td>0.034434</td>\n",
       "      <td>0.080427</td>\n",
       "      <td>0.08906</td>\n",
       "      <td>0.067441</td>\n",
       "      <td>0.02648</td>\n",
       "      <td>0.066437</td>\n",
       "      <td>0.059719</td>\n",
       "      <td>0.055147</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>0.130681</td>\n",
       "      <td>0.05192</td>\n",
       "      <td>0.047997</td>\n",
       "      <td>0.087659</td>\n",
       "      <td>0.233071</td>\n",
       "      <td>0.044034</td>\n",
       "      <td>0.195231</td>\n",
       "      <td>0.087471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215512</td>\n",
       "      <td>0.15667</td>\n",
       "      <td>0.148774</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.143561</td>\n",
       "      <td>0.056464</td>\n",
       "      <td>0.038422</td>\n",
       "      <td>0.01864</td>\n",
       "      <td>0.026862</td>\n",
       "      <td>0.168265</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.01067</td>\n",
       "      <td>0.295883</td>\n",
       "      <td>0.084071</td>\n",
       "      <td>0.171458</td>\n",
       "      <td>0.071303</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>0.02131</td>\n",
       "      <td>0.077145</td>\n",
       "      <td>0.027658</td>\n",
       "      <td>0.019318</td>\n",
       "      <td>0.150282</td>\n",
       "      <td>0.032593</td>\n",
       "      <td>0.022746</td>\n",
       "      <td>0.104199</td>\n",
       "      <td>0.182705</td>\n",
       "      <td>0.241377</td>\n",
       "      <td>0.148547</td>\n",
       "      <td>0.051326</td>\n",
       "      <td>0.07352</td>\n",
       "      <td>0.096315</td>\n",
       "      <td>0.074368</td>\n",
       "      <td>0.122008</td>\n",
       "      <td>0.14354</td>\n",
       "      <td>0.086266</td>\n",
       "      <td>0.077234</td>\n",
       "      <td>0.049306</td>\n",
       "      <td>0.182765</td>\n",
       "      <td>0.208849</td>\n",
       "      <td>0.224775</td>\n",
       "      <td>0.022863</td>\n",
       "      <td>0.174186</td>\n",
       "      <td>0.052935</td>\n",
       "      <td>0.044002</td>\n",
       "      <td>0.034277</td>\n",
       "      <td>0.103193</td>\n",
       "      <td>0.126305</td>\n",
       "      <td>0.080259</td>\n",
       "      <td>0.09344</td>\n",
       "      <td>0.90656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>0.025992</td>\n",
       "      <td>0.135041</td>\n",
       "      <td>0.08754</td>\n",
       "      <td>0.190641</td>\n",
       "      <td>0.021145</td>\n",
       "      <td>0.050264</td>\n",
       "      <td>0.030484</td>\n",
       "      <td>0.07442</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.140669</td>\n",
       "      <td>0.11606</td>\n",
       "      <td>0.200298</td>\n",
       "      <td>0.179447</td>\n",
       "      <td>0.09055</td>\n",
       "      <td>0.074146</td>\n",
       "      <td>0.057277</td>\n",
       "      <td>0.158453</td>\n",
       "      <td>0.07023</td>\n",
       "      <td>0.287781</td>\n",
       "      <td>0.043823</td>\n",
       "      <td>0.097248</td>\n",
       "      <td>0.014342</td>\n",
       "      <td>0.07335</td>\n",
       "      <td>0.122987</td>\n",
       "      <td>0.12438</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.080452</td>\n",
       "      <td>0.058202</td>\n",
       "      <td>0.379883</td>\n",
       "      <td>0.051994</td>\n",
       "      <td>0.034434</td>\n",
       "      <td>0.080427</td>\n",
       "      <td>0.08906</td>\n",
       "      <td>0.067441</td>\n",
       "      <td>0.02648</td>\n",
       "      <td>0.066437</td>\n",
       "      <td>0.059719</td>\n",
       "      <td>0.055147</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>0.130681</td>\n",
       "      <td>0.05192</td>\n",
       "      <td>0.047997</td>\n",
       "      <td>0.087659</td>\n",
       "      <td>0.233071</td>\n",
       "      <td>0.044034</td>\n",
       "      <td>0.195231</td>\n",
       "      <td>0.087471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215512</td>\n",
       "      <td>0.15667</td>\n",
       "      <td>0.148774</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.143561</td>\n",
       "      <td>0.056464</td>\n",
       "      <td>0.038422</td>\n",
       "      <td>0.01864</td>\n",
       "      <td>0.026862</td>\n",
       "      <td>0.168265</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.01067</td>\n",
       "      <td>0.295883</td>\n",
       "      <td>0.084071</td>\n",
       "      <td>0.171458</td>\n",
       "      <td>0.071303</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>0.02131</td>\n",
       "      <td>0.077145</td>\n",
       "      <td>0.027658</td>\n",
       "      <td>0.019318</td>\n",
       "      <td>0.150282</td>\n",
       "      <td>0.032593</td>\n",
       "      <td>0.022746</td>\n",
       "      <td>0.104199</td>\n",
       "      <td>0.182705</td>\n",
       "      <td>0.241377</td>\n",
       "      <td>0.148547</td>\n",
       "      <td>0.051326</td>\n",
       "      <td>0.07352</td>\n",
       "      <td>0.096315</td>\n",
       "      <td>0.074368</td>\n",
       "      <td>0.122008</td>\n",
       "      <td>0.14354</td>\n",
       "      <td>0.086266</td>\n",
       "      <td>0.077234</td>\n",
       "      <td>0.049306</td>\n",
       "      <td>0.182765</td>\n",
       "      <td>0.208849</td>\n",
       "      <td>0.224775</td>\n",
       "      <td>0.022863</td>\n",
       "      <td>0.174186</td>\n",
       "      <td>0.052935</td>\n",
       "      <td>0.044002</td>\n",
       "      <td>0.034277</td>\n",
       "      <td>0.103193</td>\n",
       "      <td>0.126305</td>\n",
       "      <td>0.080259</td>\n",
       "      <td>0.09344</td>\n",
       "      <td>0.90656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>0.025992</td>\n",
       "      <td>0.135041</td>\n",
       "      <td>0.08754</td>\n",
       "      <td>0.190641</td>\n",
       "      <td>0.021145</td>\n",
       "      <td>0.050264</td>\n",
       "      <td>0.030484</td>\n",
       "      <td>0.07442</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.140669</td>\n",
       "      <td>0.11606</td>\n",
       "      <td>0.200298</td>\n",
       "      <td>0.179447</td>\n",
       "      <td>0.09055</td>\n",
       "      <td>0.074146</td>\n",
       "      <td>0.057277</td>\n",
       "      <td>0.158453</td>\n",
       "      <td>0.07023</td>\n",
       "      <td>0.287781</td>\n",
       "      <td>0.043823</td>\n",
       "      <td>0.097248</td>\n",
       "      <td>0.014342</td>\n",
       "      <td>0.07335</td>\n",
       "      <td>0.122987</td>\n",
       "      <td>0.12438</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.080452</td>\n",
       "      <td>0.058202</td>\n",
       "      <td>0.379883</td>\n",
       "      <td>0.051994</td>\n",
       "      <td>0.034434</td>\n",
       "      <td>0.080427</td>\n",
       "      <td>0.08906</td>\n",
       "      <td>0.067441</td>\n",
       "      <td>0.02648</td>\n",
       "      <td>0.066437</td>\n",
       "      <td>0.059719</td>\n",
       "      <td>0.055147</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>0.130681</td>\n",
       "      <td>0.05192</td>\n",
       "      <td>0.047997</td>\n",
       "      <td>0.087659</td>\n",
       "      <td>0.233071</td>\n",
       "      <td>0.044034</td>\n",
       "      <td>0.195231</td>\n",
       "      <td>0.087471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215512</td>\n",
       "      <td>0.15667</td>\n",
       "      <td>0.148774</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.143561</td>\n",
       "      <td>0.056464</td>\n",
       "      <td>0.038422</td>\n",
       "      <td>0.01864</td>\n",
       "      <td>0.026862</td>\n",
       "      <td>0.168265</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.01067</td>\n",
       "      <td>0.295883</td>\n",
       "      <td>0.084071</td>\n",
       "      <td>0.171458</td>\n",
       "      <td>0.071303</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>0.02131</td>\n",
       "      <td>0.077145</td>\n",
       "      <td>0.027658</td>\n",
       "      <td>0.019318</td>\n",
       "      <td>0.150282</td>\n",
       "      <td>0.032593</td>\n",
       "      <td>0.022746</td>\n",
       "      <td>0.104199</td>\n",
       "      <td>0.182705</td>\n",
       "      <td>0.241377</td>\n",
       "      <td>0.148547</td>\n",
       "      <td>0.051326</td>\n",
       "      <td>0.07352</td>\n",
       "      <td>0.096315</td>\n",
       "      <td>0.074368</td>\n",
       "      <td>0.122008</td>\n",
       "      <td>0.14354</td>\n",
       "      <td>0.086266</td>\n",
       "      <td>0.077234</td>\n",
       "      <td>0.049306</td>\n",
       "      <td>0.182765</td>\n",
       "      <td>0.208849</td>\n",
       "      <td>0.224775</td>\n",
       "      <td>0.022863</td>\n",
       "      <td>0.174186</td>\n",
       "      <td>0.052935</td>\n",
       "      <td>0.044002</td>\n",
       "      <td>0.034277</td>\n",
       "      <td>0.103193</td>\n",
       "      <td>0.126305</td>\n",
       "      <td>0.080259</td>\n",
       "      <td>0.09344</td>\n",
       "      <td>0.90656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>0.025992</td>\n",
       "      <td>0.135041</td>\n",
       "      <td>0.08754</td>\n",
       "      <td>0.190641</td>\n",
       "      <td>0.021145</td>\n",
       "      <td>0.050264</td>\n",
       "      <td>0.030484</td>\n",
       "      <td>0.07442</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.140669</td>\n",
       "      <td>0.11606</td>\n",
       "      <td>0.200298</td>\n",
       "      <td>0.179447</td>\n",
       "      <td>0.09055</td>\n",
       "      <td>0.074146</td>\n",
       "      <td>0.057277</td>\n",
       "      <td>0.158453</td>\n",
       "      <td>0.07023</td>\n",
       "      <td>0.287781</td>\n",
       "      <td>0.043823</td>\n",
       "      <td>0.097248</td>\n",
       "      <td>0.014342</td>\n",
       "      <td>0.07335</td>\n",
       "      <td>0.122987</td>\n",
       "      <td>0.12438</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.080452</td>\n",
       "      <td>0.058202</td>\n",
       "      <td>0.379883</td>\n",
       "      <td>0.051994</td>\n",
       "      <td>0.034434</td>\n",
       "      <td>0.080427</td>\n",
       "      <td>0.08906</td>\n",
       "      <td>0.067441</td>\n",
       "      <td>0.02648</td>\n",
       "      <td>0.066437</td>\n",
       "      <td>0.059719</td>\n",
       "      <td>0.055147</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>0.130681</td>\n",
       "      <td>0.05192</td>\n",
       "      <td>0.047997</td>\n",
       "      <td>0.087659</td>\n",
       "      <td>0.233071</td>\n",
       "      <td>0.044034</td>\n",
       "      <td>0.195231</td>\n",
       "      <td>0.087471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215512</td>\n",
       "      <td>0.15667</td>\n",
       "      <td>0.148774</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.143561</td>\n",
       "      <td>0.056464</td>\n",
       "      <td>0.038422</td>\n",
       "      <td>0.01864</td>\n",
       "      <td>0.026862</td>\n",
       "      <td>0.168265</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.01067</td>\n",
       "      <td>0.295883</td>\n",
       "      <td>0.084071</td>\n",
       "      <td>0.171458</td>\n",
       "      <td>0.071303</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>0.02131</td>\n",
       "      <td>0.077145</td>\n",
       "      <td>0.027658</td>\n",
       "      <td>0.019318</td>\n",
       "      <td>0.150282</td>\n",
       "      <td>0.032593</td>\n",
       "      <td>0.022746</td>\n",
       "      <td>0.104199</td>\n",
       "      <td>0.182705</td>\n",
       "      <td>0.241377</td>\n",
       "      <td>0.148547</td>\n",
       "      <td>0.051326</td>\n",
       "      <td>0.07352</td>\n",
       "      <td>0.096315</td>\n",
       "      <td>0.074368</td>\n",
       "      <td>0.122008</td>\n",
       "      <td>0.14354</td>\n",
       "      <td>0.086266</td>\n",
       "      <td>0.077234</td>\n",
       "      <td>0.049306</td>\n",
       "      <td>0.182765</td>\n",
       "      <td>0.208849</td>\n",
       "      <td>0.224775</td>\n",
       "      <td>0.022863</td>\n",
       "      <td>0.174186</td>\n",
       "      <td>0.052935</td>\n",
       "      <td>0.044002</td>\n",
       "      <td>0.034277</td>\n",
       "      <td>0.103193</td>\n",
       "      <td>0.126305</td>\n",
       "      <td>0.080259</td>\n",
       "      <td>0.09344</td>\n",
       "      <td>0.90656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>0.025992</td>\n",
       "      <td>0.135041</td>\n",
       "      <td>0.08754</td>\n",
       "      <td>0.190641</td>\n",
       "      <td>0.021145</td>\n",
       "      <td>0.050264</td>\n",
       "      <td>0.030484</td>\n",
       "      <td>0.07442</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.140669</td>\n",
       "      <td>0.11606</td>\n",
       "      <td>0.200298</td>\n",
       "      <td>0.179447</td>\n",
       "      <td>0.09055</td>\n",
       "      <td>0.074146</td>\n",
       "      <td>0.057277</td>\n",
       "      <td>0.158453</td>\n",
       "      <td>0.07023</td>\n",
       "      <td>0.287781</td>\n",
       "      <td>0.043823</td>\n",
       "      <td>0.097248</td>\n",
       "      <td>0.014342</td>\n",
       "      <td>0.07335</td>\n",
       "      <td>0.122987</td>\n",
       "      <td>0.12438</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.080452</td>\n",
       "      <td>0.058202</td>\n",
       "      <td>0.379883</td>\n",
       "      <td>0.051994</td>\n",
       "      <td>0.034434</td>\n",
       "      <td>0.080427</td>\n",
       "      <td>0.08906</td>\n",
       "      <td>0.067441</td>\n",
       "      <td>0.02648</td>\n",
       "      <td>0.066437</td>\n",
       "      <td>0.059719</td>\n",
       "      <td>0.055147</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>0.130681</td>\n",
       "      <td>0.05192</td>\n",
       "      <td>0.047997</td>\n",
       "      <td>0.087659</td>\n",
       "      <td>0.233071</td>\n",
       "      <td>0.044034</td>\n",
       "      <td>0.195231</td>\n",
       "      <td>0.087471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215512</td>\n",
       "      <td>0.15667</td>\n",
       "      <td>0.148774</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.143561</td>\n",
       "      <td>0.056464</td>\n",
       "      <td>0.038422</td>\n",
       "      <td>0.01864</td>\n",
       "      <td>0.026862</td>\n",
       "      <td>0.168265</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.01067</td>\n",
       "      <td>0.295883</td>\n",
       "      <td>0.084071</td>\n",
       "      <td>0.171458</td>\n",
       "      <td>0.071303</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>0.02131</td>\n",
       "      <td>0.077145</td>\n",
       "      <td>0.027658</td>\n",
       "      <td>0.019318</td>\n",
       "      <td>0.150282</td>\n",
       "      <td>0.032593</td>\n",
       "      <td>0.022746</td>\n",
       "      <td>0.104199</td>\n",
       "      <td>0.182705</td>\n",
       "      <td>0.241377</td>\n",
       "      <td>0.148547</td>\n",
       "      <td>0.051326</td>\n",
       "      <td>0.07352</td>\n",
       "      <td>0.096315</td>\n",
       "      <td>0.074368</td>\n",
       "      <td>0.122008</td>\n",
       "      <td>0.14354</td>\n",
       "      <td>0.086266</td>\n",
       "      <td>0.077234</td>\n",
       "      <td>0.049306</td>\n",
       "      <td>0.182765</td>\n",
       "      <td>0.208849</td>\n",
       "      <td>0.224775</td>\n",
       "      <td>0.022863</td>\n",
       "      <td>0.174186</td>\n",
       "      <td>0.052935</td>\n",
       "      <td>0.044002</td>\n",
       "      <td>0.034277</td>\n",
       "      <td>0.103193</td>\n",
       "      <td>0.126305</td>\n",
       "      <td>0.080259</td>\n",
       "      <td>0.09344</td>\n",
       "      <td>0.90656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id    pred_1    pred_2    pred_3   pred_4    pred_5    pred_6  \\\n",
       "0  00eed32682bb  0.012038  0.025992  0.135041  0.08754  0.190641  0.021145   \n",
       "1  010ebe33f668  0.012038  0.025992  0.135041  0.08754  0.190641  0.021145   \n",
       "2  02fa521e1838  0.012038  0.025992  0.135041  0.08754  0.190641  0.021145   \n",
       "3  040e15f562a2  0.012038  0.025992  0.135041  0.08754  0.190641  0.021145   \n",
       "4  046e85c7cc7f  0.012038  0.025992  0.135041  0.08754  0.190641  0.021145   \n",
       "\n",
       "     pred_7    pred_8   pred_9   pred_10   pred_11  pred_12   pred_13  \\\n",
       "0  0.050264  0.030484  0.07442  0.014735  0.140669  0.11606  0.200298   \n",
       "1  0.050264  0.030484  0.07442  0.014735  0.140669  0.11606  0.200298   \n",
       "2  0.050264  0.030484  0.07442  0.014735  0.140669  0.11606  0.200298   \n",
       "3  0.050264  0.030484  0.07442  0.014735  0.140669  0.11606  0.200298   \n",
       "4  0.050264  0.030484  0.07442  0.014735  0.140669  0.11606  0.200298   \n",
       "\n",
       "    pred_14  pred_15   pred_16   pred_17   pred_18  pred_19   pred_20  \\\n",
       "0  0.179447  0.09055  0.074146  0.057277  0.158453  0.07023  0.287781   \n",
       "1  0.179447  0.09055  0.074146  0.057277  0.158453  0.07023  0.287781   \n",
       "2  0.179447  0.09055  0.074146  0.057277  0.158453  0.07023  0.287781   \n",
       "3  0.179447  0.09055  0.074146  0.057277  0.158453  0.07023  0.287781   \n",
       "4  0.179447  0.09055  0.074146  0.057277  0.158453  0.07023  0.287781   \n",
       "\n",
       "    pred_21   pred_22   pred_23  pred_24   pred_25  pred_26  pred_27  \\\n",
       "0  0.043823  0.097248  0.014342  0.07335  0.122987  0.12438   0.0706   \n",
       "1  0.043823  0.097248  0.014342  0.07335  0.122987  0.12438   0.0706   \n",
       "2  0.043823  0.097248  0.014342  0.07335  0.122987  0.12438   0.0706   \n",
       "3  0.043823  0.097248  0.014342  0.07335  0.122987  0.12438   0.0706   \n",
       "4  0.043823  0.097248  0.014342  0.07335  0.122987  0.12438   0.0706   \n",
       "\n",
       "    pred_28   pred_29   pred_30   pred_31   pred_32   pred_33   pred_34  \\\n",
       "0  0.014131  0.080452  0.058202  0.379883  0.051994  0.034434  0.080427   \n",
       "1  0.014131  0.080452  0.058202  0.379883  0.051994  0.034434  0.080427   \n",
       "2  0.014131  0.080452  0.058202  0.379883  0.051994  0.034434  0.080427   \n",
       "3  0.014131  0.080452  0.058202  0.379883  0.051994  0.034434  0.080427   \n",
       "4  0.014131  0.080452  0.058202  0.379883  0.051994  0.034434  0.080427   \n",
       "\n",
       "   pred_35   pred_36  pred_37   pred_38   pred_39   pred_40   pred_41  \\\n",
       "0  0.08906  0.067441  0.02648  0.066437  0.059719  0.055147  0.020675   \n",
       "1  0.08906  0.067441  0.02648  0.066437  0.059719  0.055147  0.020675   \n",
       "2  0.08906  0.067441  0.02648  0.066437  0.059719  0.055147  0.020675   \n",
       "3  0.08906  0.067441  0.02648  0.066437  0.059719  0.055147  0.020675   \n",
       "4  0.08906  0.067441  0.02648  0.066437  0.059719  0.055147  0.020675   \n",
       "\n",
       "    pred_42  pred_43   pred_44   pred_45   pred_46   pred_47   pred_48  \\\n",
       "0  0.130681  0.05192  0.047997  0.087659  0.233071  0.044034  0.195231   \n",
       "1  0.130681  0.05192  0.047997  0.087659  0.233071  0.044034  0.195231   \n",
       "2  0.130681  0.05192  0.047997  0.087659  0.233071  0.044034  0.195231   \n",
       "3  0.130681  0.05192  0.047997  0.087659  0.233071  0.044034  0.195231   \n",
       "4  0.130681  0.05192  0.047997  0.087659  0.233071  0.044034  0.195231   \n",
       "\n",
       "    pred_49  ...   pred_53  pred_54   pred_55   pred_56   pred_57   pred_58  \\\n",
       "0  0.087471  ...  0.215512  0.15667  0.148774  0.001568  0.143561  0.056464   \n",
       "1  0.087471  ...  0.215512  0.15667  0.148774  0.001568  0.143561  0.056464   \n",
       "2  0.087471  ...  0.215512  0.15667  0.148774  0.001568  0.143561  0.056464   \n",
       "3  0.087471  ...  0.215512  0.15667  0.148774  0.001568  0.143561  0.056464   \n",
       "4  0.087471  ...  0.215512  0.15667  0.148774  0.001568  0.143561  0.056464   \n",
       "\n",
       "    pred_59  pred_60   pred_61   pred_62   pred_63  pred_64   pred_65  \\\n",
       "0  0.038422  0.01864  0.026862  0.168265  0.003964  0.01067  0.295883   \n",
       "1  0.038422  0.01864  0.026862  0.168265  0.003964  0.01067  0.295883   \n",
       "2  0.038422  0.01864  0.026862  0.168265  0.003964  0.01067  0.295883   \n",
       "3  0.038422  0.01864  0.026862  0.168265  0.003964  0.01067  0.295883   \n",
       "4  0.038422  0.01864  0.026862  0.168265  0.003964  0.01067  0.295883   \n",
       "\n",
       "    pred_66   pred_67   pred_68   pred_69  pred_70   pred_71   pred_72  \\\n",
       "0  0.084071  0.171458  0.071303  0.014065  0.02131  0.077145  0.027658   \n",
       "1  0.084071  0.171458  0.071303  0.014065  0.02131  0.077145  0.027658   \n",
       "2  0.084071  0.171458  0.071303  0.014065  0.02131  0.077145  0.027658   \n",
       "3  0.084071  0.171458  0.071303  0.014065  0.02131  0.077145  0.027658   \n",
       "4  0.084071  0.171458  0.071303  0.014065  0.02131  0.077145  0.027658   \n",
       "\n",
       "    pred_73   pred_74   pred_75   pred_76   pred_77   pred_78   pred_79  \\\n",
       "0  0.019318  0.150282  0.032593  0.022746  0.104199  0.182705  0.241377   \n",
       "1  0.019318  0.150282  0.032593  0.022746  0.104199  0.182705  0.241377   \n",
       "2  0.019318  0.150282  0.032593  0.022746  0.104199  0.182705  0.241377   \n",
       "3  0.019318  0.150282  0.032593  0.022746  0.104199  0.182705  0.241377   \n",
       "4  0.019318  0.150282  0.032593  0.022746  0.104199  0.182705  0.241377   \n",
       "\n",
       "    pred_80   pred_81  pred_82   pred_83   pred_84   pred_85  pred_86  \\\n",
       "0  0.148547  0.051326  0.07352  0.096315  0.074368  0.122008  0.14354   \n",
       "1  0.148547  0.051326  0.07352  0.096315  0.074368  0.122008  0.14354   \n",
       "2  0.148547  0.051326  0.07352  0.096315  0.074368  0.122008  0.14354   \n",
       "3  0.148547  0.051326  0.07352  0.096315  0.074368  0.122008  0.14354   \n",
       "4  0.148547  0.051326  0.07352  0.096315  0.074368  0.122008  0.14354   \n",
       "\n",
       "    pred_87   pred_88   pred_89   pred_90   pred_91   pred_92   pred_93  \\\n",
       "0  0.086266  0.077234  0.049306  0.182765  0.208849  0.224775  0.022863   \n",
       "1  0.086266  0.077234  0.049306  0.182765  0.208849  0.224775  0.022863   \n",
       "2  0.086266  0.077234  0.049306  0.182765  0.208849  0.224775  0.022863   \n",
       "3  0.086266  0.077234  0.049306  0.182765  0.208849  0.224775  0.022863   \n",
       "4  0.086266  0.077234  0.049306  0.182765  0.208849  0.224775  0.022863   \n",
       "\n",
       "    pred_94   pred_95   pred_96   pred_97   pred_98   pred_99  pred_100  \\\n",
       "0  0.174186  0.052935  0.044002  0.034277  0.103193  0.126305  0.080259   \n",
       "1  0.174186  0.052935  0.044002  0.034277  0.103193  0.126305  0.080259   \n",
       "2  0.174186  0.052935  0.044002  0.034277  0.103193  0.126305  0.080259   \n",
       "3  0.174186  0.052935  0.044002  0.034277  0.103193  0.126305  0.080259   \n",
       "4  0.174186  0.052935  0.044002  0.034277  0.103193  0.126305  0.080259   \n",
       "\n",
       "   class_1  class_0  \n",
       "0  0.09344  0.90656  \n",
       "1  0.09344  0.90656  \n",
       "2  0.09344  0.90656  \n",
       "3  0.09344  0.90656  \n",
       "4  0.09344  0.90656  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_preds[\"class_1\"] = model1_preds.drop(\"Id\", axis=1).mean(axis=1).tolist()\n",
    "model1_preds[\"class_0\"] = (1 - model1_preds[\"class_1\"]).tolist()\n",
    "model1_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c08b97",
   "metadata": {
    "papermill": {
     "duration": 0.021594,
     "end_time": "2023-08-10T15:10:49.185207",
     "exception": false,
     "start_time": "2023-08-10T15:10:49.163613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# XGBoost + NestedCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bbf0cea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:49.230694Z",
     "iopub.status.busy": "2023-08-10T15:10:49.229936Z",
     "iopub.status.idle": "2023-08-10T15:10:49.323407Z",
     "shell.execute_reply": "2023-08-10T15:10:49.322281Z"
    },
    "papermill": {
     "duration": 0.118762,
     "end_time": "2023-08-10T15:10:49.326039",
     "exception": false,
     "start_time": "2023-08-10T15:10:49.207277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>BN</th>\n",
       "      <th>BP</th>\n",
       "      <th>BQ</th>\n",
       "      <th>BR</th>\n",
       "      <th>BZ</th>\n",
       "      <th>CB</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>CF</th>\n",
       "      <th>CH</th>\n",
       "      <th>CL</th>\n",
       "      <th>CR</th>\n",
       "      <th>CS</th>\n",
       "      <th>CU</th>\n",
       "      <th>CW</th>\n",
       "      <th>DA</th>\n",
       "      <th>DE</th>\n",
       "      <th>DF</th>\n",
       "      <th>DH</th>\n",
       "      <th>DI</th>\n",
       "      <th>DL</th>\n",
       "      <th>DN</th>\n",
       "      <th>DU</th>\n",
       "      <th>DV</th>\n",
       "      <th>DY</th>\n",
       "      <th>EB</th>\n",
       "      <th>EE</th>\n",
       "      <th>EG</th>\n",
       "      <th>EH</th>\n",
       "      <th>EJ</th>\n",
       "      <th>EL</th>\n",
       "      <th>EP</th>\n",
       "      <th>EU</th>\n",
       "      <th>FC</th>\n",
       "      <th>FD</th>\n",
       "      <th>FE</th>\n",
       "      <th>FI</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000ff2bfdfe9</td>\n",
       "      <td>0.209377</td>\n",
       "      <td>3109.03329</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.812214</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>4126.58731</td>\n",
       "      <td>22.5984</td>\n",
       "      <td>175.638726</td>\n",
       "      <td>152.707705</td>\n",
       "      <td>823.928241</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>47.223358</td>\n",
       "      <td>0.563481</td>\n",
       "      <td>23.387600</td>\n",
       "      <td>4.851915</td>\n",
       "      <td>0.023482</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.069225</td>\n",
       "      <td>13.784111</td>\n",
       "      <td>1.302012</td>\n",
       "      <td>36.205956</td>\n",
       "      <td>69.08340</td>\n",
       "      <td>295.570575</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.284232</td>\n",
       "      <td>89.245560</td>\n",
       "      <td>84.31664</td>\n",
       "      <td>29.657104</td>\n",
       "      <td>5.310690</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>23.187704</td>\n",
       "      <td>7.294176</td>\n",
       "      <td>1.987283</td>\n",
       "      <td>1433.166750</td>\n",
       "      <td>0.949104</td>\n",
       "      <td>1</td>\n",
       "      <td>30.879420</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>3.828384</td>\n",
       "      <td>13.394640</td>\n",
       "      <td>10.265073</td>\n",
       "      <td>9028.291921</td>\n",
       "      <td>3.583450</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>11.339138</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>22.136229</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007255e47698</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>978.76416</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.632190</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>13.517790</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5496.92824</td>\n",
       "      <td>19.4205</td>\n",
       "      <td>155.868030</td>\n",
       "      <td>14.754720</td>\n",
       "      <td>51.216883</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>30.284345</td>\n",
       "      <td>0.484710</td>\n",
       "      <td>50.628208</td>\n",
       "      <td>6.085041</td>\n",
       "      <td>0.031442</td>\n",
       "      <td>1.113875</td>\n",
       "      <td>1.117800</td>\n",
       "      <td>28.310953</td>\n",
       "      <td>1.357182</td>\n",
       "      <td>37.476568</td>\n",
       "      <td>70.79836</td>\n",
       "      <td>178.553100</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.363489</td>\n",
       "      <td>110.581815</td>\n",
       "      <td>75.74548</td>\n",
       "      <td>37.532000</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>17.222328</td>\n",
       "      <td>4.926396</td>\n",
       "      <td>0.858603</td>\n",
       "      <td>1111.287150</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>95.415086</td>\n",
       "      <td>52.260480</td>\n",
       "      <td>17.175984</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>6785.003474</td>\n",
       "      <td>10.358927</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>9.292698</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>29.135430</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>013f2bd269f5</td>\n",
       "      <td>0.470030</td>\n",
       "      <td>2635.10654</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>6.732840</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.824570</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5135.78024</td>\n",
       "      <td>26.4825</td>\n",
       "      <td>128.988531</td>\n",
       "      <td>219.320160</td>\n",
       "      <td>482.141594</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>32.563713</td>\n",
       "      <td>0.495852</td>\n",
       "      <td>85.955376</td>\n",
       "      <td>5.376488</td>\n",
       "      <td>0.036218</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.700350</td>\n",
       "      <td>39.364743</td>\n",
       "      <td>1.009611</td>\n",
       "      <td>21.459644</td>\n",
       "      <td>70.81970</td>\n",
       "      <td>321.426625</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.210441</td>\n",
       "      <td>120.056438</td>\n",
       "      <td>65.46984</td>\n",
       "      <td>28.053464</td>\n",
       "      <td>1.289739</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>36.861352</td>\n",
       "      <td>7.813674</td>\n",
       "      <td>8.146651</td>\n",
       "      <td>1494.076488</td>\n",
       "      <td>0.377208</td>\n",
       "      <td>1</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>5.390628</td>\n",
       "      <td>224.207424</td>\n",
       "      <td>8.745201</td>\n",
       "      <td>8338.906181</td>\n",
       "      <td>11.626917</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>37.077772</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>28.022851</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>043ac50845d5</td>\n",
       "      <td>0.252107</td>\n",
       "      <td>3819.65177</td>\n",
       "      <td>120.201618</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.685344</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>11.053708</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4169.67738</td>\n",
       "      <td>23.6577</td>\n",
       "      <td>237.282264</td>\n",
       "      <td>11.050410</td>\n",
       "      <td>661.518640</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>15.201914</td>\n",
       "      <td>0.717882</td>\n",
       "      <td>88.159360</td>\n",
       "      <td>2.347652</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>1.400300</td>\n",
       "      <td>0.636075</td>\n",
       "      <td>41.116960</td>\n",
       "      <td>0.722727</td>\n",
       "      <td>21.530392</td>\n",
       "      <td>47.27586</td>\n",
       "      <td>196.607985</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.292431</td>\n",
       "      <td>139.824570</td>\n",
       "      <td>71.57120</td>\n",
       "      <td>24.354856</td>\n",
       "      <td>2.655345</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>52.003884</td>\n",
       "      <td>7.386060</td>\n",
       "      <td>3.813326</td>\n",
       "      <td>15691.552180</td>\n",
       "      <td>0.614484</td>\n",
       "      <td>1</td>\n",
       "      <td>31.674357</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>31.323372</td>\n",
       "      <td>59.301984</td>\n",
       "      <td>7.884336</td>\n",
       "      <td>10965.766040</td>\n",
       "      <td>14.852022</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>18.529584</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>39.948656</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>044fb8a146ec</td>\n",
       "      <td>0.380297</td>\n",
       "      <td>3733.04844</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.942255</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>5728.73412</td>\n",
       "      <td>24.0108</td>\n",
       "      <td>324.546318</td>\n",
       "      <td>149.717165</td>\n",
       "      <td>6074.859475</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>82.213495</td>\n",
       "      <td>0.536467</td>\n",
       "      <td>72.644264</td>\n",
       "      <td>30.537722</td>\n",
       "      <td>0.025472</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.693150</td>\n",
       "      <td>31.724726</td>\n",
       "      <td>0.827550</td>\n",
       "      <td>34.415360</td>\n",
       "      <td>74.06532</td>\n",
       "      <td>200.178160</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.207708</td>\n",
       "      <td>97.920120</td>\n",
       "      <td>52.83888</td>\n",
       "      <td>26.019912</td>\n",
       "      <td>1.144902</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>9.064856</td>\n",
       "      <td>7.350720</td>\n",
       "      <td>3.490846</td>\n",
       "      <td>1403.656300</td>\n",
       "      <td>0.164268</td>\n",
       "      <td>1</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>91.994825</td>\n",
       "      <td>51.141336</td>\n",
       "      <td>29.102640</td>\n",
       "      <td>4.274640</td>\n",
       "      <td>16198.049590</td>\n",
       "      <td>13.666727</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>16.408728</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>45.381316</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id        AB          AF          AH         AM        AR  \\\n",
       "0  000ff2bfdfe9  0.209377  3109.03329   85.200147  22.394407  8.138688   \n",
       "1  007255e47698  0.145282   978.76416   85.200147  36.968889  8.138688   \n",
       "2  013f2bd269f5  0.470030  2635.10654   85.200147  32.360553  8.138688   \n",
       "3  043ac50845d5  0.252107  3819.65177  120.201618  77.112203  8.138688   \n",
       "4  044fb8a146ec  0.380297  3733.04844   85.200147  14.103738  8.138688   \n",
       "\n",
       "         AX        AY         AZ          BC         BD        BN          BP  \\\n",
       "0  0.699861  0.025578   9.812214    5.555634  4126.58731  22.5984  175.638726   \n",
       "1  3.632190  0.025578  13.517790    1.229900  5496.92824  19.4205  155.868030   \n",
       "2  6.732840  0.025578  12.824570    1.229900  5135.78024  26.4825  128.988531   \n",
       "3  3.685344  0.025578  11.053708    1.229900  4169.67738  23.6577  237.282264   \n",
       "4  3.942255  0.054810   3.396778  102.151980  5728.73412  24.0108  324.546318   \n",
       "\n",
       "           BQ           BR          BZ         CB        CC        CD   \\\n",
       "0  152.707705   823.928241  257.432377  47.223358  0.563481  23.387600   \n",
       "1   14.754720    51.216883  257.432377  30.284345  0.484710  50.628208   \n",
       "2  219.320160   482.141594  257.432377  32.563713  0.495852  85.955376   \n",
       "3   11.050410   661.518640  257.432377  15.201914  0.717882  88.159360   \n",
       "4  149.717165  6074.859475  257.432377  82.213495  0.536467  72.644264   \n",
       "\n",
       "          CF        CH        CL        CR         CS        CU        CW   \\\n",
       "0   4.851915  0.023482  1.050225  0.069225  13.784111  1.302012  36.205956   \n",
       "1   6.085041  0.031442  1.113875  1.117800  28.310953  1.357182  37.476568   \n",
       "2   5.376488  0.036218  1.050225  0.700350  39.364743  1.009611  21.459644   \n",
       "3   2.347652  0.029054  1.400300  0.636075  41.116960  0.722727  21.530392   \n",
       "4  30.537722  0.025472  1.050225  0.693150  31.724726  0.827550  34.415360   \n",
       "\n",
       "         DA          DE       DF        DH          DI        DL         DN  \\\n",
       "0  69.08340  295.570575  0.23868  0.284232   89.245560  84.31664  29.657104   \n",
       "1  70.79836  178.553100  0.23868  0.363489  110.581815  75.74548  37.532000   \n",
       "2  70.81970  321.426625  0.23868  0.210441  120.056438  65.46984  28.053464   \n",
       "3  47.27586  196.607985  0.23868  0.292431  139.824570  71.57120  24.354856   \n",
       "4  74.06532  200.178160  0.23868  0.207708   97.920120  52.83888  26.019912   \n",
       "\n",
       "         DU       DV         DY        EB        EE            EG        EH  \\\n",
       "0  5.310690  1.74307  23.187704  7.294176  1.987283   1433.166750  0.949104   \n",
       "1  0.005518  1.74307  17.222328  4.926396  0.858603   1111.287150  0.003042   \n",
       "2  1.289739  1.74307  36.861352  7.813674  8.146651   1494.076488  0.377208   \n",
       "3  2.655345  1.74307  52.003884  7.386060  3.813326  15691.552180  0.614484   \n",
       "4  1.144902  1.74307   9.064856  7.350720  3.490846   1403.656300  0.164268   \n",
       "\n",
       "   EJ          EL         EP         EU          FC        FD             FE  \\\n",
       "0   1   30.879420  78.526968   3.828384   13.394640  10.265073   9028.291921   \n",
       "1   0  109.125159  95.415086  52.260480   17.175984   0.296850   6785.003474   \n",
       "2   1  109.125159  78.526968   5.390628  224.207424   8.745201   8338.906181   \n",
       "3   1   31.674357  78.526968  31.323372   59.301984   7.884336  10965.766040   \n",
       "4   1  109.125159  91.994825  51.141336   29.102640   4.274640  16198.049590   \n",
       "\n",
       "          FI        FL        FR        FS         GB          GE  \\\n",
       "0   3.583450  7.298162   1.73855  0.094822  11.339138   72.611063   \n",
       "1  10.358927  0.173229   0.49706  0.568932   9.292698   72.611063   \n",
       "2  11.626917  7.709560   0.97556  1.198821  37.077772   88.609437   \n",
       "3  14.852022  6.122162   0.49706  0.284466  18.529584   82.416803   \n",
       "4  13.666727  8.153058  48.50134  0.121914  16.408728  146.109943   \n",
       "\n",
       "             GF         GH         GI         GL  Class  \n",
       "0   2003.810319  22.136229  69.834944   0.120343      1  \n",
       "1  27981.562750  29.135430  32.131996  21.978000      0  \n",
       "2  13676.957810  28.022851  35.192676   0.196941      0  \n",
       "3   2094.262452  39.948656  90.493248   0.155829      0  \n",
       "4   8524.370502  45.381316  36.262628   0.096614      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN', 'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS', 'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY', 'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI', 'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL']\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "target = \"Class\"\n",
    "random_state = 13\n",
    "n_splits = 10\n",
    "num_boost_round = 1000\n",
    "stopping_rounds = 50\n",
    "learning_rate = 0.1\n",
    "n_trials = 300\n",
    "model_name = \"XGBoost\"\n",
    "\n",
    "df_train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n",
    "df_sub = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "df_greeks = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/greeks.csv\")\n",
    "\n",
    "\n",
    "# FE\n",
    "df_train[\"EJ\"] = df_train[\"EJ\"].replace({'A':0, 'B':1})\n",
    "df_test[\"EJ\"] = df_test[\"EJ\"].replace({'A':0, 'B':1})\n",
    "\n",
    "# EX\n",
    "ex_columns = ['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n",
    "       'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n",
    "       'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
    "       'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n",
    "       'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL'] # All\n",
    "\n",
    "exclude_columns = []\n",
    "\n",
    "ex_columns = sorted(list(set(ex_columns) - set(exclude_columns)))\n",
    "use_columns = ex_columns + [target]\n",
    "X_sub = df_test[ex_columns]\n",
    "\n",
    "display(df_train.head())\n",
    "\n",
    "print(ex_columns)\n",
    "print(len(ex_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f52fe75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:49.406080Z",
     "iopub.status.busy": "2023-08-10T15:10:49.405773Z",
     "iopub.status.idle": "2023-08-10T15:10:49.418186Z",
     "shell.execute_reply": "2023-08-10T15:10:49.417362Z"
    },
    "papermill": {
     "duration": 0.048917,
     "end_time": "2023-08-10T15:10:49.420131",
     "exception": false,
     "start_time": "2023-08-10T15:10:49.371214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_cv_params = [{'min_child_weight': 8,\n",
    "  'max_depth': 17,\n",
    "  'max_delta_step': 7.428803969367076,\n",
    "  'subsample': 0.7920569357442612,\n",
    "  'colsample_bytree': 0.9731010296239732,\n",
    "  'gamma': 0.011371581333620819,\n",
    "  'scale_pos_weight': 155.09900324714164},\n",
    " {'min_child_weight': 6,\n",
    "  'max_depth': 12,\n",
    "  'max_delta_step': 18.09220194779129,\n",
    "  'subsample': 0.7010637164370721,\n",
    "  'colsample_bytree': 0.6202802949673988,\n",
    "  'gamma': 2.4276207158251635e-06,\n",
    "  'scale_pos_weight': 77.05477788071082},\n",
    " {'min_child_weight': 7,\n",
    "  'max_depth': 44,\n",
    "  'max_delta_step': 8.607817932565986,\n",
    "  'subsample': 0.8318320589603841,\n",
    "  'colsample_bytree': 0.4355186701914686,\n",
    "  'gamma': 1.2487845561482165e-05,\n",
    "  'scale_pos_weight': 201.21791081062247},\n",
    " {'min_child_weight': 9,\n",
    "  'max_depth': 6,\n",
    "  'max_delta_step': 8.555054870917278,\n",
    "  'subsample': 0.7113482335377683,\n",
    "  'colsample_bytree': 0.6121685564892682,\n",
    "  'gamma': 2.319644405596966e-05,\n",
    "  'scale_pos_weight': 102.41277731980534},\n",
    " {'min_child_weight': 7,\n",
    "  'max_depth': 21,\n",
    "  'max_delta_step': 7.783986712189754,\n",
    "  'subsample': 0.6538816038301629,\n",
    "  'colsample_bytree': 0.7153539636716781,\n",
    "  'gamma': 7.734810091814072e-06,\n",
    "  'scale_pos_weight': 71.05476915193654},\n",
    " {'min_child_weight': 7,\n",
    "  'max_depth': 44,\n",
    "  'max_delta_step': 11.576507464953075,\n",
    "  'subsample': 0.6615916949825313,\n",
    "  'colsample_bytree': 0.3391423869190513,\n",
    "  'gamma': 0.0004551091955707892,\n",
    "  'scale_pos_weight': 160.8558351721162},\n",
    " {'min_child_weight': 8,\n",
    "  'max_depth': 29,\n",
    "  'max_delta_step': 7.538637784459595,\n",
    "  'subsample': 0.9107151436938841,\n",
    "  'colsample_bytree': 0.8897571647738725,\n",
    "  'gamma': 2.0506808841691723e-09,\n",
    "  'scale_pos_weight': 84.37760083879326},\n",
    " {'min_child_weight': 7,\n",
    "  'max_depth': 35,\n",
    "  'max_delta_step': 12.653663000448985,\n",
    "  'subsample': 0.7472661201571688,\n",
    "  'colsample_bytree': 0.4013032545066121,\n",
    "  'gamma': 0.25500762211520384,\n",
    "  'scale_pos_weight': 172.97973668960879},\n",
    " {'min_child_weight': 7,\n",
    "  'max_depth': 47,\n",
    "  'max_delta_step': 10.32403314731059,\n",
    "  'subsample': 0.6548989327197157,\n",
    "  'colsample_bytree': 0.9084438440111091,\n",
    "  'gamma': 6.347715018571132e-07,\n",
    "  'scale_pos_weight': 42.15400063527052},\n",
    " {'min_child_weight': 5,\n",
    "  'max_depth': 36,\n",
    "  'max_delta_step': 2.665744246392611,\n",
    "  'subsample': 0.9501494253117905,\n",
    "  'colsample_bytree': 0.516559545007966,\n",
    "  'gamma': 5.623009240626623e-07,\n",
    "  'scale_pos_weight': 296.4576172851205}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a39c481f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:10:49.467101Z",
     "iopub.status.busy": "2023-08-10T15:10:49.466294Z",
     "iopub.status.idle": "2023-08-10T15:11:00.253086Z",
     "shell.execute_reply": "2023-08-10T15:11:00.252185Z"
    },
    "papermill": {
     "duration": 10.812928,
     "end_time": "2023-08-10T15:11:00.255842",
     "exception": false,
     "start_time": "2023-08-10T15:10:49.442914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Out CV 0 *****\n",
      "***** Out CV 1 *****\n",
      "***** Out CV 2 *****\n",
      "***** Out CV 3 *****\n",
      "***** Out CV 4 *****\n",
      "***** Out CV 5 *****\n",
      "***** Out CV 6 *****\n",
      "***** Out CV 7 *****\n",
      "***** Out CV 8 *****\n",
      "***** Out CV 9 *****\n"
     ]
    }
   ],
   "source": [
    "outer_cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "X, y = df_train[ex_columns], df_train[target]\n",
    "model2_preds = df_sub[[\"Id\"]].copy()\n",
    "scores = []\n",
    "model2_oof = np.zeros(len(y))\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# CV\n",
    "for i, (train_inds, test_inds) in enumerate(outer_cv.split(X, y)):\n",
    "    print(f\"***** Out CV {i} *****\")\n",
    "    X_train_out, y_train_out = X.iloc[train_inds], y.iloc[train_inds]\n",
    "    X_test, y_test = X.iloc[test_inds], y.iloc[test_inds]\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    param = {**base_params_dict[model_name], **best_cv_params[i]}\n",
    "\n",
    "    # X_test\n",
    "    data_train = xgb.DMatrix(X_train_out, y_train_out)\n",
    "    data_test = xgb.DMatrix(X_test, y_test)\n",
    "    data_sub = xgb.DMatrix(X_sub)\n",
    "    evals_result = {}\n",
    "    model = xgb.train(\n",
    "        param,\n",
    "        dtrain=data_train,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=[(data_train, \"train\"), (data_test, \"val\")],\n",
    "        early_stopping_rounds=stopping_rounds,\n",
    "        verbose_eval=False,\n",
    "        evals_result=evals_result\n",
    "    )\n",
    "    y_pred = model.predict(data_test)\n",
    "    score = metric(y_test, y_pred)\n",
    "    scores.append(score)\n",
    "    model2_oof[test_inds] = y_pred\n",
    "    # \n",
    "    pred = model.predict(data_sub)\n",
    "    model2_preds[f\"pred_{i+1}\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72e6c208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:11:00.308159Z",
     "iopub.status.busy": "2023-08-10T15:11:00.307839Z",
     "iopub.status.idle": "2023-08-10T15:11:00.313011Z",
     "shell.execute_reply": "2023-08-10T15:11:00.312131Z"
    },
    "papermill": {
     "duration": 0.032078,
     "end_time": "2023-08-10T15:11:00.315276",
     "exception": false,
     "start_time": "2023-08-10T15:11:00.283198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score mean: 0.1859770658729706, Score var: 0.02545220655889065\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score mean: {np.mean(scores)}, Score var: {np.var(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14bf9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:11:00.364802Z",
     "iopub.status.busy": "2023-08-10T15:11:00.364120Z",
     "iopub.status.idle": "2023-08-10T15:11:00.384872Z",
     "shell.execute_reply": "2023-08-10T15:11:00.383980Z"
    },
    "papermill": {
     "duration": 0.046973,
     "end_time": "2023-08-10T15:11:00.387086",
     "exception": false,
     "start_time": "2023-08-10T15:11:00.340113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>pred_4</th>\n",
       "      <th>pred_5</th>\n",
       "      <th>pred_6</th>\n",
       "      <th>pred_7</th>\n",
       "      <th>pred_8</th>\n",
       "      <th>pred_9</th>\n",
       "      <th>pred_10</th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.233156</td>\n",
       "      <td>0.467711</td>\n",
       "      <td>0.134117</td>\n",
       "      <td>0.429134</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.285808</td>\n",
       "      <td>0.349782</td>\n",
       "      <td>0.191995</td>\n",
       "      <td>0.310409</td>\n",
       "      <td>0.232898</td>\n",
       "      <td>0.282155</td>\n",
       "      <td>0.717845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.233156</td>\n",
       "      <td>0.467711</td>\n",
       "      <td>0.134117</td>\n",
       "      <td>0.429134</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.285808</td>\n",
       "      <td>0.349782</td>\n",
       "      <td>0.191995</td>\n",
       "      <td>0.310409</td>\n",
       "      <td>0.232898</td>\n",
       "      <td>0.282155</td>\n",
       "      <td>0.717845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.233156</td>\n",
       "      <td>0.467711</td>\n",
       "      <td>0.134117</td>\n",
       "      <td>0.429134</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.285808</td>\n",
       "      <td>0.349782</td>\n",
       "      <td>0.191995</td>\n",
       "      <td>0.310409</td>\n",
       "      <td>0.232898</td>\n",
       "      <td>0.282155</td>\n",
       "      <td>0.717845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.233156</td>\n",
       "      <td>0.467711</td>\n",
       "      <td>0.134117</td>\n",
       "      <td>0.429134</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.285808</td>\n",
       "      <td>0.349782</td>\n",
       "      <td>0.191995</td>\n",
       "      <td>0.310409</td>\n",
       "      <td>0.232898</td>\n",
       "      <td>0.282155</td>\n",
       "      <td>0.717845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.233156</td>\n",
       "      <td>0.467711</td>\n",
       "      <td>0.134117</td>\n",
       "      <td>0.429134</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.285808</td>\n",
       "      <td>0.349782</td>\n",
       "      <td>0.191995</td>\n",
       "      <td>0.310409</td>\n",
       "      <td>0.232898</td>\n",
       "      <td>0.282155</td>\n",
       "      <td>0.717845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id    pred_1    pred_2    pred_3    pred_4    pred_5    pred_6  \\\n",
       "0  00eed32682bb  0.233156  0.467711  0.134117  0.429134  0.186543  0.285808   \n",
       "1  010ebe33f668  0.233156  0.467711  0.134117  0.429134  0.186543  0.285808   \n",
       "2  02fa521e1838  0.233156  0.467711  0.134117  0.429134  0.186543  0.285808   \n",
       "3  040e15f562a2  0.233156  0.467711  0.134117  0.429134  0.186543  0.285808   \n",
       "4  046e85c7cc7f  0.233156  0.467711  0.134117  0.429134  0.186543  0.285808   \n",
       "\n",
       "     pred_7    pred_8    pred_9   pred_10   class_1   class_0  \n",
       "0  0.349782  0.191995  0.310409  0.232898  0.282155  0.717845  \n",
       "1  0.349782  0.191995  0.310409  0.232898  0.282155  0.717845  \n",
       "2  0.349782  0.191995  0.310409  0.232898  0.282155  0.717845  \n",
       "3  0.349782  0.191995  0.310409  0.232898  0.282155  0.717845  \n",
       "4  0.349782  0.191995  0.310409  0.232898  0.282155  0.717845  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_preds[\"class_1\"] = model2_preds.drop(\"Id\", axis=1).mean(axis=1).tolist()\n",
    "model2_preds[\"class_0\"] = (1 - model2_preds[\"class_1\"]).tolist()\n",
    "model2_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bac19d",
   "metadata": {
    "papermill": {
     "duration": 0.023541,
     "end_time": "2023-08-10T15:11:00.435341",
     "exception": false,
     "start_time": "2023-08-10T15:11:00.411800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7eed855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:11:00.483597Z",
     "iopub.status.busy": "2023-08-10T15:11:00.483337Z",
     "iopub.status.idle": "2023-08-10T15:11:00.560111Z",
     "shell.execute_reply": "2023-08-10T15:11:00.559244Z"
    },
    "papermill": {
     "duration": 0.103634,
     "end_time": "2023-08-10T15:11:00.562437",
     "exception": false,
     "start_time": "2023-08-10T15:11:00.458803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>BN</th>\n",
       "      <th>BP</th>\n",
       "      <th>BQ</th>\n",
       "      <th>BR</th>\n",
       "      <th>BZ</th>\n",
       "      <th>CB</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>CF</th>\n",
       "      <th>CH</th>\n",
       "      <th>CL</th>\n",
       "      <th>CR</th>\n",
       "      <th>CS</th>\n",
       "      <th>CU</th>\n",
       "      <th>CW</th>\n",
       "      <th>DA</th>\n",
       "      <th>DE</th>\n",
       "      <th>DF</th>\n",
       "      <th>DH</th>\n",
       "      <th>DI</th>\n",
       "      <th>DL</th>\n",
       "      <th>DN</th>\n",
       "      <th>DU</th>\n",
       "      <th>DV</th>\n",
       "      <th>DY</th>\n",
       "      <th>EB</th>\n",
       "      <th>EE</th>\n",
       "      <th>EG</th>\n",
       "      <th>EH</th>\n",
       "      <th>EJ</th>\n",
       "      <th>EL</th>\n",
       "      <th>EP</th>\n",
       "      <th>EU</th>\n",
       "      <th>FC</th>\n",
       "      <th>FD</th>\n",
       "      <th>FE</th>\n",
       "      <th>FI</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000ff2bfdfe9</td>\n",
       "      <td>0.209377</td>\n",
       "      <td>3109.03329</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.812214</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>4126.58731</td>\n",
       "      <td>22.5984</td>\n",
       "      <td>175.638726</td>\n",
       "      <td>152.707705</td>\n",
       "      <td>823.928241</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>47.223358</td>\n",
       "      <td>0.563481</td>\n",
       "      <td>23.387600</td>\n",
       "      <td>4.851915</td>\n",
       "      <td>0.023482</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.069225</td>\n",
       "      <td>13.784111</td>\n",
       "      <td>1.302012</td>\n",
       "      <td>36.205956</td>\n",
       "      <td>69.08340</td>\n",
       "      <td>295.570575</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.284232</td>\n",
       "      <td>89.245560</td>\n",
       "      <td>84.31664</td>\n",
       "      <td>29.657104</td>\n",
       "      <td>5.310690</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>23.187704</td>\n",
       "      <td>7.294176</td>\n",
       "      <td>1.987283</td>\n",
       "      <td>1433.166750</td>\n",
       "      <td>0.949104</td>\n",
       "      <td>1</td>\n",
       "      <td>30.879420</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>3.828384</td>\n",
       "      <td>13.394640</td>\n",
       "      <td>10.265073</td>\n",
       "      <td>9028.291921</td>\n",
       "      <td>3.583450</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>11.339138</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>22.136229</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007255e47698</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>978.76416</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.632190</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>13.517790</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5496.92824</td>\n",
       "      <td>19.4205</td>\n",
       "      <td>155.868030</td>\n",
       "      <td>14.754720</td>\n",
       "      <td>51.216883</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>30.284345</td>\n",
       "      <td>0.484710</td>\n",
       "      <td>50.628208</td>\n",
       "      <td>6.085041</td>\n",
       "      <td>0.031442</td>\n",
       "      <td>1.113875</td>\n",
       "      <td>1.117800</td>\n",
       "      <td>28.310953</td>\n",
       "      <td>1.357182</td>\n",
       "      <td>37.476568</td>\n",
       "      <td>70.79836</td>\n",
       "      <td>178.553100</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.363489</td>\n",
       "      <td>110.581815</td>\n",
       "      <td>75.74548</td>\n",
       "      <td>37.532000</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>17.222328</td>\n",
       "      <td>4.926396</td>\n",
       "      <td>0.858603</td>\n",
       "      <td>1111.287150</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>95.415086</td>\n",
       "      <td>52.260480</td>\n",
       "      <td>17.175984</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>6785.003474</td>\n",
       "      <td>10.358927</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>9.292698</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>29.135430</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>013f2bd269f5</td>\n",
       "      <td>0.470030</td>\n",
       "      <td>2635.10654</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>6.732840</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.824570</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5135.78024</td>\n",
       "      <td>26.4825</td>\n",
       "      <td>128.988531</td>\n",
       "      <td>219.320160</td>\n",
       "      <td>482.141594</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>32.563713</td>\n",
       "      <td>0.495852</td>\n",
       "      <td>85.955376</td>\n",
       "      <td>5.376488</td>\n",
       "      <td>0.036218</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.700350</td>\n",
       "      <td>39.364743</td>\n",
       "      <td>1.009611</td>\n",
       "      <td>21.459644</td>\n",
       "      <td>70.81970</td>\n",
       "      <td>321.426625</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.210441</td>\n",
       "      <td>120.056438</td>\n",
       "      <td>65.46984</td>\n",
       "      <td>28.053464</td>\n",
       "      <td>1.289739</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>36.861352</td>\n",
       "      <td>7.813674</td>\n",
       "      <td>8.146651</td>\n",
       "      <td>1494.076488</td>\n",
       "      <td>0.377208</td>\n",
       "      <td>1</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>5.390628</td>\n",
       "      <td>224.207424</td>\n",
       "      <td>8.745201</td>\n",
       "      <td>8338.906181</td>\n",
       "      <td>11.626917</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>37.077772</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>28.022851</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>043ac50845d5</td>\n",
       "      <td>0.252107</td>\n",
       "      <td>3819.65177</td>\n",
       "      <td>120.201618</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.685344</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>11.053708</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4169.67738</td>\n",
       "      <td>23.6577</td>\n",
       "      <td>237.282264</td>\n",
       "      <td>11.050410</td>\n",
       "      <td>661.518640</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>15.201914</td>\n",
       "      <td>0.717882</td>\n",
       "      <td>88.159360</td>\n",
       "      <td>2.347652</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>1.400300</td>\n",
       "      <td>0.636075</td>\n",
       "      <td>41.116960</td>\n",
       "      <td>0.722727</td>\n",
       "      <td>21.530392</td>\n",
       "      <td>47.27586</td>\n",
       "      <td>196.607985</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.292431</td>\n",
       "      <td>139.824570</td>\n",
       "      <td>71.57120</td>\n",
       "      <td>24.354856</td>\n",
       "      <td>2.655345</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>52.003884</td>\n",
       "      <td>7.386060</td>\n",
       "      <td>3.813326</td>\n",
       "      <td>15691.552180</td>\n",
       "      <td>0.614484</td>\n",
       "      <td>1</td>\n",
       "      <td>31.674357</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>31.323372</td>\n",
       "      <td>59.301984</td>\n",
       "      <td>7.884336</td>\n",
       "      <td>10965.766040</td>\n",
       "      <td>14.852022</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>18.529584</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>39.948656</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>044fb8a146ec</td>\n",
       "      <td>0.380297</td>\n",
       "      <td>3733.04844</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.942255</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>5728.73412</td>\n",
       "      <td>24.0108</td>\n",
       "      <td>324.546318</td>\n",
       "      <td>149.717165</td>\n",
       "      <td>6074.859475</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>82.213495</td>\n",
       "      <td>0.536467</td>\n",
       "      <td>72.644264</td>\n",
       "      <td>30.537722</td>\n",
       "      <td>0.025472</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.693150</td>\n",
       "      <td>31.724726</td>\n",
       "      <td>0.827550</td>\n",
       "      <td>34.415360</td>\n",
       "      <td>74.06532</td>\n",
       "      <td>200.178160</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.207708</td>\n",
       "      <td>97.920120</td>\n",
       "      <td>52.83888</td>\n",
       "      <td>26.019912</td>\n",
       "      <td>1.144902</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>9.064856</td>\n",
       "      <td>7.350720</td>\n",
       "      <td>3.490846</td>\n",
       "      <td>1403.656300</td>\n",
       "      <td>0.164268</td>\n",
       "      <td>1</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>91.994825</td>\n",
       "      <td>51.141336</td>\n",
       "      <td>29.102640</td>\n",
       "      <td>4.274640</td>\n",
       "      <td>16198.049590</td>\n",
       "      <td>13.666727</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>16.408728</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>45.381316</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id        AB          AF          AH         AM        AR  \\\n",
       "0  000ff2bfdfe9  0.209377  3109.03329   85.200147  22.394407  8.138688   \n",
       "1  007255e47698  0.145282   978.76416   85.200147  36.968889  8.138688   \n",
       "2  013f2bd269f5  0.470030  2635.10654   85.200147  32.360553  8.138688   \n",
       "3  043ac50845d5  0.252107  3819.65177  120.201618  77.112203  8.138688   \n",
       "4  044fb8a146ec  0.380297  3733.04844   85.200147  14.103738  8.138688   \n",
       "\n",
       "         AX        AY         AZ          BC         BD        BN          BP  \\\n",
       "0  0.699861  0.025578   9.812214    5.555634  4126.58731  22.5984  175.638726   \n",
       "1  3.632190  0.025578  13.517790    1.229900  5496.92824  19.4205  155.868030   \n",
       "2  6.732840  0.025578  12.824570    1.229900  5135.78024  26.4825  128.988531   \n",
       "3  3.685344  0.025578  11.053708    1.229900  4169.67738  23.6577  237.282264   \n",
       "4  3.942255  0.054810   3.396778  102.151980  5728.73412  24.0108  324.546318   \n",
       "\n",
       "           BQ           BR          BZ         CB        CC        CD   \\\n",
       "0  152.707705   823.928241  257.432377  47.223358  0.563481  23.387600   \n",
       "1   14.754720    51.216883  257.432377  30.284345  0.484710  50.628208   \n",
       "2  219.320160   482.141594  257.432377  32.563713  0.495852  85.955376   \n",
       "3   11.050410   661.518640  257.432377  15.201914  0.717882  88.159360   \n",
       "4  149.717165  6074.859475  257.432377  82.213495  0.536467  72.644264   \n",
       "\n",
       "          CF        CH        CL        CR         CS        CU        CW   \\\n",
       "0   4.851915  0.023482  1.050225  0.069225  13.784111  1.302012  36.205956   \n",
       "1   6.085041  0.031442  1.113875  1.117800  28.310953  1.357182  37.476568   \n",
       "2   5.376488  0.036218  1.050225  0.700350  39.364743  1.009611  21.459644   \n",
       "3   2.347652  0.029054  1.400300  0.636075  41.116960  0.722727  21.530392   \n",
       "4  30.537722  0.025472  1.050225  0.693150  31.724726  0.827550  34.415360   \n",
       "\n",
       "         DA          DE       DF        DH          DI        DL         DN  \\\n",
       "0  69.08340  295.570575  0.23868  0.284232   89.245560  84.31664  29.657104   \n",
       "1  70.79836  178.553100  0.23868  0.363489  110.581815  75.74548  37.532000   \n",
       "2  70.81970  321.426625  0.23868  0.210441  120.056438  65.46984  28.053464   \n",
       "3  47.27586  196.607985  0.23868  0.292431  139.824570  71.57120  24.354856   \n",
       "4  74.06532  200.178160  0.23868  0.207708   97.920120  52.83888  26.019912   \n",
       "\n",
       "         DU       DV         DY        EB        EE            EG        EH  \\\n",
       "0  5.310690  1.74307  23.187704  7.294176  1.987283   1433.166750  0.949104   \n",
       "1  0.005518  1.74307  17.222328  4.926396  0.858603   1111.287150  0.003042   \n",
       "2  1.289739  1.74307  36.861352  7.813674  8.146651   1494.076488  0.377208   \n",
       "3  2.655345  1.74307  52.003884  7.386060  3.813326  15691.552180  0.614484   \n",
       "4  1.144902  1.74307   9.064856  7.350720  3.490846   1403.656300  0.164268   \n",
       "\n",
       "   EJ          EL         EP         EU          FC        FD             FE  \\\n",
       "0   1   30.879420  78.526968   3.828384   13.394640  10.265073   9028.291921   \n",
       "1   0  109.125159  95.415086  52.260480   17.175984   0.296850   6785.003474   \n",
       "2   1  109.125159  78.526968   5.390628  224.207424   8.745201   8338.906181   \n",
       "3   1   31.674357  78.526968  31.323372   59.301984   7.884336  10965.766040   \n",
       "4   1  109.125159  91.994825  51.141336   29.102640   4.274640  16198.049590   \n",
       "\n",
       "          FI        FL        FR        FS         GB          GE  \\\n",
       "0   3.583450  7.298162   1.73855  0.094822  11.339138   72.611063   \n",
       "1  10.358927  0.173229   0.49706  0.568932   9.292698   72.611063   \n",
       "2  11.626917  7.709560   0.97556  1.198821  37.077772   88.609437   \n",
       "3  14.852022  6.122162   0.49706  0.284466  18.529584   82.416803   \n",
       "4  13.666727  8.153058  48.50134  0.121914  16.408728  146.109943   \n",
       "\n",
       "             GF         GH         GI         GL  Class  \n",
       "0   2003.810319  22.136229  69.834944   0.120343      1  \n",
       "1  27981.562750  29.135430  32.131996  21.978000      0  \n",
       "2  13676.957810  28.022851  35.192676   0.196941      0  \n",
       "3   2094.262452  39.948656  90.493248   0.155829      0  \n",
       "4   8524.370502  45.381316  36.262628   0.096614      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN', 'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS', 'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY', 'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI', 'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL']\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "random_state = 13\n",
    "n_splits = 10\n",
    "n_repeats = 10\n",
    "\n",
    "df_train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n",
    "df_sub = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "df_greeks = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/greeks.csv\")\n",
    "\n",
    "# FE\n",
    "df_train[\"EJ\"] = df_train[\"EJ\"].replace({'A':0, 'B':1})\n",
    "df_test[\"EJ\"] = df_test[\"EJ\"].replace({'A':0, 'B':1})\n",
    "\n",
    "ex_columns = ['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n",
    "       'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n",
    "       'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
    "       'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n",
    "       'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL']\n",
    "exclude_columns = []\n",
    "ex_columns = sorted(list(set(ex_columns) - set(exclude_columns)))\n",
    "use_columns = ex_columns + [target]\n",
    "X_sub = df_test[ex_columns]\n",
    "\n",
    "display(df_train.head())\n",
    "\n",
    "print(ex_columns)\n",
    "print(len(ex_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b53923b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:11:00.613965Z",
     "iopub.status.busy": "2023-08-10T15:11:00.613209Z",
     "iopub.status.idle": "2023-08-10T15:16:27.528099Z",
     "shell.execute_reply": "2023-08-10T15:16:27.525799Z"
    },
    "papermill": {
     "duration": 326.942191,
     "end_time": "2023-08-10T15:16:27.530403",
     "exception": false,
     "start_time": "2023-08-10T15:11:00.588212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== START FOLD 1 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 2 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 3 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 4 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 5 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 6 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 7 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 8 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 9 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 10 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 11 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 12 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 13 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 14 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 15 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 16 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 17 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 18 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 19 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 20 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 21 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 22 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 23 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 24 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 25 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 26 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 27 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 28 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 29 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 30 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 31 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 32 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 33 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 34 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 35 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 36 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 37 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 38 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 39 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 40 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 41 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 42 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 43 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 44 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 45 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 46 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 47 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 48 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 49 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 50 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 51 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 52 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 53 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 54 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 55 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 56 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 57 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 58 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 59 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 60 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 61 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 62 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 63 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 64 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 65 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 66 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 67 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 68 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 69 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 70 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 71 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 72 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 73 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 74 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 75 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 76 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 77 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 78 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 79 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 80 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 81 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 82 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 83 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 84 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 85 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 86 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 87 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 88 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 89 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 90 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 91 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 92 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 93 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 94 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 95 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 96 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 97 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 98 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 99 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "==================== START FOLD 100 ====================\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/4129957716.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  model3_preds[f\"pred_{i+1}\"] = pred\n"
     ]
    }
   ],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state+111)\n",
    "df_preds = df_sub[[\"Id\"]].copy()\n",
    "scores = []\n",
    "\n",
    "y = df_train[target]\n",
    "X = df_train[ex_columns]\n",
    "\n",
    "model3_preds = df_sub[[\"Id\"]].copy()\n",
    "model3_oof = np.zeros(len(y))\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(cv.split(X, y)):\n",
    "    print(\"=\" * 20, f\"START FOLD {i+1}\", \"=\" * 20)\n",
    "    y_train = y.iloc[train_index].to_numpy()\n",
    "    X_train = X.iloc[train_index].to_numpy()\n",
    "    y_val = y.iloc[val_index].to_numpy()\n",
    "    X_val = X.iloc[val_index].to_numpy()\n",
    "\n",
    "    model = TabPFNClassifier(N_ensemble_configurations=64, device='cuda:0')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict_proba(X_val)[:, 1]\n",
    "    score = metric(y_val, y_pred)\n",
    "    scores.append(score)\n",
    "    model3_oof[val_index] += y_pred / n_repeats\n",
    "    \n",
    "    # \n",
    "    pred = model.predict_proba(X_sub)[:, 1]\n",
    "    model3_preds[f\"pred_{i+1}\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4eb2115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:16:27.612100Z",
     "iopub.status.busy": "2023-08-10T15:16:27.611183Z",
     "iopub.status.idle": "2023-08-10T15:16:27.618480Z",
     "shell.execute_reply": "2023-08-10T15:16:27.617386Z"
    },
    "papermill": {
     "duration": 0.049434,
     "end_time": "2023-08-10T15:16:27.620493",
     "exception": false,
     "start_time": "2023-08-10T15:16:27.571059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score mean: 0.3771251063073964\n",
      "oof Score mean: 0.3690115966232314\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score mean: {np.mean(scores)}\")\n",
    "print(f\"oof Score mean: {metric(df_train[target], model3_oof)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75451049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:16:27.702668Z",
     "iopub.status.busy": "2023-08-10T15:16:27.702358Z",
     "iopub.status.idle": "2023-08-10T15:16:27.803186Z",
     "shell.execute_reply": "2023-08-10T15:16:27.802274Z"
    },
    "papermill": {
     "duration": 0.144613,
     "end_time": "2023-08-10T15:16:27.805149",
     "exception": false,
     "start_time": "2023-08-10T15:16:27.660536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1708353678.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  model3_preds[\"class_1\"] = model3_preds.drop(\"Id\", axis=1).mean(axis=1).tolist()\n",
      "/tmp/ipykernel_23/1708353678.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  model3_preds[\"class_0\"] = (1 - model3_preds[\"class_1\"]).tolist()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>pred_4</th>\n",
       "      <th>pred_5</th>\n",
       "      <th>pred_6</th>\n",
       "      <th>pred_7</th>\n",
       "      <th>pred_8</th>\n",
       "      <th>pred_9</th>\n",
       "      <th>pred_10</th>\n",
       "      <th>pred_11</th>\n",
       "      <th>pred_12</th>\n",
       "      <th>pred_13</th>\n",
       "      <th>pred_14</th>\n",
       "      <th>pred_15</th>\n",
       "      <th>pred_16</th>\n",
       "      <th>pred_17</th>\n",
       "      <th>pred_18</th>\n",
       "      <th>pred_19</th>\n",
       "      <th>pred_20</th>\n",
       "      <th>pred_21</th>\n",
       "      <th>pred_22</th>\n",
       "      <th>pred_23</th>\n",
       "      <th>pred_24</th>\n",
       "      <th>pred_25</th>\n",
       "      <th>pred_26</th>\n",
       "      <th>pred_27</th>\n",
       "      <th>pred_28</th>\n",
       "      <th>pred_29</th>\n",
       "      <th>pred_30</th>\n",
       "      <th>pred_31</th>\n",
       "      <th>pred_32</th>\n",
       "      <th>pred_33</th>\n",
       "      <th>pred_34</th>\n",
       "      <th>pred_35</th>\n",
       "      <th>pred_36</th>\n",
       "      <th>pred_37</th>\n",
       "      <th>pred_38</th>\n",
       "      <th>pred_39</th>\n",
       "      <th>pred_40</th>\n",
       "      <th>pred_41</th>\n",
       "      <th>pred_42</th>\n",
       "      <th>pred_43</th>\n",
       "      <th>pred_44</th>\n",
       "      <th>pred_45</th>\n",
       "      <th>pred_46</th>\n",
       "      <th>pred_47</th>\n",
       "      <th>pred_48</th>\n",
       "      <th>pred_49</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_53</th>\n",
       "      <th>pred_54</th>\n",
       "      <th>pred_55</th>\n",
       "      <th>pred_56</th>\n",
       "      <th>pred_57</th>\n",
       "      <th>pred_58</th>\n",
       "      <th>pred_59</th>\n",
       "      <th>pred_60</th>\n",
       "      <th>pred_61</th>\n",
       "      <th>pred_62</th>\n",
       "      <th>pred_63</th>\n",
       "      <th>pred_64</th>\n",
       "      <th>pred_65</th>\n",
       "      <th>pred_66</th>\n",
       "      <th>pred_67</th>\n",
       "      <th>pred_68</th>\n",
       "      <th>pred_69</th>\n",
       "      <th>pred_70</th>\n",
       "      <th>pred_71</th>\n",
       "      <th>pred_72</th>\n",
       "      <th>pred_73</th>\n",
       "      <th>pred_74</th>\n",
       "      <th>pred_75</th>\n",
       "      <th>pred_76</th>\n",
       "      <th>pred_77</th>\n",
       "      <th>pred_78</th>\n",
       "      <th>pred_79</th>\n",
       "      <th>pred_80</th>\n",
       "      <th>pred_81</th>\n",
       "      <th>pred_82</th>\n",
       "      <th>pred_83</th>\n",
       "      <th>pred_84</th>\n",
       "      <th>pred_85</th>\n",
       "      <th>pred_86</th>\n",
       "      <th>pred_87</th>\n",
       "      <th>pred_88</th>\n",
       "      <th>pred_89</th>\n",
       "      <th>pred_90</th>\n",
       "      <th>pred_91</th>\n",
       "      <th>pred_92</th>\n",
       "      <th>pred_93</th>\n",
       "      <th>pred_94</th>\n",
       "      <th>pred_95</th>\n",
       "      <th>pred_96</th>\n",
       "      <th>pred_97</th>\n",
       "      <th>pred_98</th>\n",
       "      <th>pred_99</th>\n",
       "      <th>pred_100</th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.486626</td>\n",
       "      <td>0.266114</td>\n",
       "      <td>0.329985</td>\n",
       "      <td>0.381091</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.343325</td>\n",
       "      <td>0.332939</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>0.35793</td>\n",
       "      <td>0.339916</td>\n",
       "      <td>0.374096</td>\n",
       "      <td>0.336846</td>\n",
       "      <td>0.369171</td>\n",
       "      <td>0.327333</td>\n",
       "      <td>0.368727</td>\n",
       "      <td>0.327148</td>\n",
       "      <td>0.315132</td>\n",
       "      <td>0.329116</td>\n",
       "      <td>0.352492</td>\n",
       "      <td>0.395083</td>\n",
       "      <td>0.273321</td>\n",
       "      <td>0.392576</td>\n",
       "      <td>0.329311</td>\n",
       "      <td>0.352917</td>\n",
       "      <td>0.451257</td>\n",
       "      <td>0.403674</td>\n",
       "      <td>0.327206</td>\n",
       "      <td>0.336845</td>\n",
       "      <td>0.31011</td>\n",
       "      <td>0.319097</td>\n",
       "      <td>0.329701</td>\n",
       "      <td>0.35433</td>\n",
       "      <td>0.399262</td>\n",
       "      <td>0.292417</td>\n",
       "      <td>0.371391</td>\n",
       "      <td>0.43788</td>\n",
       "      <td>0.268418</td>\n",
       "      <td>0.348067</td>\n",
       "      <td>0.300686</td>\n",
       "      <td>0.359076</td>\n",
       "      <td>0.328841</td>\n",
       "      <td>0.389331</td>\n",
       "      <td>0.477851</td>\n",
       "      <td>0.340024</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.313341</td>\n",
       "      <td>0.348723</td>\n",
       "      <td>0.295654</td>\n",
       "      <td>0.334055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360403</td>\n",
       "      <td>0.361425</td>\n",
       "      <td>0.311121</td>\n",
       "      <td>0.330622</td>\n",
       "      <td>0.288767</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.356378</td>\n",
       "      <td>0.347843</td>\n",
       "      <td>0.295498</td>\n",
       "      <td>0.363151</td>\n",
       "      <td>0.288831</td>\n",
       "      <td>0.323705</td>\n",
       "      <td>0.36124</td>\n",
       "      <td>0.384168</td>\n",
       "      <td>0.337279</td>\n",
       "      <td>0.325347</td>\n",
       "      <td>0.411305</td>\n",
       "      <td>0.395288</td>\n",
       "      <td>0.294686</td>\n",
       "      <td>0.466519</td>\n",
       "      <td>0.367848</td>\n",
       "      <td>0.27515</td>\n",
       "      <td>0.3349</td>\n",
       "      <td>0.35515</td>\n",
       "      <td>0.304164</td>\n",
       "      <td>0.332547</td>\n",
       "      <td>0.318042</td>\n",
       "      <td>0.390003</td>\n",
       "      <td>0.257252</td>\n",
       "      <td>0.347721</td>\n",
       "      <td>0.32725</td>\n",
       "      <td>0.314639</td>\n",
       "      <td>0.371706</td>\n",
       "      <td>0.415569</td>\n",
       "      <td>0.408612</td>\n",
       "      <td>0.360701</td>\n",
       "      <td>0.361048</td>\n",
       "      <td>0.30191</td>\n",
       "      <td>0.329552</td>\n",
       "      <td>0.443688</td>\n",
       "      <td>0.323746</td>\n",
       "      <td>0.267841</td>\n",
       "      <td>0.275786</td>\n",
       "      <td>0.294942</td>\n",
       "      <td>0.372752</td>\n",
       "      <td>0.434201</td>\n",
       "      <td>0.376813</td>\n",
       "      <td>0.3762</td>\n",
       "      <td>0.347443</td>\n",
       "      <td>0.652557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.486626</td>\n",
       "      <td>0.266114</td>\n",
       "      <td>0.329985</td>\n",
       "      <td>0.381091</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.343325</td>\n",
       "      <td>0.332939</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>0.35793</td>\n",
       "      <td>0.339916</td>\n",
       "      <td>0.374096</td>\n",
       "      <td>0.336846</td>\n",
       "      <td>0.369171</td>\n",
       "      <td>0.327333</td>\n",
       "      <td>0.368727</td>\n",
       "      <td>0.327148</td>\n",
       "      <td>0.315132</td>\n",
       "      <td>0.329116</td>\n",
       "      <td>0.352492</td>\n",
       "      <td>0.395083</td>\n",
       "      <td>0.273321</td>\n",
       "      <td>0.392576</td>\n",
       "      <td>0.329311</td>\n",
       "      <td>0.352917</td>\n",
       "      <td>0.451257</td>\n",
       "      <td>0.403674</td>\n",
       "      <td>0.327206</td>\n",
       "      <td>0.336845</td>\n",
       "      <td>0.31011</td>\n",
       "      <td>0.319097</td>\n",
       "      <td>0.329701</td>\n",
       "      <td>0.35433</td>\n",
       "      <td>0.399262</td>\n",
       "      <td>0.292417</td>\n",
       "      <td>0.371391</td>\n",
       "      <td>0.43788</td>\n",
       "      <td>0.268418</td>\n",
       "      <td>0.348067</td>\n",
       "      <td>0.300686</td>\n",
       "      <td>0.359076</td>\n",
       "      <td>0.328841</td>\n",
       "      <td>0.389331</td>\n",
       "      <td>0.477851</td>\n",
       "      <td>0.340024</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.313341</td>\n",
       "      <td>0.348723</td>\n",
       "      <td>0.295654</td>\n",
       "      <td>0.334055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360403</td>\n",
       "      <td>0.361425</td>\n",
       "      <td>0.311121</td>\n",
       "      <td>0.330622</td>\n",
       "      <td>0.288767</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.356378</td>\n",
       "      <td>0.347843</td>\n",
       "      <td>0.295498</td>\n",
       "      <td>0.363151</td>\n",
       "      <td>0.288831</td>\n",
       "      <td>0.323705</td>\n",
       "      <td>0.36124</td>\n",
       "      <td>0.384168</td>\n",
       "      <td>0.337279</td>\n",
       "      <td>0.325347</td>\n",
       "      <td>0.411305</td>\n",
       "      <td>0.395288</td>\n",
       "      <td>0.294686</td>\n",
       "      <td>0.466519</td>\n",
       "      <td>0.367848</td>\n",
       "      <td>0.27515</td>\n",
       "      <td>0.3349</td>\n",
       "      <td>0.35515</td>\n",
       "      <td>0.304164</td>\n",
       "      <td>0.332547</td>\n",
       "      <td>0.318042</td>\n",
       "      <td>0.390003</td>\n",
       "      <td>0.257252</td>\n",
       "      <td>0.347721</td>\n",
       "      <td>0.32725</td>\n",
       "      <td>0.314639</td>\n",
       "      <td>0.371706</td>\n",
       "      <td>0.415569</td>\n",
       "      <td>0.408612</td>\n",
       "      <td>0.360701</td>\n",
       "      <td>0.361048</td>\n",
       "      <td>0.30191</td>\n",
       "      <td>0.329552</td>\n",
       "      <td>0.443688</td>\n",
       "      <td>0.323746</td>\n",
       "      <td>0.267841</td>\n",
       "      <td>0.275786</td>\n",
       "      <td>0.294942</td>\n",
       "      <td>0.372752</td>\n",
       "      <td>0.434201</td>\n",
       "      <td>0.376813</td>\n",
       "      <td>0.3762</td>\n",
       "      <td>0.347443</td>\n",
       "      <td>0.652557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.486626</td>\n",
       "      <td>0.266114</td>\n",
       "      <td>0.329985</td>\n",
       "      <td>0.381091</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.343325</td>\n",
       "      <td>0.332939</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>0.35793</td>\n",
       "      <td>0.339916</td>\n",
       "      <td>0.374096</td>\n",
       "      <td>0.336846</td>\n",
       "      <td>0.369171</td>\n",
       "      <td>0.327333</td>\n",
       "      <td>0.368727</td>\n",
       "      <td>0.327148</td>\n",
       "      <td>0.315132</td>\n",
       "      <td>0.329116</td>\n",
       "      <td>0.352492</td>\n",
       "      <td>0.395083</td>\n",
       "      <td>0.273321</td>\n",
       "      <td>0.392576</td>\n",
       "      <td>0.329311</td>\n",
       "      <td>0.352917</td>\n",
       "      <td>0.451257</td>\n",
       "      <td>0.403674</td>\n",
       "      <td>0.327206</td>\n",
       "      <td>0.336845</td>\n",
       "      <td>0.31011</td>\n",
       "      <td>0.319097</td>\n",
       "      <td>0.329701</td>\n",
       "      <td>0.35433</td>\n",
       "      <td>0.399262</td>\n",
       "      <td>0.292417</td>\n",
       "      <td>0.371391</td>\n",
       "      <td>0.43788</td>\n",
       "      <td>0.268418</td>\n",
       "      <td>0.348067</td>\n",
       "      <td>0.300686</td>\n",
       "      <td>0.359076</td>\n",
       "      <td>0.328841</td>\n",
       "      <td>0.389331</td>\n",
       "      <td>0.477851</td>\n",
       "      <td>0.340024</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.313341</td>\n",
       "      <td>0.348723</td>\n",
       "      <td>0.295654</td>\n",
       "      <td>0.334055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360403</td>\n",
       "      <td>0.361425</td>\n",
       "      <td>0.311121</td>\n",
       "      <td>0.330622</td>\n",
       "      <td>0.288767</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.356378</td>\n",
       "      <td>0.347843</td>\n",
       "      <td>0.295498</td>\n",
       "      <td>0.363151</td>\n",
       "      <td>0.288831</td>\n",
       "      <td>0.323705</td>\n",
       "      <td>0.36124</td>\n",
       "      <td>0.384168</td>\n",
       "      <td>0.337279</td>\n",
       "      <td>0.325347</td>\n",
       "      <td>0.411305</td>\n",
       "      <td>0.395288</td>\n",
       "      <td>0.294686</td>\n",
       "      <td>0.466519</td>\n",
       "      <td>0.367848</td>\n",
       "      <td>0.27515</td>\n",
       "      <td>0.3349</td>\n",
       "      <td>0.35515</td>\n",
       "      <td>0.304164</td>\n",
       "      <td>0.332547</td>\n",
       "      <td>0.318042</td>\n",
       "      <td>0.390003</td>\n",
       "      <td>0.257252</td>\n",
       "      <td>0.347721</td>\n",
       "      <td>0.32725</td>\n",
       "      <td>0.314639</td>\n",
       "      <td>0.371706</td>\n",
       "      <td>0.415569</td>\n",
       "      <td>0.408612</td>\n",
       "      <td>0.360701</td>\n",
       "      <td>0.361048</td>\n",
       "      <td>0.30191</td>\n",
       "      <td>0.329552</td>\n",
       "      <td>0.443688</td>\n",
       "      <td>0.323746</td>\n",
       "      <td>0.267841</td>\n",
       "      <td>0.275786</td>\n",
       "      <td>0.294942</td>\n",
       "      <td>0.372752</td>\n",
       "      <td>0.434201</td>\n",
       "      <td>0.376813</td>\n",
       "      <td>0.3762</td>\n",
       "      <td>0.347443</td>\n",
       "      <td>0.652557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.486626</td>\n",
       "      <td>0.266114</td>\n",
       "      <td>0.329985</td>\n",
       "      <td>0.381091</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.343325</td>\n",
       "      <td>0.332939</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>0.35793</td>\n",
       "      <td>0.339916</td>\n",
       "      <td>0.374096</td>\n",
       "      <td>0.336846</td>\n",
       "      <td>0.369171</td>\n",
       "      <td>0.327333</td>\n",
       "      <td>0.368727</td>\n",
       "      <td>0.327148</td>\n",
       "      <td>0.315132</td>\n",
       "      <td>0.329116</td>\n",
       "      <td>0.352492</td>\n",
       "      <td>0.395083</td>\n",
       "      <td>0.273321</td>\n",
       "      <td>0.392576</td>\n",
       "      <td>0.329311</td>\n",
       "      <td>0.352917</td>\n",
       "      <td>0.451257</td>\n",
       "      <td>0.403674</td>\n",
       "      <td>0.327206</td>\n",
       "      <td>0.336845</td>\n",
       "      <td>0.31011</td>\n",
       "      <td>0.319097</td>\n",
       "      <td>0.329701</td>\n",
       "      <td>0.35433</td>\n",
       "      <td>0.399262</td>\n",
       "      <td>0.292417</td>\n",
       "      <td>0.371391</td>\n",
       "      <td>0.43788</td>\n",
       "      <td>0.268418</td>\n",
       "      <td>0.348067</td>\n",
       "      <td>0.300686</td>\n",
       "      <td>0.359076</td>\n",
       "      <td>0.328841</td>\n",
       "      <td>0.389331</td>\n",
       "      <td>0.477851</td>\n",
       "      <td>0.340024</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.313341</td>\n",
       "      <td>0.348723</td>\n",
       "      <td>0.295654</td>\n",
       "      <td>0.334055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360403</td>\n",
       "      <td>0.361425</td>\n",
       "      <td>0.311121</td>\n",
       "      <td>0.330622</td>\n",
       "      <td>0.288767</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.356378</td>\n",
       "      <td>0.347843</td>\n",
       "      <td>0.295498</td>\n",
       "      <td>0.363151</td>\n",
       "      <td>0.288831</td>\n",
       "      <td>0.323705</td>\n",
       "      <td>0.36124</td>\n",
       "      <td>0.384168</td>\n",
       "      <td>0.337279</td>\n",
       "      <td>0.325347</td>\n",
       "      <td>0.411305</td>\n",
       "      <td>0.395288</td>\n",
       "      <td>0.294686</td>\n",
       "      <td>0.466519</td>\n",
       "      <td>0.367848</td>\n",
       "      <td>0.27515</td>\n",
       "      <td>0.3349</td>\n",
       "      <td>0.35515</td>\n",
       "      <td>0.304164</td>\n",
       "      <td>0.332547</td>\n",
       "      <td>0.318042</td>\n",
       "      <td>0.390003</td>\n",
       "      <td>0.257252</td>\n",
       "      <td>0.347721</td>\n",
       "      <td>0.32725</td>\n",
       "      <td>0.314639</td>\n",
       "      <td>0.371706</td>\n",
       "      <td>0.415569</td>\n",
       "      <td>0.408612</td>\n",
       "      <td>0.360701</td>\n",
       "      <td>0.361048</td>\n",
       "      <td>0.30191</td>\n",
       "      <td>0.329552</td>\n",
       "      <td>0.443688</td>\n",
       "      <td>0.323746</td>\n",
       "      <td>0.267841</td>\n",
       "      <td>0.275786</td>\n",
       "      <td>0.294942</td>\n",
       "      <td>0.372752</td>\n",
       "      <td>0.434201</td>\n",
       "      <td>0.376813</td>\n",
       "      <td>0.3762</td>\n",
       "      <td>0.347443</td>\n",
       "      <td>0.652557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.486626</td>\n",
       "      <td>0.266114</td>\n",
       "      <td>0.329985</td>\n",
       "      <td>0.381091</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.343325</td>\n",
       "      <td>0.332939</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>0.35793</td>\n",
       "      <td>0.339916</td>\n",
       "      <td>0.374096</td>\n",
       "      <td>0.336846</td>\n",
       "      <td>0.369171</td>\n",
       "      <td>0.327333</td>\n",
       "      <td>0.368727</td>\n",
       "      <td>0.327148</td>\n",
       "      <td>0.315132</td>\n",
       "      <td>0.329116</td>\n",
       "      <td>0.352492</td>\n",
       "      <td>0.395083</td>\n",
       "      <td>0.273321</td>\n",
       "      <td>0.392576</td>\n",
       "      <td>0.329311</td>\n",
       "      <td>0.352917</td>\n",
       "      <td>0.451257</td>\n",
       "      <td>0.403674</td>\n",
       "      <td>0.327206</td>\n",
       "      <td>0.336845</td>\n",
       "      <td>0.31011</td>\n",
       "      <td>0.319097</td>\n",
       "      <td>0.329701</td>\n",
       "      <td>0.35433</td>\n",
       "      <td>0.399262</td>\n",
       "      <td>0.292417</td>\n",
       "      <td>0.371391</td>\n",
       "      <td>0.43788</td>\n",
       "      <td>0.268418</td>\n",
       "      <td>0.348067</td>\n",
       "      <td>0.300686</td>\n",
       "      <td>0.359076</td>\n",
       "      <td>0.328841</td>\n",
       "      <td>0.389331</td>\n",
       "      <td>0.477851</td>\n",
       "      <td>0.340024</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.313341</td>\n",
       "      <td>0.348723</td>\n",
       "      <td>0.295654</td>\n",
       "      <td>0.334055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360403</td>\n",
       "      <td>0.361425</td>\n",
       "      <td>0.311121</td>\n",
       "      <td>0.330622</td>\n",
       "      <td>0.288767</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.356378</td>\n",
       "      <td>0.347843</td>\n",
       "      <td>0.295498</td>\n",
       "      <td>0.363151</td>\n",
       "      <td>0.288831</td>\n",
       "      <td>0.323705</td>\n",
       "      <td>0.36124</td>\n",
       "      <td>0.384168</td>\n",
       "      <td>0.337279</td>\n",
       "      <td>0.325347</td>\n",
       "      <td>0.411305</td>\n",
       "      <td>0.395288</td>\n",
       "      <td>0.294686</td>\n",
       "      <td>0.466519</td>\n",
       "      <td>0.367848</td>\n",
       "      <td>0.27515</td>\n",
       "      <td>0.3349</td>\n",
       "      <td>0.35515</td>\n",
       "      <td>0.304164</td>\n",
       "      <td>0.332547</td>\n",
       "      <td>0.318042</td>\n",
       "      <td>0.390003</td>\n",
       "      <td>0.257252</td>\n",
       "      <td>0.347721</td>\n",
       "      <td>0.32725</td>\n",
       "      <td>0.314639</td>\n",
       "      <td>0.371706</td>\n",
       "      <td>0.415569</td>\n",
       "      <td>0.408612</td>\n",
       "      <td>0.360701</td>\n",
       "      <td>0.361048</td>\n",
       "      <td>0.30191</td>\n",
       "      <td>0.329552</td>\n",
       "      <td>0.443688</td>\n",
       "      <td>0.323746</td>\n",
       "      <td>0.267841</td>\n",
       "      <td>0.275786</td>\n",
       "      <td>0.294942</td>\n",
       "      <td>0.372752</td>\n",
       "      <td>0.434201</td>\n",
       "      <td>0.376813</td>\n",
       "      <td>0.3762</td>\n",
       "      <td>0.347443</td>\n",
       "      <td>0.652557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id    pred_1    pred_2    pred_3    pred_4    pred_5    pred_6  \\\n",
       "0  00eed32682bb  0.486626  0.266114  0.329985  0.381091  0.318438  0.343325   \n",
       "1  010ebe33f668  0.486626  0.266114  0.329985  0.381091  0.318438  0.343325   \n",
       "2  02fa521e1838  0.486626  0.266114  0.329985  0.381091  0.318438  0.343325   \n",
       "3  040e15f562a2  0.486626  0.266114  0.329985  0.381091  0.318438  0.343325   \n",
       "4  046e85c7cc7f  0.486626  0.266114  0.329985  0.381091  0.318438  0.343325   \n",
       "\n",
       "     pred_7    pred_8   pred_9   pred_10   pred_11   pred_12   pred_13  \\\n",
       "0  0.332939  0.315942  0.35793  0.339916  0.374096  0.336846  0.369171   \n",
       "1  0.332939  0.315942  0.35793  0.339916  0.374096  0.336846  0.369171   \n",
       "2  0.332939  0.315942  0.35793  0.339916  0.374096  0.336846  0.369171   \n",
       "3  0.332939  0.315942  0.35793  0.339916  0.374096  0.336846  0.369171   \n",
       "4  0.332939  0.315942  0.35793  0.339916  0.374096  0.336846  0.369171   \n",
       "\n",
       "    pred_14   pred_15   pred_16   pred_17   pred_18   pred_19   pred_20  \\\n",
       "0  0.327333  0.368727  0.327148  0.315132  0.329116  0.352492  0.395083   \n",
       "1  0.327333  0.368727  0.327148  0.315132  0.329116  0.352492  0.395083   \n",
       "2  0.327333  0.368727  0.327148  0.315132  0.329116  0.352492  0.395083   \n",
       "3  0.327333  0.368727  0.327148  0.315132  0.329116  0.352492  0.395083   \n",
       "4  0.327333  0.368727  0.327148  0.315132  0.329116  0.352492  0.395083   \n",
       "\n",
       "    pred_21   pred_22   pred_23   pred_24   pred_25   pred_26   pred_27  \\\n",
       "0  0.273321  0.392576  0.329311  0.352917  0.451257  0.403674  0.327206   \n",
       "1  0.273321  0.392576  0.329311  0.352917  0.451257  0.403674  0.327206   \n",
       "2  0.273321  0.392576  0.329311  0.352917  0.451257  0.403674  0.327206   \n",
       "3  0.273321  0.392576  0.329311  0.352917  0.451257  0.403674  0.327206   \n",
       "4  0.273321  0.392576  0.329311  0.352917  0.451257  0.403674  0.327206   \n",
       "\n",
       "    pred_28  pred_29   pred_30   pred_31  pred_32   pred_33   pred_34  \\\n",
       "0  0.336845  0.31011  0.319097  0.329701  0.35433  0.399262  0.292417   \n",
       "1  0.336845  0.31011  0.319097  0.329701  0.35433  0.399262  0.292417   \n",
       "2  0.336845  0.31011  0.319097  0.329701  0.35433  0.399262  0.292417   \n",
       "3  0.336845  0.31011  0.319097  0.329701  0.35433  0.399262  0.292417   \n",
       "4  0.336845  0.31011  0.319097  0.329701  0.35433  0.399262  0.292417   \n",
       "\n",
       "    pred_35  pred_36   pred_37   pred_38   pred_39   pred_40   pred_41  \\\n",
       "0  0.371391  0.43788  0.268418  0.348067  0.300686  0.359076  0.328841   \n",
       "1  0.371391  0.43788  0.268418  0.348067  0.300686  0.359076  0.328841   \n",
       "2  0.371391  0.43788  0.268418  0.348067  0.300686  0.359076  0.328841   \n",
       "3  0.371391  0.43788  0.268418  0.348067  0.300686  0.359076  0.328841   \n",
       "4  0.371391  0.43788  0.268418  0.348067  0.300686  0.359076  0.328841   \n",
       "\n",
       "    pred_42   pred_43   pred_44   pred_45   pred_46   pred_47   pred_48  \\\n",
       "0  0.389331  0.477851  0.340024  0.320667  0.313341  0.348723  0.295654   \n",
       "1  0.389331  0.477851  0.340024  0.320667  0.313341  0.348723  0.295654   \n",
       "2  0.389331  0.477851  0.340024  0.320667  0.313341  0.348723  0.295654   \n",
       "3  0.389331  0.477851  0.340024  0.320667  0.313341  0.348723  0.295654   \n",
       "4  0.389331  0.477851  0.340024  0.320667  0.313341  0.348723  0.295654   \n",
       "\n",
       "    pred_49  ...   pred_53   pred_54   pred_55   pred_56   pred_57   pred_58  \\\n",
       "0  0.334055  ...  0.360403  0.361425  0.311121  0.330622  0.288767  0.295824   \n",
       "1  0.334055  ...  0.360403  0.361425  0.311121  0.330622  0.288767  0.295824   \n",
       "2  0.334055  ...  0.360403  0.361425  0.311121  0.330622  0.288767  0.295824   \n",
       "3  0.334055  ...  0.360403  0.361425  0.311121  0.330622  0.288767  0.295824   \n",
       "4  0.334055  ...  0.360403  0.361425  0.311121  0.330622  0.288767  0.295824   \n",
       "\n",
       "    pred_59   pred_60   pred_61   pred_62   pred_63   pred_64  pred_65  \\\n",
       "0  0.356378  0.347843  0.295498  0.363151  0.288831  0.323705  0.36124   \n",
       "1  0.356378  0.347843  0.295498  0.363151  0.288831  0.323705  0.36124   \n",
       "2  0.356378  0.347843  0.295498  0.363151  0.288831  0.323705  0.36124   \n",
       "3  0.356378  0.347843  0.295498  0.363151  0.288831  0.323705  0.36124   \n",
       "4  0.356378  0.347843  0.295498  0.363151  0.288831  0.323705  0.36124   \n",
       "\n",
       "    pred_66   pred_67   pred_68   pred_69   pred_70   pred_71   pred_72  \\\n",
       "0  0.384168  0.337279  0.325347  0.411305  0.395288  0.294686  0.466519   \n",
       "1  0.384168  0.337279  0.325347  0.411305  0.395288  0.294686  0.466519   \n",
       "2  0.384168  0.337279  0.325347  0.411305  0.395288  0.294686  0.466519   \n",
       "3  0.384168  0.337279  0.325347  0.411305  0.395288  0.294686  0.466519   \n",
       "4  0.384168  0.337279  0.325347  0.411305  0.395288  0.294686  0.466519   \n",
       "\n",
       "    pred_73  pred_74  pred_75  pred_76   pred_77   pred_78   pred_79  \\\n",
       "0  0.367848  0.27515   0.3349  0.35515  0.304164  0.332547  0.318042   \n",
       "1  0.367848  0.27515   0.3349  0.35515  0.304164  0.332547  0.318042   \n",
       "2  0.367848  0.27515   0.3349  0.35515  0.304164  0.332547  0.318042   \n",
       "3  0.367848  0.27515   0.3349  0.35515  0.304164  0.332547  0.318042   \n",
       "4  0.367848  0.27515   0.3349  0.35515  0.304164  0.332547  0.318042   \n",
       "\n",
       "    pred_80   pred_81   pred_82  pred_83   pred_84   pred_85   pred_86  \\\n",
       "0  0.390003  0.257252  0.347721  0.32725  0.314639  0.371706  0.415569   \n",
       "1  0.390003  0.257252  0.347721  0.32725  0.314639  0.371706  0.415569   \n",
       "2  0.390003  0.257252  0.347721  0.32725  0.314639  0.371706  0.415569   \n",
       "3  0.390003  0.257252  0.347721  0.32725  0.314639  0.371706  0.415569   \n",
       "4  0.390003  0.257252  0.347721  0.32725  0.314639  0.371706  0.415569   \n",
       "\n",
       "    pred_87   pred_88   pred_89  pred_90   pred_91   pred_92   pred_93  \\\n",
       "0  0.408612  0.360701  0.361048  0.30191  0.329552  0.443688  0.323746   \n",
       "1  0.408612  0.360701  0.361048  0.30191  0.329552  0.443688  0.323746   \n",
       "2  0.408612  0.360701  0.361048  0.30191  0.329552  0.443688  0.323746   \n",
       "3  0.408612  0.360701  0.361048  0.30191  0.329552  0.443688  0.323746   \n",
       "4  0.408612  0.360701  0.361048  0.30191  0.329552  0.443688  0.323746   \n",
       "\n",
       "    pred_94   pred_95   pred_96   pred_97   pred_98   pred_99  pred_100  \\\n",
       "0  0.267841  0.275786  0.294942  0.372752  0.434201  0.376813    0.3762   \n",
       "1  0.267841  0.275786  0.294942  0.372752  0.434201  0.376813    0.3762   \n",
       "2  0.267841  0.275786  0.294942  0.372752  0.434201  0.376813    0.3762   \n",
       "3  0.267841  0.275786  0.294942  0.372752  0.434201  0.376813    0.3762   \n",
       "4  0.267841  0.275786  0.294942  0.372752  0.434201  0.376813    0.3762   \n",
       "\n",
       "    class_1   class_0  \n",
       "0  0.347443  0.652557  \n",
       "1  0.347443  0.652557  \n",
       "2  0.347443  0.652557  \n",
       "3  0.347443  0.652557  \n",
       "4  0.347443  0.652557  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3_preds[\"class_1\"] = model3_preds.drop(\"Id\", axis=1).mean(axis=1).tolist()\n",
    "model3_preds[\"class_0\"] = (1 - model3_preds[\"class_1\"]).tolist()\n",
    "model3_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa841d74",
   "metadata": {
    "papermill": {
     "duration": 0.039797,
     "end_time": "2023-08-10T15:16:27.887739",
     "exception": false,
     "start_time": "2023-08-10T15:16:27.847942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LightGBM + NestedCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4330497",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:16:27.969410Z",
     "iopub.status.busy": "2023-08-10T15:16:27.968598Z",
     "iopub.status.idle": "2023-08-10T15:16:28.052756Z",
     "shell.execute_reply": "2023-08-10T15:16:28.051737Z"
    },
    "papermill": {
     "duration": 0.128904,
     "end_time": "2023-08-10T15:16:28.056645",
     "exception": false,
     "start_time": "2023-08-10T15:16:27.927741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>BN</th>\n",
       "      <th>BP</th>\n",
       "      <th>BQ</th>\n",
       "      <th>BR</th>\n",
       "      <th>BZ</th>\n",
       "      <th>CB</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>CF</th>\n",
       "      <th>CH</th>\n",
       "      <th>CL</th>\n",
       "      <th>CR</th>\n",
       "      <th>CS</th>\n",
       "      <th>CU</th>\n",
       "      <th>CW</th>\n",
       "      <th>DA</th>\n",
       "      <th>DE</th>\n",
       "      <th>DF</th>\n",
       "      <th>DH</th>\n",
       "      <th>DI</th>\n",
       "      <th>DL</th>\n",
       "      <th>DN</th>\n",
       "      <th>DU</th>\n",
       "      <th>DV</th>\n",
       "      <th>DY</th>\n",
       "      <th>EB</th>\n",
       "      <th>EE</th>\n",
       "      <th>EG</th>\n",
       "      <th>EH</th>\n",
       "      <th>EJ</th>\n",
       "      <th>EL</th>\n",
       "      <th>EP</th>\n",
       "      <th>EU</th>\n",
       "      <th>FC</th>\n",
       "      <th>FD</th>\n",
       "      <th>FE</th>\n",
       "      <th>FI</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000ff2bfdfe9</td>\n",
       "      <td>0.209377</td>\n",
       "      <td>3109.03329</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.812214</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>4126.58731</td>\n",
       "      <td>22.5984</td>\n",
       "      <td>175.638726</td>\n",
       "      <td>152.707705</td>\n",
       "      <td>823.928241</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>47.223358</td>\n",
       "      <td>0.563481</td>\n",
       "      <td>23.387600</td>\n",
       "      <td>4.851915</td>\n",
       "      <td>0.023482</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.069225</td>\n",
       "      <td>13.784111</td>\n",
       "      <td>1.302012</td>\n",
       "      <td>36.205956</td>\n",
       "      <td>69.08340</td>\n",
       "      <td>295.570575</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.284232</td>\n",
       "      <td>89.245560</td>\n",
       "      <td>84.31664</td>\n",
       "      <td>29.657104</td>\n",
       "      <td>5.310690</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>23.187704</td>\n",
       "      <td>7.294176</td>\n",
       "      <td>1.987283</td>\n",
       "      <td>1433.166750</td>\n",
       "      <td>0.949104</td>\n",
       "      <td>1</td>\n",
       "      <td>30.879420</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>3.828384</td>\n",
       "      <td>13.394640</td>\n",
       "      <td>10.265073</td>\n",
       "      <td>9028.291921</td>\n",
       "      <td>3.583450</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>11.339138</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>22.136229</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007255e47698</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>978.76416</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.632190</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>13.517790</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5496.92824</td>\n",
       "      <td>19.4205</td>\n",
       "      <td>155.868030</td>\n",
       "      <td>14.754720</td>\n",
       "      <td>51.216883</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>30.284345</td>\n",
       "      <td>0.484710</td>\n",
       "      <td>50.628208</td>\n",
       "      <td>6.085041</td>\n",
       "      <td>0.031442</td>\n",
       "      <td>1.113875</td>\n",
       "      <td>1.117800</td>\n",
       "      <td>28.310953</td>\n",
       "      <td>1.357182</td>\n",
       "      <td>37.476568</td>\n",
       "      <td>70.79836</td>\n",
       "      <td>178.553100</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.363489</td>\n",
       "      <td>110.581815</td>\n",
       "      <td>75.74548</td>\n",
       "      <td>37.532000</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>17.222328</td>\n",
       "      <td>4.926396</td>\n",
       "      <td>0.858603</td>\n",
       "      <td>1111.287150</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>95.415086</td>\n",
       "      <td>52.260480</td>\n",
       "      <td>17.175984</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>6785.003474</td>\n",
       "      <td>10.358927</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>9.292698</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>29.135430</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>013f2bd269f5</td>\n",
       "      <td>0.470030</td>\n",
       "      <td>2635.10654</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>6.732840</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.824570</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5135.78024</td>\n",
       "      <td>26.4825</td>\n",
       "      <td>128.988531</td>\n",
       "      <td>219.320160</td>\n",
       "      <td>482.141594</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>32.563713</td>\n",
       "      <td>0.495852</td>\n",
       "      <td>85.955376</td>\n",
       "      <td>5.376488</td>\n",
       "      <td>0.036218</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.700350</td>\n",
       "      <td>39.364743</td>\n",
       "      <td>1.009611</td>\n",
       "      <td>21.459644</td>\n",
       "      <td>70.81970</td>\n",
       "      <td>321.426625</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.210441</td>\n",
       "      <td>120.056438</td>\n",
       "      <td>65.46984</td>\n",
       "      <td>28.053464</td>\n",
       "      <td>1.289739</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>36.861352</td>\n",
       "      <td>7.813674</td>\n",
       "      <td>8.146651</td>\n",
       "      <td>1494.076488</td>\n",
       "      <td>0.377208</td>\n",
       "      <td>1</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>5.390628</td>\n",
       "      <td>224.207424</td>\n",
       "      <td>8.745201</td>\n",
       "      <td>8338.906181</td>\n",
       "      <td>11.626917</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>37.077772</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>28.022851</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>043ac50845d5</td>\n",
       "      <td>0.252107</td>\n",
       "      <td>3819.65177</td>\n",
       "      <td>120.201618</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.685344</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>11.053708</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4169.67738</td>\n",
       "      <td>23.6577</td>\n",
       "      <td>237.282264</td>\n",
       "      <td>11.050410</td>\n",
       "      <td>661.518640</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>15.201914</td>\n",
       "      <td>0.717882</td>\n",
       "      <td>88.159360</td>\n",
       "      <td>2.347652</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>1.400300</td>\n",
       "      <td>0.636075</td>\n",
       "      <td>41.116960</td>\n",
       "      <td>0.722727</td>\n",
       "      <td>21.530392</td>\n",
       "      <td>47.27586</td>\n",
       "      <td>196.607985</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.292431</td>\n",
       "      <td>139.824570</td>\n",
       "      <td>71.57120</td>\n",
       "      <td>24.354856</td>\n",
       "      <td>2.655345</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>52.003884</td>\n",
       "      <td>7.386060</td>\n",
       "      <td>3.813326</td>\n",
       "      <td>15691.552180</td>\n",
       "      <td>0.614484</td>\n",
       "      <td>1</td>\n",
       "      <td>31.674357</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>31.323372</td>\n",
       "      <td>59.301984</td>\n",
       "      <td>7.884336</td>\n",
       "      <td>10965.766040</td>\n",
       "      <td>14.852022</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>18.529584</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>39.948656</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>044fb8a146ec</td>\n",
       "      <td>0.380297</td>\n",
       "      <td>3733.04844</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.942255</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>5728.73412</td>\n",
       "      <td>24.0108</td>\n",
       "      <td>324.546318</td>\n",
       "      <td>149.717165</td>\n",
       "      <td>6074.859475</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>82.213495</td>\n",
       "      <td>0.536467</td>\n",
       "      <td>72.644264</td>\n",
       "      <td>30.537722</td>\n",
       "      <td>0.025472</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.693150</td>\n",
       "      <td>31.724726</td>\n",
       "      <td>0.827550</td>\n",
       "      <td>34.415360</td>\n",
       "      <td>74.06532</td>\n",
       "      <td>200.178160</td>\n",
       "      <td>0.23868</td>\n",
       "      <td>0.207708</td>\n",
       "      <td>97.920120</td>\n",
       "      <td>52.83888</td>\n",
       "      <td>26.019912</td>\n",
       "      <td>1.144902</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>9.064856</td>\n",
       "      <td>7.350720</td>\n",
       "      <td>3.490846</td>\n",
       "      <td>1403.656300</td>\n",
       "      <td>0.164268</td>\n",
       "      <td>1</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>91.994825</td>\n",
       "      <td>51.141336</td>\n",
       "      <td>29.102640</td>\n",
       "      <td>4.274640</td>\n",
       "      <td>16198.049590</td>\n",
       "      <td>13.666727</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>16.408728</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>45.381316</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id        AB          AF          AH         AM        AR  \\\n",
       "0  000ff2bfdfe9  0.209377  3109.03329   85.200147  22.394407  8.138688   \n",
       "1  007255e47698  0.145282   978.76416   85.200147  36.968889  8.138688   \n",
       "2  013f2bd269f5  0.470030  2635.10654   85.200147  32.360553  8.138688   \n",
       "3  043ac50845d5  0.252107  3819.65177  120.201618  77.112203  8.138688   \n",
       "4  044fb8a146ec  0.380297  3733.04844   85.200147  14.103738  8.138688   \n",
       "\n",
       "         AX        AY         AZ          BC         BD        BN          BP  \\\n",
       "0  0.699861  0.025578   9.812214    5.555634  4126.58731  22.5984  175.638726   \n",
       "1  3.632190  0.025578  13.517790    1.229900  5496.92824  19.4205  155.868030   \n",
       "2  6.732840  0.025578  12.824570    1.229900  5135.78024  26.4825  128.988531   \n",
       "3  3.685344  0.025578  11.053708    1.229900  4169.67738  23.6577  237.282264   \n",
       "4  3.942255  0.054810   3.396778  102.151980  5728.73412  24.0108  324.546318   \n",
       "\n",
       "           BQ           BR          BZ         CB        CC        CD   \\\n",
       "0  152.707705   823.928241  257.432377  47.223358  0.563481  23.387600   \n",
       "1   14.754720    51.216883  257.432377  30.284345  0.484710  50.628208   \n",
       "2  219.320160   482.141594  257.432377  32.563713  0.495852  85.955376   \n",
       "3   11.050410   661.518640  257.432377  15.201914  0.717882  88.159360   \n",
       "4  149.717165  6074.859475  257.432377  82.213495  0.536467  72.644264   \n",
       "\n",
       "          CF        CH        CL        CR         CS        CU        CW   \\\n",
       "0   4.851915  0.023482  1.050225  0.069225  13.784111  1.302012  36.205956   \n",
       "1   6.085041  0.031442  1.113875  1.117800  28.310953  1.357182  37.476568   \n",
       "2   5.376488  0.036218  1.050225  0.700350  39.364743  1.009611  21.459644   \n",
       "3   2.347652  0.029054  1.400300  0.636075  41.116960  0.722727  21.530392   \n",
       "4  30.537722  0.025472  1.050225  0.693150  31.724726  0.827550  34.415360   \n",
       "\n",
       "         DA          DE       DF        DH          DI        DL         DN  \\\n",
       "0  69.08340  295.570575  0.23868  0.284232   89.245560  84.31664  29.657104   \n",
       "1  70.79836  178.553100  0.23868  0.363489  110.581815  75.74548  37.532000   \n",
       "2  70.81970  321.426625  0.23868  0.210441  120.056438  65.46984  28.053464   \n",
       "3  47.27586  196.607985  0.23868  0.292431  139.824570  71.57120  24.354856   \n",
       "4  74.06532  200.178160  0.23868  0.207708   97.920120  52.83888  26.019912   \n",
       "\n",
       "         DU       DV         DY        EB        EE            EG        EH  \\\n",
       "0  5.310690  1.74307  23.187704  7.294176  1.987283   1433.166750  0.949104   \n",
       "1  0.005518  1.74307  17.222328  4.926396  0.858603   1111.287150  0.003042   \n",
       "2  1.289739  1.74307  36.861352  7.813674  8.146651   1494.076488  0.377208   \n",
       "3  2.655345  1.74307  52.003884  7.386060  3.813326  15691.552180  0.614484   \n",
       "4  1.144902  1.74307   9.064856  7.350720  3.490846   1403.656300  0.164268   \n",
       "\n",
       "   EJ          EL         EP         EU          FC        FD             FE  \\\n",
       "0   1   30.879420  78.526968   3.828384   13.394640  10.265073   9028.291921   \n",
       "1   0  109.125159  95.415086  52.260480   17.175984   0.296850   6785.003474   \n",
       "2   1  109.125159  78.526968   5.390628  224.207424   8.745201   8338.906181   \n",
       "3   1   31.674357  78.526968  31.323372   59.301984   7.884336  10965.766040   \n",
       "4   1  109.125159  91.994825  51.141336   29.102640   4.274640  16198.049590   \n",
       "\n",
       "          FI        FL        FR        FS         GB          GE  \\\n",
       "0   3.583450  7.298162   1.73855  0.094822  11.339138   72.611063   \n",
       "1  10.358927  0.173229   0.49706  0.568932   9.292698   72.611063   \n",
       "2  11.626917  7.709560   0.97556  1.198821  37.077772   88.609437   \n",
       "3  14.852022  6.122162   0.49706  0.284466  18.529584   82.416803   \n",
       "4  13.666727  8.153058  48.50134  0.121914  16.408728  146.109943   \n",
       "\n",
       "             GF         GH         GI         GL  Class  \n",
       "0   2003.810319  22.136229  69.834944   0.120343      1  \n",
       "1  27981.562750  29.135430  32.131996  21.978000      0  \n",
       "2  13676.957810  28.022851  35.192676   0.196941      0  \n",
       "3   2094.262452  39.948656  90.493248   0.155829      0  \n",
       "4   8524.370502  45.381316  36.262628   0.096614      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN', 'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS', 'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY', 'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI', 'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL']\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "target = \"Class\"\n",
    "random_state = 13\n",
    "n_splits = 10\n",
    "num_boost_round = 1000\n",
    "stopping_rounds = 50\n",
    "learning_rate = 0.1\n",
    "n_trials = 300\n",
    "model_name = \"LightGBM\"\n",
    "\n",
    "df_train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n",
    "df_sub = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "df_greeks = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/greeks.csv\")\n",
    "\n",
    "\n",
    "# FE\n",
    "df_train[\"EJ\"] = df_train[\"EJ\"].replace({'A':0, 'B':1})\n",
    "df_test[\"EJ\"] = df_test[\"EJ\"].replace({'A':0, 'B':1})\n",
    "\n",
    "# EX\n",
    "ex_columns = ['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n",
    "       'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n",
    "       'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
    "       'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n",
    "       'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL'] # All\n",
    "\n",
    "exclude_columns = []\n",
    "\n",
    "ex_columns = sorted(list(set(ex_columns) - set(exclude_columns)))\n",
    "use_columns = ex_columns + [target]\n",
    "X_sub = df_test[ex_columns]\n",
    "\n",
    "display(df_train.head())\n",
    "\n",
    "print(ex_columns)\n",
    "print(len(ex_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3f0a674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:16:28.140371Z",
     "iopub.status.busy": "2023-08-10T15:16:28.140067Z",
     "iopub.status.idle": "2023-08-10T15:16:28.154772Z",
     "shell.execute_reply": "2023-08-10T15:16:28.153929Z"
    },
    "papermill": {
     "duration": 0.058513,
     "end_time": "2023-08-10T15:16:28.156682",
     "exception": false,
     "start_time": "2023-08-10T15:16:28.098169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_cv_params = [{'max_depth': 3, 'num_leaves': 51, 'min_data_in_leaf': 16, 'min_gain_to_split': 0.08637809153718612, 'max_bin': 133, 'subsample': 0.5064145001524588, 'subsample_freq': 3, 'feature_fraction': 0.16235118405130844, 'reg_alpha': 0.10995532541271508, 'reg_lambda': 0.0005646201562204504, 'scale_pos_weight': 24.41388167411479}, {'max_depth': 4, 'num_leaves': 41, 'min_data_in_leaf': 17, 'min_gain_to_split': 0.025525456894214503, 'max_bin': 177, 'subsample': 0.558518599899886, 'subsample_freq': 4, 'feature_fraction': 0.6719922974261585, 'reg_alpha': 0.0015480871808771868, 'reg_lambda': 0.39672268268496197, 'scale_pos_weight': 46.69234130241483}, {'max_depth': 3, 'num_leaves': 338, 'min_data_in_leaf': 21, 'min_gain_to_split': 3.7454257412486616, 'max_bin': 502, 'subsample': 0.4948537941906335, 'subsample_freq': 1, 'feature_fraction': 0.8623678720169038, 'reg_alpha': 2.1869241946636042e-06, 'reg_lambda': 9.004663294554265e-07, 'scale_pos_weight': 20.997515955144113}, {'max_depth': 5, 'num_leaves': 253, 'min_data_in_leaf': 20, 'min_gain_to_split': 1.7255942067581924, 'max_bin': 119, 'subsample': 0.6503703259820698, 'subsample_freq': 1, 'feature_fraction': 0.6043586645285949, 'reg_alpha': 0.009844181876172238, 'reg_lambda': 0.009499769539641614, 'scale_pos_weight': 68.85918507416122}, {'max_depth': 8, 'num_leaves': 55, 'min_data_in_leaf': 6, 'min_gain_to_split': 4.333232358237045, 'max_bin': 286, 'subsample': 0.7179702184884593, 'subsample_freq': 1, 'feature_fraction': 0.4333028945426099, 'reg_alpha': 0.028802959049079372, 'reg_lambda': 0.0002760087123277094, 'scale_pos_weight': 28.880688189391368}, {'max_depth': 3, 'num_leaves': 60, 'min_data_in_leaf': 24, 'min_gain_to_split': 3.3932475721381037, 'max_bin': 268, 'subsample': 0.5016487829591301, 'subsample_freq': 1, 'feature_fraction': 0.694134726389784, 'reg_alpha': 1.2343444127530416e-07, 'reg_lambda': 3.036253125121441e-08, 'scale_pos_weight': 37.88927825506205}, {'max_depth': 5, 'num_leaves': 10, 'min_data_in_leaf': 12, 'min_gain_to_split': 0.9436273173334714, 'max_bin': 467, 'subsample': 0.7923792392254261, 'subsample_freq': 4, 'feature_fraction': 0.7653371964487882, 'reg_alpha': 0.06672342182361392, 'reg_lambda': 9.184235220202404e-07, 'scale_pos_weight': 35.048496128895856}, {'max_depth': 6, 'num_leaves': 448, 'min_data_in_leaf': 8, 'min_gain_to_split': 1.080178638356669, 'max_bin': 293, 'subsample': 0.6635508385595192, 'subsample_freq': 1, 'feature_fraction': 0.43932137326681225, 'reg_alpha': 0.00192565651735273, 'reg_lambda': 3.444554709746977, 'scale_pos_weight': 48.929896223989246}, {'max_depth': 3, 'num_leaves': 350, 'min_data_in_leaf': 80, 'min_gain_to_split': 10.088046158122664, 'max_bin': 298, 'subsample': 0.7287525511357824, 'subsample_freq': 3, 'feature_fraction': 0.23839761568586276, 'reg_alpha': 0.01379146464309962, 'reg_lambda': 1.1047850106749175, 'scale_pos_weight': 5.548911994874806}, {'max_depth': 4, 'num_leaves': 277, 'min_data_in_leaf': 25, 'min_gain_to_split': 0.008462452382852037, 'max_bin': 114, 'subsample': 0.9692844748757705, 'subsample_freq': 0, 'feature_fraction': 0.5947877824759527, 'reg_alpha': 1.4001566018102642, 'reg_lambda': 0.0024869599944277727, 'scale_pos_weight': 27.219213112922905}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13c5ef41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:16:28.239244Z",
     "iopub.status.busy": "2023-08-10T15:16:28.238464Z",
     "iopub.status.idle": "2023-08-10T15:16:29.314433Z",
     "shell.execute_reply": "2023-08-10T15:16:29.312983Z"
    },
    "papermill": {
     "duration": 1.119892,
     "end_time": "2023-08-10T15:16:29.316806",
     "exception": false,
     "start_time": "2023-08-10T15:16:28.196914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Out CV 0 *****\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's binary_logloss: 0.0706397\tvalid_1's binary_logloss: 0.288624\n",
      "***** Out CV 1 *****\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[112]\ttraining's binary_logloss: 0.0222547\tvalid_1's binary_logloss: 0.162845\n",
      "***** Out CV 2 *****\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[187]\ttraining's binary_logloss: 0.0523293\tvalid_1's binary_logloss: 0.115127\n",
      "***** Out CV 3 *****\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[241]\ttraining's binary_logloss: 0.0231675\tvalid_1's binary_logloss: 0.118251\n",
      "***** Out CV 4 *****\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[43]\ttraining's binary_logloss: 0.124023\tvalid_1's binary_logloss: 0.298886\n",
      "***** Out CV 5 *****\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[289]\ttraining's binary_logloss: 0.0381115\tvalid_1's binary_logloss: 0.278074\n",
      "***** Out CV 6 *****\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[113]\ttraining's binary_logloss: 0.0212989\tvalid_1's binary_logloss: 0.0241342\n",
      "***** Out CV 7 *****\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[299]\ttraining's binary_logloss: 0.0307869\tvalid_1's binary_logloss: 0.0505814\n",
      "***** Out CV 8 *****\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[83]\ttraining's binary_logloss: 0.209108\tvalid_1's binary_logloss: 0.338844\n",
      "***** Out CV 9 *****\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[143]\ttraining's binary_logloss: 0.0211236\tvalid_1's binary_logloss: 0.0734132\n"
     ]
    }
   ],
   "source": [
    "outer_cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "X, y = df_train[ex_columns], df_train[target]\n",
    "model4_preds = df_sub[[\"Id\"]].copy()\n",
    "scores = []\n",
    "model4_oof = np.zeros(len(y))\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# CV\n",
    "for i, (train_inds, test_inds) in enumerate(outer_cv.split(X, y)):\n",
    "    print(f\"***** Out CV {i} *****\")\n",
    "    X_train_out, y_train_out = X.iloc[train_inds], y.iloc[train_inds]\n",
    "    X_test, y_test = X.iloc[test_inds], y.iloc[test_inds]\n",
    "\n",
    "    params = {**base_params_dict[model_name], **best_cv_params[i]}\n",
    "\n",
    "    # X_test\n",
    "    data_train = lgb.Dataset(X_train_out, y_train_out)\n",
    "    data_val = lgb.Dataset(X_test, y_test)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set=data_train,\n",
    "        valid_sets=[data_train, data_val],\n",
    "        num_boost_round=num_boost_round,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n",
    "            lgb.record_evaluation(evals_result),\n",
    "        ],\n",
    "    )\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = metric(y_test, y_pred)\n",
    "    scores.append(score)\n",
    "    model4_oof[test_inds] = y_pred\n",
    "    # \n",
    "    pred = model.predict(X_sub)\n",
    "    model4_preds[f\"pred_{i+1}\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5668add7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:16:29.404506Z",
     "iopub.status.busy": "2023-08-10T15:16:29.403578Z",
     "iopub.status.idle": "2023-08-10T15:16:29.409362Z",
     "shell.execute_reply": "2023-08-10T15:16:29.408292Z"
    },
    "papermill": {
     "duration": 0.050646,
     "end_time": "2023-08-10T15:16:29.411541",
     "exception": false,
     "start_time": "2023-08-10T15:16:29.360895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score mean: 0.21240880935310505, Score var: 0.02278429938280975\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score mean: {np.mean(scores)}, Score var: {np.var(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "825a70e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:16:29.499435Z",
     "iopub.status.busy": "2023-08-10T15:16:29.499132Z",
     "iopub.status.idle": "2023-08-10T15:16:29.519396Z",
     "shell.execute_reply": "2023-08-10T15:16:29.518471Z"
    },
    "papermill": {
     "duration": 0.067198,
     "end_time": "2023-08-10T15:16:29.521466",
     "exception": false,
     "start_time": "2023-08-10T15:16:29.454268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>pred_4</th>\n",
       "      <th>pred_5</th>\n",
       "      <th>pred_6</th>\n",
       "      <th>pred_7</th>\n",
       "      <th>pred_8</th>\n",
       "      <th>pred_9</th>\n",
       "      <th>pred_10</th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.558004</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.407371</td>\n",
       "      <td>0.15053</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>0.410301</td>\n",
       "      <td>0.087282</td>\n",
       "      <td>0.524441</td>\n",
       "      <td>0.332833</td>\n",
       "      <td>0.277583</td>\n",
       "      <td>0.722417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.558004</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.407371</td>\n",
       "      <td>0.15053</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>0.410301</td>\n",
       "      <td>0.087282</td>\n",
       "      <td>0.524441</td>\n",
       "      <td>0.332833</td>\n",
       "      <td>0.277583</td>\n",
       "      <td>0.722417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.558004</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.407371</td>\n",
       "      <td>0.15053</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>0.410301</td>\n",
       "      <td>0.087282</td>\n",
       "      <td>0.524441</td>\n",
       "      <td>0.332833</td>\n",
       "      <td>0.277583</td>\n",
       "      <td>0.722417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.558004</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.407371</td>\n",
       "      <td>0.15053</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>0.410301</td>\n",
       "      <td>0.087282</td>\n",
       "      <td>0.524441</td>\n",
       "      <td>0.332833</td>\n",
       "      <td>0.277583</td>\n",
       "      <td>0.722417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.558004</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.407371</td>\n",
       "      <td>0.15053</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>0.410301</td>\n",
       "      <td>0.087282</td>\n",
       "      <td>0.524441</td>\n",
       "      <td>0.332833</td>\n",
       "      <td>0.277583</td>\n",
       "      <td>0.722417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id    pred_1    pred_2    pred_3   pred_4    pred_5    pred_6  \\\n",
       "0  00eed32682bb  0.558004  0.046667  0.407371  0.15053  0.022696  0.235702   \n",
       "1  010ebe33f668  0.558004  0.046667  0.407371  0.15053  0.022696  0.235702   \n",
       "2  02fa521e1838  0.558004  0.046667  0.407371  0.15053  0.022696  0.235702   \n",
       "3  040e15f562a2  0.558004  0.046667  0.407371  0.15053  0.022696  0.235702   \n",
       "4  046e85c7cc7f  0.558004  0.046667  0.407371  0.15053  0.022696  0.235702   \n",
       "\n",
       "     pred_7    pred_8    pred_9   pred_10   class_1   class_0  \n",
       "0  0.410301  0.087282  0.524441  0.332833  0.277583  0.722417  \n",
       "1  0.410301  0.087282  0.524441  0.332833  0.277583  0.722417  \n",
       "2  0.410301  0.087282  0.524441  0.332833  0.277583  0.722417  \n",
       "3  0.410301  0.087282  0.524441  0.332833  0.277583  0.722417  \n",
       "4  0.410301  0.087282  0.524441  0.332833  0.277583  0.722417  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_preds[\"class_1\"] = model4_preds.drop(\"Id\", axis=1).mean(axis=1).tolist()\n",
    "model4_preds[\"class_0\"] = (1 - model4_preds[\"class_1\"]).tolist()\n",
    "model4_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88d364",
   "metadata": {
    "papermill": {
     "duration": 0.041713,
     "end_time": "2023-08-10T15:16:29.605346",
     "exception": false,
     "start_time": "2023-08-10T15:16:29.563633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# WoE + LR\n",
    "\n",
    "https://www.kaggle.com/code/tatudoug/logistic-regression-baseline/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0bac7f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:16:29.692978Z",
     "iopub.status.busy": "2023-08-10T15:16:29.692672Z",
     "iopub.status.idle": "2023-08-10T15:16:54.769807Z",
     "shell.execute_reply": "2023-08-10T15:16:54.768656Z"
    },
    "papermill": {
     "duration": 25.124758,
     "end_time": "2023-08-10T15:16:54.772354",
     "exception": false,
     "start_time": "2023-08-10T15:16:29.647596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from numpy import dot, exp\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import copy \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n",
    "greeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\n",
    "\n",
    "\n",
    "greeks['k'] = greeks['Alpha']+greeks['Beta']+greeks['Gamma']+greeks['Delta']\n",
    "train = pd.merge( greeks[['k','Id']],train,on='Id')\n",
    "\n",
    "names = ['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n",
    "       'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n",
    "       'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
    "       'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n",
    "       'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL']\n",
    "target_name = 'Class'\n",
    "\n",
    "train['EJ'] = pd.Series(np.where(train.EJ.values == 'A', 1, 0),\n",
    "          train.index)\n",
    "test['EJ'] = pd.Series(np.where(test.EJ.values == 'A', 1, 0),\n",
    "          test.index)\n",
    "\n",
    "# fill nan data with mean values \n",
    "train[names] = train[names].fillna(train[names].mean())\n",
    "test[names] = test[names].fillna(train[names].mean())\n",
    "# clip values to avoid different values in the test set from train\n",
    "test = test[names].clip(train[names].min(axis=0).values,train[names].max(axis=0).values, axis=1)\n",
    "\n",
    "# data scaled to allow the features interaction (by multiplication)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train2 = copy.copy(train)\n",
    "teste2 = copy.copy(test)\n",
    "\n",
    "vals = scaler.fit_transform(train[names])\n",
    "vals_test = scaler.transform(test[names])\n",
    "\n",
    "train2[names] = vals\n",
    "teste2[names] = vals_test\n",
    "\n",
    "\n",
    "\n",
    "# def multiply and make a array of all interactions\n",
    "def mab(df,nome1,nome2):\n",
    "    a  = df[nome1]*df[nome2]\n",
    "    return(a/max(a))\n",
    "h = []\n",
    "ht = []\n",
    "n = 1\n",
    "for n1 in names:\n",
    "    for n2 in names[n:]:\n",
    "        h.append(mab(train2,n1,n2).rename(n1+'_mul_'+n2))\n",
    "        ht.append(mab(teste2,n1,n2).rename(n1+'_mul_'+n2)) \n",
    "    n+=1\n",
    "    \n",
    "newF = pd.DataFrame(h)\n",
    "newF_test = pd.DataFrame(ht)\n",
    "\n",
    "\n",
    "#https://lucastiagooliveira.github.io/datascience/iv/woe/python/2020/12/15/iv_woe.html\n",
    "def iv_woe(data, target, bins=10, show_woe=False):\n",
    "    \n",
    "    #Empty Dataframe\n",
    "    newDF,woeDF = pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    #Extract Column Names\n",
    "    cols = data.columns\n",
    "    \n",
    "    #Run WOE and IV on all the independent variables\n",
    "    for ivars in cols[~cols.isin([target])]:\n",
    "        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):\n",
    "            binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')\n",
    "            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})\n",
    "        else:\n",
    "            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})\n",
    "\n",
    "        \n",
    "        # Calculate the number of events in each group (bin)\n",
    "        d = d0.groupby(\"x\", as_index=False).agg({\"y\": [\"count\", \"sum\"]})\n",
    "        d.columns = ['Cutoff', 'N', 'Events']\n",
    "        \n",
    "        # Calculate % of events in each group.\n",
    "        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()\n",
    "\n",
    "        # Calculate the non events in each group.\n",
    "        d['Non-Events'] = d['N'] - d['Events']\n",
    "        # Calculate % of non events in each group.\n",
    "        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()\n",
    "\n",
    "        # Calculate WOE by taking natural log of division of % of non-events and % of events\n",
    "        d['WoE'] = np.log(d['% of Events']/d['% of Non-Events'])\n",
    "        d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])\n",
    "        d.insert(loc=0, column='Variable', value=ivars)\n",
    "        #print(\"Information value of \" + ivars + \" is \" + str(round(d['IV'].sum(),6)))\n",
    "        temp =pd.DataFrame({\"Variable\" : [ivars], \"IV\" : [d['IV'].sum()]}, columns = [\"Variable\", \"IV\"])\n",
    "        newDF=pd.concat([newDF,temp], axis=0)\n",
    "        woeDF=pd.concat([woeDF,d], axis=0)\n",
    "\n",
    "        #Show WOE Table\n",
    "        if show_woe == True:\n",
    "            print(d)\n",
    "    return newDF, woeDF\n",
    "\n",
    "# multiv_woe\n",
    "a,b = iv_woe(train2, target_name, bins=10, show_woe=False)\n",
    "\n",
    "\n",
    "# Reordering the dataframe to keep IV with higger values in front\n",
    "trainE = train[a.sort_values(by='IV',ascending=False).Variable.values]\n",
    "trainE[target_name] = train[target_name]\n",
    "testeE = test[a.sort_values(by='IV',ascending=False).Variable.values[2:]]\n",
    "\n",
    "# join the original vars and the interactions between them\n",
    "# concat\n",
    "ff = pd.concat([trainE,newF.T],axis=1)\n",
    "ff_teste = pd.concat([testeE,newF_test.T],axis=1)\n",
    "\n",
    "# mult\n",
    "a,b = iv_woe(ff, target_name, bins=10, show_woe=False)\n",
    "\n",
    "# deleting all IVs below 0.05\n",
    "a = a.loc[a['IV']> 0.05]\n",
    "\n",
    "allNames = a.sort_values(by='IV',ascending=False).Variable.values\n",
    "crossNames = [x for x in allNames if '_mul_' in x]\n",
    "\n",
    "nomes2 = list(trainE)+crossNames\n",
    "nomes2.remove('Class')\n",
    "\n",
    "\n",
    "# threshold to correlation features\n",
    "threshold = 0.3\n",
    "\n",
    "cc = ff[nomes2[2:]].corr()\n",
    "\n",
    "mat_x = abs(cc)>threshold\n",
    "mat_x = mat_x.to_numpy()\n",
    "\n",
    "\n",
    "# selection tof the variables with low correlation, there are +- 70 features with low correlation\n",
    "var1 = []\n",
    "nomes = list(cc)\n",
    "var1.append(nomes[0]) # IV\n",
    "max_vars = 100\n",
    "\n",
    "count = 1\n",
    "for n in range(1,len(nomes)):\n",
    "    \n",
    "    if (mat_x[n,:n+1].sum() ) == 1:\n",
    "        \n",
    "        var1.append(nomes[n])        \n",
    "        count+=1\n",
    "        \n",
    "        if(count == max_vars):\n",
    "            break\n",
    "\n",
    "            \n",
    "# create dic with WoE transformation\n",
    "list_dics = []\n",
    "\n",
    "for var in var1:\n",
    "    df_temp = b.loc[b['Variable']==var].reset_index()\n",
    "    # criando dicionario\n",
    "    dict_var = {}\n",
    "    for x in range(len(df_temp)):\n",
    "        line = df_temp.iloc[x]\n",
    "        dict_var[ line['Cutoff'] ] = line['WoE']\n",
    "    list_dics.append(dict_var)\n",
    "    \n",
    "\n",
    "# train and test data\n",
    "df_original = ff[var1+[target_name]+ ['k'] ]\n",
    "df_teste2 = ff_teste[var1]\n",
    "names = var1\n",
    "\n",
    "# In this part there is some data leakage as the map is using the full dataset\n",
    "n= 0\n",
    "\n",
    "for var in var1:\n",
    "    df_original.loc[:,var] = df_original[var].map(list_dics[n])\n",
    "    df_teste2.loc[:,var] = df_teste2[var].map(list_dics[n])\n",
    "\n",
    "    n=n+1\n",
    "    \n",
    "df_original.loc[:,names] = df_original[names].fillna(df_original[names].mean())\n",
    "df_teste2.loc[:,names] = df_teste2[names].fillna(df_original[names].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0105519f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:16:54.858801Z",
     "iopub.status.busy": "2023-08-10T15:16:54.858473Z",
     "iopub.status.idle": "2023-08-10T15:17:04.036428Z",
     "shell.execute_reply": "2023-08-10T15:17:04.035062Z"
    },
    "papermill": {
     "duration": 9.223315,
     "end_time": "2023-08-10T15:17:04.038749",
     "exception": false,
     "start_time": "2023-08-10T15:16:54.815434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Out CV 0 *****\n",
      "params0.1, score0.17698052743939482\n",
      "***** Out CV 1 *****\n",
      "params0.1, score0.20324375113826867\n",
      "***** Out CV 2 *****\n",
      "params0.1, score0.20454661506125354\n",
      "***** Out CV 3 *****\n",
      "params0.1, score0.18794923474486833\n",
      "***** Out CV 4 *****\n",
      "params0.1, score0.19153289231652312\n",
      "***** Out CV 5 *****\n",
      "params0.1, score0.19281163835946102\n",
      "***** Out CV 6 *****\n",
      "params0.1, score0.1995234659196253\n",
      "***** Out CV 7 *****\n",
      "params0.1, score0.2052976700556822\n",
      "***** Out CV 8 *****\n",
      "params0.1, score0.16393992261410026\n",
      "***** Out CV 9 *****\n",
      "params0.1, score0.197744397734064\n",
      "0.1896884967633897\n",
      "***** Out CV 0 *****\n",
      "***** Out CV 1 *****\n",
      "***** Out CV 2 *****\n",
      "***** Out CV 3 *****\n",
      "***** Out CV 4 *****\n",
      "***** Out CV 5 *****\n",
      "***** Out CV 6 *****\n",
      "***** Out CV 7 *****\n",
      "***** Out CV 8 *****\n",
      "***** Out CV 9 *****\n",
      "0.1896884967633897\n"
     ]
    }
   ],
   "source": [
    "def lr_nested_cv(X, y, params):\n",
    "    \n",
    "    nested_scores, best_cv_params = [], []\n",
    "    outer_cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    # CV\n",
    "    for i, (train_inds, test_inds) in enumerate(outer_cv.split(X, y)):\n",
    "        print(f\"***** Out CV {i} *****\")\n",
    "        X_train_out, y_train_out = X.iloc[train_inds], y.iloc[train_inds]\n",
    "        X_test, y_test = X.iloc[test_inds], y.iloc[test_inds]\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        best_params, best_score = None, 1000000\n",
    "        for C in params:\n",
    "            scores = []\n",
    "            for j, (train_idx, val_idx) in enumerate(cv.split(X_train_out, y_train_out)):\n",
    "                X_train_in, y_train_in = X_train_out.iloc[train_idx], y_train_out.iloc[train_idx]\n",
    "                X_val, y_val = X_train_out.iloc[val_idx], y_train_out.iloc[val_idx]\n",
    "\n",
    "                model = LogisticRegression(C=C, random_state=random_state, l1_ratio=l1_ratio,\n",
    "                                     solver=solver, class_weight=class_weight, penalty=penalty, max_iter=max_iter)\n",
    "                model.fit(X_train_in, y_train_in)\n",
    "                y_pred = model.predict_proba(X_val)[:, 1]\n",
    "                score = metric(y_val, y_pred)\n",
    "                scores.append(score)\n",
    "\n",
    "            # outFoldC\n",
    "            cv_score = np.mean(scores)\n",
    "            if best_score > cv_score:\n",
    "                best_params = C\n",
    "                best_score = cv_score\n",
    "\n",
    "        print(f\"params{best_params}, score{best_score}\")\n",
    "        model = LogisticRegression(C=best_params, random_state=random_state, l1_ratio=l1_ratio,\n",
    "                                 solver=solver, class_weight=class_weight, penalty=penalty, max_iter=max_iter)\n",
    "        model.fit(X_train_out, y_train_out)\n",
    "        y_pred = model.predict_proba(X_test)[:, 1]\n",
    "        score = metric(y_test, y_pred)\n",
    "        nested_scores.append(score)\n",
    "        best_cv_params.append(best_params)\n",
    "    return nested_scores, best_cv_params\n",
    "\n",
    "n_splits = 10\n",
    "solver=\"liblinear\"\n",
    "class_weight=\"balanced\"\n",
    "penalty=\"l2\"\n",
    "l1_ratio = None\n",
    "max_iter = 1000\n",
    "random_state = 13\n",
    "params = [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "X, y = df_original[names], df_original[target_name]\n",
    "nested_scores, best_cv_params = lr_nested_cv(X, y, params)\n",
    "print(np.mean(nested_scores))\n",
    "\n",
    "\n",
    "#X_sub = df_test[ex_columns]\n",
    "X_sub = df_teste2[names]\n",
    "model5_preds = df_sub[[\"Id\"]].copy()\n",
    "scores = []\n",
    "model5_oof = np.zeros(len(y))\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "# CV\n",
    "for i, (train_inds, test_inds) in enumerate(outer_cv.split(X, y)):\n",
    "    print(f\"***** Out CV {i} *****\")\n",
    "    X_train_out, y_train_out = X.iloc[train_inds], y.iloc[train_inds]\n",
    "    X_test, y_test = X.iloc[test_inds], y.iloc[test_inds]\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    C = best_cv_params[i]\n",
    "    model = LogisticRegression(C=C, random_state=random_state, l1_ratio=l1_ratio,\n",
    "                                solver=solver, class_weight=class_weight, penalty=penalty, max_iter=max_iter)\n",
    "    model.fit(X_train_out, y_train_out)\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    score = metric(y_test, y_pred)\n",
    "    scores.append(score)\n",
    "    model5_oof[test_inds] = y_pred\n",
    "    \n",
    "    # \n",
    "    pred = model.predict_proba(X_sub)[:, 1]\n",
    "    model5_preds[f\"pred_{i+1}\"] = pred\n",
    "\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2017c022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:04.129990Z",
     "iopub.status.busy": "2023-08-10T15:17:04.128505Z",
     "iopub.status.idle": "2023-08-10T15:17:04.217174Z",
     "shell.execute_reply": "2023-08-10T15:17:04.216213Z"
    },
    "papermill": {
     "duration": 0.135248,
     "end_time": "2023-08-10T15:17:04.219633",
     "exception": false,
     "start_time": "2023-08-10T15:17:04.084385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DU</th>\n",
       "      <th>GL</th>\n",
       "      <th>CR</th>\n",
       "      <th>DA</th>\n",
       "      <th>AF</th>\n",
       "      <th>EE</th>\n",
       "      <th>FR</th>\n",
       "      <th>DE</th>\n",
       "      <th>CC</th>\n",
       "      <th>BN</th>\n",
       "      <th>FI</th>\n",
       "      <th>FE</th>\n",
       "      <th>DH</th>\n",
       "      <th>EU</th>\n",
       "      <th>GH</th>\n",
       "      <th>DY</th>\n",
       "      <th>CB</th>\n",
       "      <th>FC</th>\n",
       "      <th>BR</th>\n",
       "      <th>FS</th>\n",
       "      <th>AZ</th>\n",
       "      <th>CW</th>\n",
       "      <th>EG</th>\n",
       "      <th>DU_mul_FE</th>\n",
       "      <th>DA_mul_DU</th>\n",
       "      <th>CR_mul_DE</th>\n",
       "      <th>DI_mul_FE</th>\n",
       "      <th>BQ_mul_FE</th>\n",
       "      <th>BQ_mul_CB</th>\n",
       "      <th>BQ_mul_CW</th>\n",
       "      <th>DA_mul_DL</th>\n",
       "      <th>CR_mul_GE</th>\n",
       "      <th>EE_mul_GF</th>\n",
       "      <th>CR_mul_FE</th>\n",
       "      <th>BQ_mul_FC</th>\n",
       "      <th>DE_mul_GE</th>\n",
       "      <th>CD _mul_DF</th>\n",
       "      <th>CR_mul_GI</th>\n",
       "      <th>DE_mul_DL</th>\n",
       "      <th>DL_mul_EL</th>\n",
       "      <th>BN_mul_BQ</th>\n",
       "      <th>BN_mul_DA</th>\n",
       "      <th>CU_mul_DN</th>\n",
       "      <th>AZ_mul_GL</th>\n",
       "      <th>CW _mul_DL</th>\n",
       "      <th>DE_mul_EL</th>\n",
       "      <th>CW _mul_EE</th>\n",
       "      <th>BN_mul_CR</th>\n",
       "      <th>DE_mul_GF</th>\n",
       "      <th>DH_mul_EG</th>\n",
       "      <th>CC_mul_EL</th>\n",
       "      <th>CU_mul_EG</th>\n",
       "      <th>DN_mul_FI</th>\n",
       "      <th>AZ_mul_FE</th>\n",
       "      <th>DN_mul_EL</th>\n",
       "      <th>CW _mul_EL</th>\n",
       "      <th>AZ_mul_CU</th>\n",
       "      <th>CW _mul_DY</th>\n",
       "      <th>DH_mul_DL</th>\n",
       "      <th>AX_mul_CU</th>\n",
       "      <th>DY_mul_FI</th>\n",
       "      <th>BN_mul_DE</th>\n",
       "      <th>BN_mul_CW</th>\n",
       "      <th>BN_mul_DY</th>\n",
       "      <th>AZ_mul_EL</th>\n",
       "      <th>AZ_mul_DE</th>\n",
       "      <th>Class</th>\n",
       "      <th>k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.367078</td>\n",
       "      <td>0.223446</td>\n",
       "      <td>1.679529</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>0.223446</td>\n",
       "      <td>-0.683275</td>\n",
       "      <td>0.576868</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>0.143403</td>\n",
       "      <td>0.352614</td>\n",
       "      <td>0.952480</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.036189</td>\n",
       "      <td>-0.677305</td>\n",
       "      <td>-0.340534</td>\n",
       "      <td>0.143403</td>\n",
       "      <td>-0.340534</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>0.223446</td>\n",
       "      <td>-0.117390</td>\n",
       "      <td>-0.098342</td>\n",
       "      <td>0.036189</td>\n",
       "      <td>0.223446</td>\n",
       "      <td>0.832477</td>\n",
       "      <td>1.679529</td>\n",
       "      <td>1.679529</td>\n",
       "      <td>0.244065</td>\n",
       "      <td>0.516243</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>0.599340</td>\n",
       "      <td>0.429726</td>\n",
       "      <td>1.356161</td>\n",
       "      <td>-0.203702</td>\n",
       "      <td>0.318173</td>\n",
       "      <td>0.576868</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>0.494264</td>\n",
       "      <td>0.576868</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>-0.098342</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>-0.078924</td>\n",
       "      <td>-0.665257</td>\n",
       "      <td>-0.078924</td>\n",
       "      <td>0.733556</td>\n",
       "      <td>0.143403</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.59934</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.143403</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.036189</td>\n",
       "      <td>0.408219</td>\n",
       "      <td>-1.106440</td>\n",
       "      <td>-0.078924</td>\n",
       "      <td>-0.078924</td>\n",
       "      <td>-0.492757</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.244065</td>\n",
       "      <td>0.733556</td>\n",
       "      <td>-0.883297</td>\n",
       "      <td>0.429726</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>0.408219</td>\n",
       "      <td>-0.203702</td>\n",
       "      <td>1</td>\n",
       "      <td>BCGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.408497</td>\n",
       "      <td>-0.506550</td>\n",
       "      <td>-0.683275</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>-0.173725</td>\n",
       "      <td>-0.203702</td>\n",
       "      <td>0.733556</td>\n",
       "      <td>-0.995214</td>\n",
       "      <td>-0.665257</td>\n",
       "      <td>-0.683275</td>\n",
       "      <td>0.056392</td>\n",
       "      <td>0.599340</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>0.576868</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>0.318173</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>-0.098342</td>\n",
       "      <td>-0.492757</td>\n",
       "      <td>-0.203702</td>\n",
       "      <td>-1.123832</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>-0.883297</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>-1.123832</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>0.223446</td>\n",
       "      <td>0.494264</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>0.656499</td>\n",
       "      <td>0.318173</td>\n",
       "      <td>0.223446</td>\n",
       "      <td>-0.098342</td>\n",
       "      <td>-0.665257</td>\n",
       "      <td>-1.123832</td>\n",
       "      <td>-0.865597</td>\n",
       "      <td>0.494264</td>\n",
       "      <td>0.408219</td>\n",
       "      <td>0.59934</td>\n",
       "      <td>-0.683275</td>\n",
       "      <td>0.143403</td>\n",
       "      <td>0.494264</td>\n",
       "      <td>0.036189</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>-0.078924</td>\n",
       "      <td>0.952480</td>\n",
       "      <td>-0.078924</td>\n",
       "      <td>0.318173</td>\n",
       "      <td>-0.098342</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>-0.665257</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>-0.340534</td>\n",
       "      <td>-0.829229</td>\n",
       "      <td>-0.340534</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>-0.683275</td>\n",
       "      <td>0</td>\n",
       "      <td>ACMB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DU        GL        CR        DA        AF        EE        FR  \\\n",
       "0  2.367078  0.223446  1.679529 -0.222751  0.223446 -0.683275  0.576868   \n",
       "1 -0.408497 -0.506550 -0.683275 -0.222751 -0.511106  0.881267 -0.173725   \n",
       "\n",
       "         DE        CC        BN        FI        FE        DH        EU  \\\n",
       "0 -0.359226  0.143403  0.352614  0.952480  0.016386  0.036189 -0.677305   \n",
       "1 -0.203702  0.733556 -0.995214 -0.665257 -0.683275  0.056392  0.599340   \n",
       "\n",
       "         GH        DY        CB        FC        BR        FS        AZ  \\\n",
       "0 -0.340534  0.143403 -0.340534 -0.359226  0.223446 -0.117390 -0.098342   \n",
       "1 -0.359226  0.576868  0.016386 -0.359226  0.318173 -0.359226 -0.098342   \n",
       "\n",
       "        CW         EG  DU_mul_FE  DA_mul_DU  CR_mul_DE  DI_mul_FE  BQ_mul_FE  \\\n",
       "0  0.036189  0.223446   0.832477   1.679529   1.679529   0.244065   0.516243   \n",
       "1 -0.492757 -0.203702  -1.123832  -0.222751  -0.511106  -0.883297  -0.511106   \n",
       "\n",
       "   BQ_mul_CB  BQ_mul_CW   DA_mul_DL  CR_mul_GE  EE_mul_GF  CR_mul_FE  \\\n",
       "0   0.881267    0.599340   0.429726   1.356161  -0.203702   0.318173   \n",
       "1  -0.511106   -0.511106  -0.511106  -0.359226  -1.123832  -0.222751   \n",
       "\n",
       "   BQ_mul_FC  DE_mul_GE  CD _mul_DF  CR_mul_GI  DE_mul_DL  DL_mul_EL  \\\n",
       "0   0.576868  -0.511106    0.494264   0.576868   0.016386  -0.098342   \n",
       "1  -0.359226   0.223446    0.494264  -0.222751   0.656499   0.318173   \n",
       "\n",
       "   BN_mul_BQ  BN_mul_DA  CU_mul_DN  AZ_mul_GL  CW _mul_DL  DE_mul_EL  \\\n",
       "0  -0.359226  -0.078924  -0.665257  -0.078924    0.733556   0.143403   \n",
       "1   0.223446  -0.098342  -0.665257  -1.123832   -0.865597   0.494264   \n",
       "\n",
       "   CW _mul_EE  BN_mul_CR  DE_mul_GF  DH_mul_EG  CC_mul_EL  CU_mul_EG  \\\n",
       "0    0.016386    0.59934   0.016386   0.143403   0.016386   0.036189   \n",
       "1    0.408219    0.59934  -0.683275   0.143403   0.494264   0.036189   \n",
       "\n",
       "   DN_mul_FI  AZ_mul_FE  DN_mul_EL  CW _mul_EL  AZ_mul_CU  CW _mul_DY  \\\n",
       "0   0.408219  -1.106440  -0.078924   -0.078924  -0.492757   -0.359226   \n",
       "1  -0.222751  -0.078924   0.952480   -0.078924   0.318173   -0.098342   \n",
       "\n",
       "   DH_mul_DL  AX_mul_CU  DY_mul_FI  BN_mul_DE  BN_mul_CW   BN_mul_DY  \\\n",
       "0   0.016386   0.244065   0.733556  -0.883297    0.429726  -0.222751   \n",
       "1  -0.511106  -0.665257  -0.222751  -0.340534   -0.829229  -0.340534   \n",
       "\n",
       "   AZ_mul_EL  AZ_mul_DE  Class     k  \n",
       "0   0.408219  -0.203702      1  BCGD  \n",
       "1  -0.222751  -0.683275      0  ACMB  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DU</th>\n",
       "      <th>GL</th>\n",
       "      <th>CR</th>\n",
       "      <th>DA</th>\n",
       "      <th>AF</th>\n",
       "      <th>EE</th>\n",
       "      <th>FR</th>\n",
       "      <th>DE</th>\n",
       "      <th>CC</th>\n",
       "      <th>BN</th>\n",
       "      <th>FI</th>\n",
       "      <th>FE</th>\n",
       "      <th>DH</th>\n",
       "      <th>EU</th>\n",
       "      <th>GH</th>\n",
       "      <th>DY</th>\n",
       "      <th>CB</th>\n",
       "      <th>FC</th>\n",
       "      <th>BR</th>\n",
       "      <th>FS</th>\n",
       "      <th>AZ</th>\n",
       "      <th>CW</th>\n",
       "      <th>EG</th>\n",
       "      <th>DU_mul_FE</th>\n",
       "      <th>DA_mul_DU</th>\n",
       "      <th>CR_mul_DE</th>\n",
       "      <th>DI_mul_FE</th>\n",
       "      <th>BQ_mul_FE</th>\n",
       "      <th>BQ_mul_CB</th>\n",
       "      <th>BQ_mul_CW</th>\n",
       "      <th>DA_mul_DL</th>\n",
       "      <th>CR_mul_GE</th>\n",
       "      <th>EE_mul_GF</th>\n",
       "      <th>CR_mul_FE</th>\n",
       "      <th>BQ_mul_FC</th>\n",
       "      <th>DE_mul_GE</th>\n",
       "      <th>CD _mul_DF</th>\n",
       "      <th>CR_mul_GI</th>\n",
       "      <th>DE_mul_DL</th>\n",
       "      <th>DL_mul_EL</th>\n",
       "      <th>BN_mul_BQ</th>\n",
       "      <th>BN_mul_DA</th>\n",
       "      <th>CU_mul_DN</th>\n",
       "      <th>AZ_mul_GL</th>\n",
       "      <th>CW _mul_DL</th>\n",
       "      <th>DE_mul_EL</th>\n",
       "      <th>CW _mul_EE</th>\n",
       "      <th>BN_mul_CR</th>\n",
       "      <th>DE_mul_GF</th>\n",
       "      <th>DH_mul_EG</th>\n",
       "      <th>CC_mul_EL</th>\n",
       "      <th>CU_mul_EG</th>\n",
       "      <th>DN_mul_FI</th>\n",
       "      <th>AZ_mul_FE</th>\n",
       "      <th>DN_mul_EL</th>\n",
       "      <th>CW _mul_EL</th>\n",
       "      <th>AZ_mul_CU</th>\n",
       "      <th>CW _mul_DY</th>\n",
       "      <th>DH_mul_DL</th>\n",
       "      <th>AX_mul_CU</th>\n",
       "      <th>DY_mul_FI</th>\n",
       "      <th>BN_mul_DE</th>\n",
       "      <th>BN_mul_CW</th>\n",
       "      <th>BN_mul_DY</th>\n",
       "      <th>AZ_mul_EL</th>\n",
       "      <th>AZ_mul_DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.408497</td>\n",
       "      <td>1.809828</td>\n",
       "      <td>1.679529</td>\n",
       "      <td>1.809828</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>-0.173725</td>\n",
       "      <td>1.290806</td>\n",
       "      <td>1.022249</td>\n",
       "      <td>-0.735461</td>\n",
       "      <td>0.95248</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>1.208567</td>\n",
       "      <td>-0.677305</td>\n",
       "      <td>0.297554</td>\n",
       "      <td>-1.123832</td>\n",
       "      <td>0.576868</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>0.318173</td>\n",
       "      <td>0.068712</td>\n",
       "      <td>-0.03989</td>\n",
       "      <td>0.228561</td>\n",
       "      <td>-0.098342</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>1.679529</td>\n",
       "      <td>1.679529</td>\n",
       "      <td>1.158275</td>\n",
       "      <td>0.656499</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>1.158275</td>\n",
       "      <td>1.356161</td>\n",
       "      <td>1.022249</td>\n",
       "      <td>0.494264</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>1.290806</td>\n",
       "      <td>0.494264</td>\n",
       "      <td>0.656499</td>\n",
       "      <td>1.158275</td>\n",
       "      <td>-0.883297</td>\n",
       "      <td>0.733556</td>\n",
       "      <td>0.318173</td>\n",
       "      <td>0.408219</td>\n",
       "      <td>0.808379</td>\n",
       "      <td>0.733556</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>0.95248</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>0.408219</td>\n",
       "      <td>0.408219</td>\n",
       "      <td>0.95248</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>0.95248</td>\n",
       "      <td>-0.098342</td>\n",
       "      <td>0.576868</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>0.656499</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>-0.628216</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.656499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.408497</td>\n",
       "      <td>1.809828</td>\n",
       "      <td>1.679529</td>\n",
       "      <td>1.809828</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>-0.173725</td>\n",
       "      <td>1.290806</td>\n",
       "      <td>1.022249</td>\n",
       "      <td>-0.735461</td>\n",
       "      <td>0.95248</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>1.208567</td>\n",
       "      <td>-0.677305</td>\n",
       "      <td>0.297554</td>\n",
       "      <td>-1.123832</td>\n",
       "      <td>0.576868</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>0.318173</td>\n",
       "      <td>0.068712</td>\n",
       "      <td>-0.03989</td>\n",
       "      <td>0.228561</td>\n",
       "      <td>-0.098342</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>1.679529</td>\n",
       "      <td>1.679529</td>\n",
       "      <td>1.158275</td>\n",
       "      <td>0.656499</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>1.158275</td>\n",
       "      <td>1.356161</td>\n",
       "      <td>1.022249</td>\n",
       "      <td>0.494264</td>\n",
       "      <td>-0.359226</td>\n",
       "      <td>1.290806</td>\n",
       "      <td>0.494264</td>\n",
       "      <td>0.656499</td>\n",
       "      <td>1.158275</td>\n",
       "      <td>-0.883297</td>\n",
       "      <td>0.733556</td>\n",
       "      <td>0.318173</td>\n",
       "      <td>0.408219</td>\n",
       "      <td>0.808379</td>\n",
       "      <td>0.733556</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>0.95248</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>0.408219</td>\n",
       "      <td>0.408219</td>\n",
       "      <td>0.95248</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>0.95248</td>\n",
       "      <td>-0.098342</td>\n",
       "      <td>0.576868</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>0.656499</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>-0.511106</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>-0.628216</td>\n",
       "      <td>-0.222751</td>\n",
       "      <td>0.016386</td>\n",
       "      <td>0.656499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DU        GL        CR        DA        AF        EE        FR  \\\n",
       "0 -0.408497  1.809828  1.679529  1.809828 -0.511106  0.881267 -0.173725   \n",
       "1 -0.408497  1.809828  1.679529  1.809828 -0.511106  0.881267 -0.173725   \n",
       "\n",
       "         DE        CC        BN       FI        FE        DH        EU  \\\n",
       "0  1.290806  1.022249 -0.735461  0.95248 -0.511106  1.208567 -0.677305   \n",
       "1  1.290806  1.022249 -0.735461  0.95248 -0.511106  1.208567 -0.677305   \n",
       "\n",
       "         GH        DY        CB        FC        BR        FS       AZ  \\\n",
       "0  0.297554 -1.123832  0.576868 -0.359226  0.318173  0.068712 -0.03989   \n",
       "1  0.297554 -1.123832  0.576868 -0.359226  0.318173  0.068712 -0.03989   \n",
       "\n",
       "        CW         EG  DU_mul_FE  DA_mul_DU  CR_mul_DE  DI_mul_FE  BQ_mul_FE  \\\n",
       "0  0.228561 -0.098342   0.881267   1.679529   1.679529   1.158275   0.656499   \n",
       "1  0.228561 -0.098342   0.881267   1.679529   1.679529   1.158275   0.656499   \n",
       "\n",
       "   BQ_mul_CB  BQ_mul_CW   DA_mul_DL  CR_mul_GE  EE_mul_GF  CR_mul_FE  \\\n",
       "0  -0.359226      0.1232   1.158275   1.356161   1.022249   0.494264   \n",
       "1  -0.359226      0.1232   1.158275   1.356161   1.022249   0.494264   \n",
       "\n",
       "   BQ_mul_FC  DE_mul_GE  CD _mul_DF  CR_mul_GI  DE_mul_DL  DL_mul_EL  \\\n",
       "0  -0.359226   1.290806    0.494264   0.656499   1.158275  -0.883297   \n",
       "1  -0.359226   1.290806    0.494264   0.656499   1.158275  -0.883297   \n",
       "\n",
       "   BN_mul_BQ  BN_mul_DA  CU_mul_DN  AZ_mul_GL  CW _mul_DL  DE_mul_EL  \\\n",
       "0   0.733556   0.318173   0.408219   0.808379    0.733556  -0.222751   \n",
       "1   0.733556   0.318173   0.408219   0.808379    0.733556  -0.222751   \n",
       "\n",
       "   CW _mul_EE  BN_mul_CR  DE_mul_GF  DH_mul_EG  CC_mul_EL  CU_mul_EG  \\\n",
       "0     0.95248   0.016386   0.881267   0.881267   0.408219   0.408219   \n",
       "1     0.95248   0.016386   0.881267   0.881267   0.408219   0.408219   \n",
       "\n",
       "   DN_mul_FI  AZ_mul_FE  DN_mul_EL  CW _mul_EL  AZ_mul_CU  CW _mul_DY  \\\n",
       "0    0.95248     0.1232    0.95248   -0.098342   0.576868   -0.222751   \n",
       "1    0.95248     0.1232    0.95248   -0.098342   0.576868   -0.222751   \n",
       "\n",
       "   DH_mul_DL  AX_mul_CU  DY_mul_FI  BN_mul_DE  BN_mul_CW   BN_mul_DY  \\\n",
       "0   0.656499  -0.222751  -0.511106     0.1232   -0.628216  -0.222751   \n",
       "1   0.656499  -0.222751  -0.511106     0.1232   -0.628216  -0.222751   \n",
       "\n",
       "   AZ_mul_EL  AZ_mul_DE  \n",
       "0   0.016386   0.656499  \n",
       "1   0.016386   0.656499  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_original.head(2))\n",
    "display(df_teste2.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdf547b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:04.310587Z",
     "iopub.status.busy": "2023-08-10T15:17:04.309957Z",
     "iopub.status.idle": "2023-08-10T15:17:04.331627Z",
     "shell.execute_reply": "2023-08-10T15:17:04.330515Z"
    },
    "papermill": {
     "duration": 0.06842,
     "end_time": "2023-08-10T15:17:04.334060",
     "exception": false,
     "start_time": "2023-08-10T15:17:04.265640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>pred_4</th>\n",
       "      <th>pred_5</th>\n",
       "      <th>pred_6</th>\n",
       "      <th>pred_7</th>\n",
       "      <th>pred_8</th>\n",
       "      <th>pred_9</th>\n",
       "      <th>pred_10</th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.998912</td>\n",
       "      <td>0.99811</td>\n",
       "      <td>0.997878</td>\n",
       "      <td>0.998852</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.998022</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.998251</td>\n",
       "      <td>0.998584</td>\n",
       "      <td>0.998253</td>\n",
       "      <td>0.99842</td>\n",
       "      <td>0.00158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.998912</td>\n",
       "      <td>0.99811</td>\n",
       "      <td>0.997878</td>\n",
       "      <td>0.998852</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.998022</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.998251</td>\n",
       "      <td>0.998584</td>\n",
       "      <td>0.998253</td>\n",
       "      <td>0.99842</td>\n",
       "      <td>0.00158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.998912</td>\n",
       "      <td>0.99811</td>\n",
       "      <td>0.997878</td>\n",
       "      <td>0.998852</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.998022</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.998251</td>\n",
       "      <td>0.998584</td>\n",
       "      <td>0.998253</td>\n",
       "      <td>0.99842</td>\n",
       "      <td>0.00158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.998912</td>\n",
       "      <td>0.99811</td>\n",
       "      <td>0.997878</td>\n",
       "      <td>0.998852</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.998022</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.998251</td>\n",
       "      <td>0.998584</td>\n",
       "      <td>0.998253</td>\n",
       "      <td>0.99842</td>\n",
       "      <td>0.00158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.998912</td>\n",
       "      <td>0.99811</td>\n",
       "      <td>0.997878</td>\n",
       "      <td>0.998852</td>\n",
       "      <td>0.999001</td>\n",
       "      <td>0.998022</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.998251</td>\n",
       "      <td>0.998584</td>\n",
       "      <td>0.998253</td>\n",
       "      <td>0.99842</td>\n",
       "      <td>0.00158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id    pred_1   pred_2    pred_3    pred_4    pred_5    pred_6  \\\n",
       "0  00eed32682bb  0.998912  0.99811  0.997878  0.998852  0.999001  0.998022   \n",
       "1  010ebe33f668  0.998912  0.99811  0.997878  0.998852  0.999001  0.998022   \n",
       "2  02fa521e1838  0.998912  0.99811  0.997878  0.998852  0.999001  0.998022   \n",
       "3  040e15f562a2  0.998912  0.99811  0.997878  0.998852  0.999001  0.998022   \n",
       "4  046e85c7cc7f  0.998912  0.99811  0.997878  0.998852  0.999001  0.998022   \n",
       "\n",
       "     pred_7    pred_8    pred_9   pred_10  class_1  class_0  \n",
       "0  0.998333  0.998251  0.998584  0.998253  0.99842  0.00158  \n",
       "1  0.998333  0.998251  0.998584  0.998253  0.99842  0.00158  \n",
       "2  0.998333  0.998251  0.998584  0.998253  0.99842  0.00158  \n",
       "3  0.998333  0.998251  0.998584  0.998253  0.99842  0.00158  \n",
       "4  0.998333  0.998251  0.998584  0.998253  0.99842  0.00158  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5_preds[\"class_1\"] = model5_preds.drop(\"Id\", axis=1).mean(axis=1).tolist()\n",
    "model5_preds[\"class_0\"] = (1 - model5_preds[\"class_1\"]).tolist()\n",
    "model5_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3669aa6",
   "metadata": {
    "papermill": {
     "duration": 0.04458,
     "end_time": "2023-08-10T15:17:04.427063",
     "exception": false,
     "start_time": "2023-08-10T15:17:04.382483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc427404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:04.515616Z",
     "iopub.status.busy": "2023-08-10T15:17:04.515317Z",
     "iopub.status.idle": "2023-08-10T15:17:05.051794Z",
     "shell.execute_reply": "2023-08-10T15:17:05.050908Z"
    },
    "papermill": {
     "duration": 0.584204,
     "end_time": "2023-08-10T15:17:05.054655",
     "exception": false,
     "start_time": "2023-08-10T15:17:04.470451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 768x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18120809674083782\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(dpi=120)\n",
    "sns.lineplot(np.sort(model1_oof), ax=ax, label=\"LightGBM+HardPointCV\")\n",
    "sns.lineplot(np.sort(model2_oof), ax=ax, label=\"XGBoost+NestedCV\")\n",
    "sns.lineplot(np.sort(model3_oof), ax=ax, label=\"TabPFN\")\n",
    "sns.lineplot(np.sort(model4_oof), ax=ax, label=\"LightGBM+NestedCV\")\n",
    "sns.lineplot(np.sort(model5_oof), ax=ax, label=\"LogisticRegression+WoE\")\n",
    "sns.lineplot(np.sort((model1_oof+model2_oof+model3_oof+model4_oof+model5_oof)/5), ax=ax, label=\"Simple Average\")\n",
    "sns.lineplot([0]*509 + [1]*108, ax=ax)\n",
    "ax.set_ylabel(\"proba\")\n",
    "plt.show()\n",
    "\n",
    "print(metric(df_train[\"Class\"].to_numpy(), (model1_oof+model2_oof+model3_oof+model4_oof+model5_oof)/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae4a5d",
   "metadata": {
    "papermill": {
     "duration": 0.045197,
     "end_time": "2023-08-10T15:17:05.145108",
     "exception": false,
     "start_time": "2023-08-10T15:17:05.099911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02c123a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:05.235940Z",
     "iopub.status.busy": "2023-08-10T15:17:05.235618Z",
     "iopub.status.idle": "2023-08-10T15:17:05.241264Z",
     "shell.execute_reply": "2023-08-10T15:17:05.240246Z"
    },
    "papermill": {
     "duration": 0.053984,
     "end_time": "2023-08-10T15:17:05.243485",
     "exception": false,
     "start_time": "2023-08-10T15:17:05.189501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# result = []\n",
    "# ws = list(range(1, 11))\n",
    "# thresh_holds = [i/10 for i in range(1, 10)]\n",
    "\n",
    "# for w in ws:\n",
    "#     for th in thresh_holds:\n",
    "#         r = []\n",
    "#         for l, x in zip(model1_oof, model2_oof):\n",
    "#             ms = np.mean(x+l)\n",
    "#             if ms<th:\n",
    "#                 r.append(((w-1)*l + x)/w)\n",
    "#             else:\n",
    "#                 r.append(((w-1)*x + l)/w)        \n",
    "#         s = metric(df_train[\"Class\"].to_numpy(), np.array(r))\n",
    "#         result.append([w, th, s])\n",
    "\n",
    "# result = pd.DataFrame(result, columns=[\"weight\", \"thresh_hold\", \"SCORE\"]).sort_values(\"SCORE\")\n",
    "\n",
    "# bw = result.iloc[0][\"weight\"]\n",
    "# bth = result.iloc[0][\"thresh_hold\"]\n",
    "# df_oof = np.zeros(len(df_train))\n",
    "# for i, (l, x) in enumerate(zip(model1_oof, model2_oof)):\n",
    "#     ms = np.mean(x+l)\n",
    "#     if ms<bth: # 00\n",
    "#         df_oof[i] = ((bw-1)*l + x)/bw\n",
    "#     else: # 11\n",
    "#         df_oof[i] = ((bw-1)*x + l)/bw      \n",
    "# df_oof = pd.DataFrame(df_oof, columns=[\"proba\"])\n",
    "# df_oof[\"Class\"] = df_train[\"Class\"].tolist()\n",
    "\n",
    "# fig, ax = plt.subplots(dpi=120)\n",
    "# sns.lineplot(np.sort(model1_oof), ax=ax, label=\"LightGBM+HardPointCV\")\n",
    "# sns.lineplot(np.sort(model2_oof), ax=ax, label=\"XGBoost+NestedCV\")\n",
    "# sns.lineplot(np.sort(df_oof[\"proba\"].to_numpy()), ax=ax, label=\"ensemble\")\n",
    "# sns.lineplot([0]*509 + [1]*108, ax=ax)\n",
    "# ax.set_ylabel(\"proba\")\n",
    "# plt.show()\n",
    "\n",
    "# print(bw, bth)\n",
    "# print(metric(df_oof[\"Class\"].to_numpy(), df_oof[\"proba\"].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2278d25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:05.333835Z",
     "iopub.status.busy": "2023-08-10T15:17:05.333550Z",
     "iopub.status.idle": "2023-08-10T15:17:05.337706Z",
     "shell.execute_reply": "2023-08-10T15:17:05.336893Z"
    },
    "papermill": {
     "duration": 0.051249,
     "end_time": "2023-08-10T15:17:05.339546",
     "exception": false,
     "start_time": "2023-08-10T15:17:05.288297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "# df_result = np.zeros(len(submission))\n",
    "\n",
    "# for i, (l, x) in enumerate(zip(model1_preds[\"class_1\"], model2_preds[\"class_1\"])):\n",
    "#     ms = np.mean(x+l)\n",
    "#     if ms<bth: # 00\n",
    "#         df_result[i] = ((bw-1)*l + x)/bw\n",
    "#     else: # 11\n",
    "#         df_result[i] = ((bw-1)*x + l)/bw      \n",
    "# df_result = pd.DataFrame(df_result, columns=[\"proba\"])\n",
    "# df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8969037",
   "metadata": {
    "papermill": {
     "duration": 0.045512,
     "end_time": "2023-08-10T15:17:05.429506",
     "exception": false,
     "start_time": "2023-08-10T15:17:05.383994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cf90bbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:05.520350Z",
     "iopub.status.busy": "2023-08-10T15:17:05.520035Z",
     "iopub.status.idle": "2023-08-10T15:17:05.526676Z",
     "shell.execute_reply": "2023-08-10T15:17:05.525690Z"
    },
    "papermill": {
     "duration": 0.054745,
     "end_time": "2023-08-10T15:17:05.528445",
     "exception": false,
     "start_time": "2023-08-10T15:17:05.473700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# df_sub = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "\n",
    "# solver=\"liblinear\"\n",
    "# class_weight=\"balanced\"\n",
    "# penalty=\"l2\"\n",
    "# l1_ratio = None\n",
    "# max_iter = 1000\n",
    "# random_state = 13\n",
    "# params = [0.001, 0.01, 0.1, 1.0, 10, 100] + [i for i in range(2, 10)]\n",
    "\n",
    "# \"\"\"\n",
    "# model1LightGBM + HardPointCV\n",
    "# model2XGBoost + NestedCV\n",
    "# model3TabPFN\n",
    "# model4LightGBM + NestedCV\n",
    "# model5:LinearRegression + WoV\n",
    "# \"\"\"\n",
    "\n",
    "# X = np.array([\n",
    "#     model1_oof, \n",
    "#     #model2_oof, \n",
    "#     model3_oof, \n",
    "#     #model4_oof,\n",
    "#     model5_oof\n",
    "# ]).T\n",
    "# X_sub = np.array([\n",
    "#     model1_preds[\"class_1\"].to_numpy(), \n",
    "#     #model2_preds[\"class_1\"].to_numpy(), \n",
    "#     model3_preds[\"class_1\"].to_numpy(), \n",
    "#     #model4_preds[\"class_1\"].to_numpy(),\n",
    "#     model5_preds[\"class_1\"].to_numpy()\n",
    "# ]).T\n",
    "\n",
    "# y = df_train[\"Class\"].to_numpy()\n",
    "\n",
    "# def lr_nested_cv(X, y, params):\n",
    "#     nested_scores, best_cv_params = [], []\n",
    "#     outer_cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "#     df_preds = df_sub[[\"Id\"]].copy()\n",
    "#     oofs = np.zeros(len(y))\n",
    "#     # CV\n",
    "#     for i, (train_inds, test_inds) in enumerate(outer_cv.split(X, y)):\n",
    "#         print(f\"***** Out CV {i} *****\")\n",
    "#         X_train_out, y_train_out = X[train_inds], y[train_inds]\n",
    "#         X_test, y_test = X[test_inds], y[test_inds]\n",
    "\n",
    "#         cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "#         best_params, best_score = None, 1000000\n",
    "#         for C in params:\n",
    "#             scores = []\n",
    "#             for j, (train_idx, val_idx) in enumerate(cv.split(X_train_out, y_train_out)):\n",
    "#                 X_train_in, y_train_in = X_train_out[train_idx], y_train_out[train_idx]\n",
    "#                 X_val, y_val = X_train_out[val_idx], y_train_out[val_idx]\n",
    "\n",
    "#                 model = LogisticRegression(C=C, random_state=random_state, l1_ratio=l1_ratio,\n",
    "#                                      solver=solver, class_weight=class_weight, penalty=penalty, max_iter=max_iter)\n",
    "#                 model.fit(X_train_in, y_train_in)\n",
    "#                 y_pred = model.predict_proba(X_val)[:, 1]\n",
    "#                 score = metric(y_val, y_pred)\n",
    "#                 scores.append(score)\n",
    "\n",
    "#             # outFoldC\n",
    "#             cv_score = np.mean(scores)\n",
    "#             if best_score > cv_score:\n",
    "#                 best_params = C\n",
    "#                 best_score = cv_score\n",
    "\n",
    "#         print(f\"params{best_params}, score{best_score}\")\n",
    "#         model = LogisticRegression(C=best_params, random_state=random_state, l1_ratio=l1_ratio,\n",
    "#                                  solver=solver, class_weight=class_weight, penalty=penalty, max_iter=max_iter)\n",
    "#         model.fit(X_train_out, y_train_out)\n",
    "#         y_pred = model.predict_proba(X_test)[:, 1]\n",
    "#         oofs[test_inds] = y_pred\n",
    "#         score = metric(y_test, y_pred)\n",
    "#         nested_scores.append(score)\n",
    "#         best_cv_params.append(best_params)\n",
    "        \n",
    "#         # \n",
    "#         pred = model.predict_proba(X_sub)[:, 1]\n",
    "#         df_preds[f\"pred_{i+1}\"] = pred\n",
    "#     return nested_scores, best_cv_params, df_preds, oofs\n",
    "\n",
    "# nested_scores, best_cv_params, df_preds, oofs = lr_nested_cv(X, y, params)\n",
    "# print(np.mean(nested_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12d57ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:05.619248Z",
     "iopub.status.busy": "2023-08-10T15:17:05.618459Z",
     "iopub.status.idle": "2023-08-10T15:17:05.623075Z",
     "shell.execute_reply": "2023-08-10T15:17:05.622199Z"
    },
    "papermill": {
     "duration": 0.051782,
     "end_time": "2023-08-10T15:17:05.624890",
     "exception": false,
     "start_time": "2023-08-10T15:17:05.573108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "# submission[\"class_1\"] = df_preds.drop(\"Id\", axis=1).mean(axis=1).tolist()\n",
    "# submission[\"class_0\"] = (1 - submission[\"class_1\"]).tolist()\n",
    "# display(submission.head())\n",
    "# submission.to_csv(f\"submission.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4393c5e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:05.718872Z",
     "iopub.status.busy": "2023-08-10T15:17:05.718545Z",
     "iopub.status.idle": "2023-08-10T15:17:05.722812Z",
     "shell.execute_reply": "2023-08-10T15:17:05.721879Z"
    },
    "papermill": {
     "duration": 0.052819,
     "end_time": "2023-08-10T15:17:05.724977",
     "exception": false,
     "start_time": "2023-08-10T15:17:05.672158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission[\"class_1\"] = df_result[\"proba\"].tolist()\n",
    "# submission[\"class_0\"] = (1 - submission[\"class_1\"]).tolist()\n",
    "# display(submission.head())\n",
    "# submission.to_csv(f\"submission.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a7529",
   "metadata": {
    "papermill": {
     "duration": 0.04451,
     "end_time": "2023-08-10T15:17:05.815843",
     "exception": false,
     "start_time": "2023-08-10T15:17:05.771333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### scipy.minimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a603f13b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:05.907636Z",
     "iopub.status.busy": "2023-08-10T15:17:05.907195Z",
     "iopub.status.idle": "2023-08-10T15:17:05.913345Z",
     "shell.execute_reply": "2023-08-10T15:17:05.912401Z"
    },
    "papermill": {
     "duration": 0.054752,
     "end_time": "2023-08-10T15:17:05.915301",
     "exception": false,
     "start_time": "2023-08-10T15:17:05.860549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from scipy.optimize import minimize\n",
    "\n",
    "# def bl_min_func(weights):\n",
    "#     final_prediction = 0\n",
    "#     for weight, prediction in zip(weights, blend_train):# blend_trainoof\n",
    "#         final_prediction += weight * prediction\n",
    "#     return balance_logloss(y_train, final_prediction)\n",
    "\n",
    "# blend_train = np.array([model1_oof, model2_oof, model3_oof, model4_oof, model5_oof])\n",
    "# blend_test = np.array([model1_preds[\"class_1\"].to_numpy(), model2_preds[\"class_1\"].to_numpy(), \n",
    "#                        model3_preds[\"class_1\"].to_numpy(), model4_preds[\"class_1\"].to_numpy(),\n",
    "#                       model5_preds[\"class_1\"].to_numpy()])\n",
    "\n",
    "# y_train = df_train[\"Class\"]\n",
    "\n",
    "# res_list = []\n",
    "# weights_list = []\n",
    "# for k in range(300):\n",
    "#     starting_values = np.random.uniform(size=len(blend_train))\n",
    "#     cons = ({'type':'eq', 'fun':lambda x: 1-sum(x)})\n",
    "#     bounds = [(0, 1)] * len(blend_train)\n",
    "#     res = minimize(bl_min_func,\n",
    "#                    starting_values,\n",
    "#                    method=\"SLSQP\",\n",
    "#                    #method=\"Nelder-Mead\",\n",
    "#                    bounds=bounds,\n",
    "#                    constraints=cons\n",
    "#                    )\n",
    "\n",
    "#     res_list.append(res['fun'])\n",
    "#     weights_list.append(res['x'])\n",
    "#     if k%100==0:\n",
    "#         print('{iter}\\tScore: {score}\\tWeights: {weights}'.format(\n",
    "#             iter=(k + 1),\n",
    "#             score=res['fun'],\n",
    "#             weights='\\t'.join([str(item) for item in res['x']])))\n",
    "\n",
    "# bestSC = np.min(res_list)\n",
    "# bestWght = weights_list[np.argmin(res_list)]\n",
    "# weights = bestWght\n",
    "# blend_score = round(bestSC, 6)\n",
    "\n",
    "# print('\\n Ensemble Score: {best_score}'.format(best_score=bestSC))\n",
    "# print('\\n Best Weights: {weights}'.format(weights=bestWght))\n",
    "\n",
    "# train_prices = np.zeros(len(blend_train[0]))\n",
    "# test_prices = np.zeros(len(blend_test[0]))\n",
    "\n",
    "# print('\\n Your final model:')\n",
    "# for k in range(len(blend_test)):\n",
    "#     print(' %.6f * model-%d' % (weights[k], (k + 1)))\n",
    "#     test_prices += blend_test[k] * weights[k]\n",
    "\n",
    "# for k in range(len(blend_train)):\n",
    "#     train_prices += blend_train[k] * weights[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58f923e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:06.005687Z",
     "iopub.status.busy": "2023-08-10T15:17:06.004899Z",
     "iopub.status.idle": "2023-08-10T15:17:06.009786Z",
     "shell.execute_reply": "2023-08-10T15:17:06.008932Z"
    },
    "papermill": {
     "duration": 0.052261,
     "end_time": "2023-08-10T15:17:06.011880",
     "exception": false,
     "start_time": "2023-08-10T15:17:05.959619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission = argsortpd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "# df_result = pd.DataFrame()\n",
    "# df_result[\"proba\"] = np.clip(test_prices, 1e-15, 1-1e-15)\n",
    "\n",
    "# submission[\"class_1\"] = df_result[\"proba\"].tolist()\n",
    "# submission[\"class_0\"] = (1 - submission[\"class_1\"]).tolist()\n",
    "# display(submission.head())\n",
    "# submission.to_csv(f\"submission.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b91d9",
   "metadata": {
    "papermill": {
     "duration": 0.044282,
     "end_time": "2023-08-10T15:17:06.100929",
     "exception": false,
     "start_time": "2023-08-10T15:17:06.056647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LR + hardpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec5118a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:06.191474Z",
     "iopub.status.busy": "2023-08-10T15:17:06.191150Z",
     "iopub.status.idle": "2023-08-10T15:17:06.692662Z",
     "shell.execute_reply": "2023-08-10T15:17:06.691430Z"
    },
    "papermill": {
     "duration": 0.549809,
     "end_time": "2023-08-10T15:17:06.695521",
     "exception": false,
     "start_time": "2023-08-10T15:17:06.145712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.1537542752855475\n"
     ]
    }
   ],
   "source": [
    "from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df_sub = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "\n",
    "def hardpoint_cv(X, y, params):\n",
    "    best_C, best_score = None, 10000 \n",
    "    oofs = np.zeros(len(y))\n",
    "    df_preds = df_sub[[\"Id\"]].copy()\n",
    "    for C in params:\n",
    "        scores = []\n",
    "        cv = RepeatedMultilabelStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "        for i, (train_index, val_index) in enumerate(cv.split(X, labels)):\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_val, y_val = X[val_index], y[val_index]\n",
    "\n",
    "            model = LogisticRegression(C=C, random_state=random_state, l1_ratio=l1_ratio,\n",
    "                                solver=solver, class_weight=class_weight, penalty=penalty, max_iter=max_iter)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict_proba(X_val)[:, 1]\n",
    "            oofs[val_index] += y_pred / n_repeats\n",
    "            score = metric(y_val, y_pred)\n",
    "            scores.append(score)\n",
    "            \n",
    "            #\n",
    "            pred = model.predict_proba(X_sub)[:, 1]\n",
    "            df_preds[f\"pred_{i+1}\"] = pred\n",
    "        if np.mean(scores) < best_score:\n",
    "            best_C = C\n",
    "            best_score = np.mean(scores)\n",
    "            best_model = model\n",
    "    return best_C, best_score, oofs, df_preds\n",
    "\n",
    "solver=\"liblinear\"\n",
    "class_weight=\"balanced\"\n",
    "penalty=\"l2\"\n",
    "l1_ratio = None\n",
    "max_iter = 1000\n",
    "random_state = 13\n",
    "params = [0.001, 0.01, 0.1, 1.0, 10] + [i*10 for i in range(2, 10)] + [i for i in range(2, 10)]\n",
    "n_splits = 10\n",
    "n_repeats = 10\n",
    "random_state = 13*13\n",
    "\"\"\"\n",
    "model1LightGBM + HardPointCV\n",
    "model2XGBoost + NestedCV\n",
    "model3TabPFN\n",
    "model4LightGBM + NestedCV\n",
    "model5:LinearRegression + WoV\n",
    "\"\"\"\n",
    "\n",
    "X = np.array([\n",
    "    model1_oof, \n",
    "    #model2_oof, \n",
    "    model3_oof, \n",
    "    #model4_oof,\n",
    "    model5_oof\n",
    "]).T\n",
    "X_sub = np.array([\n",
    "    model1_preds[\"class_1\"].to_numpy(), \n",
    "    #model2_preds[\"class_1\"].to_numpy(), \n",
    "    model3_preds[\"class_1\"].to_numpy(), \n",
    "    #model4_preds[\"class_1\"].to_numpy(),\n",
    "    model5_preds[\"class_1\"].to_numpy()\n",
    "]).T\n",
    "\n",
    "y = df_train[\"Class\"].to_numpy()\n",
    "hp = [509, 313, 479, 267, 408, 193, 145, 229, 31, 434]\n",
    "hps = [1 if i in hp else 0 for i in range(len(df_train))]\n",
    "df_train[\"HardPoint\"] = hps\n",
    "labels = pd.DataFrame({\"Class\":y, \"hp\":hps})\n",
    "\n",
    "best_C, best_score, hp_oofs, df_preds = hardpoint_cv(X, y, [3])\n",
    "print(best_C, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d127d1aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:06.788573Z",
     "iopub.status.busy": "2023-08-10T15:17:06.788215Z",
     "iopub.status.idle": "2023-08-10T15:17:06.873835Z",
     "shell.execute_reply": "2023-08-10T15:17:06.872769Z"
    },
    "papermill": {
     "duration": 0.135352,
     "end_time": "2023-08-10T15:17:06.876748",
     "exception": false,
     "start_time": "2023-08-10T15:17:06.741396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>pred_4</th>\n",
       "      <th>pred_5</th>\n",
       "      <th>pred_6</th>\n",
       "      <th>pred_7</th>\n",
       "      <th>pred_8</th>\n",
       "      <th>pred_9</th>\n",
       "      <th>pred_10</th>\n",
       "      <th>pred_11</th>\n",
       "      <th>pred_12</th>\n",
       "      <th>pred_13</th>\n",
       "      <th>pred_14</th>\n",
       "      <th>pred_15</th>\n",
       "      <th>pred_16</th>\n",
       "      <th>pred_17</th>\n",
       "      <th>pred_18</th>\n",
       "      <th>pred_19</th>\n",
       "      <th>pred_20</th>\n",
       "      <th>pred_21</th>\n",
       "      <th>pred_22</th>\n",
       "      <th>pred_23</th>\n",
       "      <th>pred_24</th>\n",
       "      <th>pred_25</th>\n",
       "      <th>pred_26</th>\n",
       "      <th>pred_27</th>\n",
       "      <th>pred_28</th>\n",
       "      <th>pred_29</th>\n",
       "      <th>pred_30</th>\n",
       "      <th>pred_31</th>\n",
       "      <th>pred_32</th>\n",
       "      <th>pred_33</th>\n",
       "      <th>pred_34</th>\n",
       "      <th>pred_35</th>\n",
       "      <th>pred_36</th>\n",
       "      <th>pred_37</th>\n",
       "      <th>pred_38</th>\n",
       "      <th>pred_39</th>\n",
       "      <th>pred_40</th>\n",
       "      <th>pred_41</th>\n",
       "      <th>pred_42</th>\n",
       "      <th>pred_43</th>\n",
       "      <th>pred_44</th>\n",
       "      <th>pred_45</th>\n",
       "      <th>pred_46</th>\n",
       "      <th>pred_47</th>\n",
       "      <th>pred_48</th>\n",
       "      <th>pred_49</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_51</th>\n",
       "      <th>pred_52</th>\n",
       "      <th>pred_53</th>\n",
       "      <th>pred_54</th>\n",
       "      <th>pred_55</th>\n",
       "      <th>pred_56</th>\n",
       "      <th>pred_57</th>\n",
       "      <th>pred_58</th>\n",
       "      <th>pred_59</th>\n",
       "      <th>pred_60</th>\n",
       "      <th>pred_61</th>\n",
       "      <th>pred_62</th>\n",
       "      <th>pred_63</th>\n",
       "      <th>pred_64</th>\n",
       "      <th>pred_65</th>\n",
       "      <th>pred_66</th>\n",
       "      <th>pred_67</th>\n",
       "      <th>pred_68</th>\n",
       "      <th>pred_69</th>\n",
       "      <th>pred_70</th>\n",
       "      <th>pred_71</th>\n",
       "      <th>pred_72</th>\n",
       "      <th>pred_73</th>\n",
       "      <th>pred_74</th>\n",
       "      <th>pred_75</th>\n",
       "      <th>pred_76</th>\n",
       "      <th>pred_77</th>\n",
       "      <th>pred_78</th>\n",
       "      <th>pred_79</th>\n",
       "      <th>pred_80</th>\n",
       "      <th>pred_81</th>\n",
       "      <th>pred_82</th>\n",
       "      <th>pred_83</th>\n",
       "      <th>pred_84</th>\n",
       "      <th>pred_85</th>\n",
       "      <th>pred_86</th>\n",
       "      <th>pred_87</th>\n",
       "      <th>pred_88</th>\n",
       "      <th>pred_89</th>\n",
       "      <th>pred_90</th>\n",
       "      <th>pred_91</th>\n",
       "      <th>pred_92</th>\n",
       "      <th>pred_93</th>\n",
       "      <th>pred_94</th>\n",
       "      <th>pred_95</th>\n",
       "      <th>pred_96</th>\n",
       "      <th>pred_97</th>\n",
       "      <th>pred_98</th>\n",
       "      <th>pred_99</th>\n",
       "      <th>pred_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.628662</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.649361</td>\n",
       "      <td>0.638067</td>\n",
       "      <td>0.67087</td>\n",
       "      <td>0.691461</td>\n",
       "      <td>0.581975</td>\n",
       "      <td>0.712678</td>\n",
       "      <td>0.646011</td>\n",
       "      <td>0.687956</td>\n",
       "      <td>0.676363</td>\n",
       "      <td>0.651101</td>\n",
       "      <td>0.650226</td>\n",
       "      <td>0.598797</td>\n",
       "      <td>0.646375</td>\n",
       "      <td>0.688472</td>\n",
       "      <td>0.722216</td>\n",
       "      <td>0.701778</td>\n",
       "      <td>0.625013</td>\n",
       "      <td>0.58307</td>\n",
       "      <td>0.570164</td>\n",
       "      <td>0.667782</td>\n",
       "      <td>0.684272</td>\n",
       "      <td>0.663152</td>\n",
       "      <td>0.695074</td>\n",
       "      <td>0.603667</td>\n",
       "      <td>0.648337</td>\n",
       "      <td>0.659915</td>\n",
       "      <td>0.664154</td>\n",
       "      <td>0.689477</td>\n",
       "      <td>0.581169</td>\n",
       "      <td>0.612416</td>\n",
       "      <td>0.634349</td>\n",
       "      <td>0.670135</td>\n",
       "      <td>0.756971</td>\n",
       "      <td>0.64836</td>\n",
       "      <td>0.650773</td>\n",
       "      <td>0.734441</td>\n",
       "      <td>0.639042</td>\n",
       "      <td>0.621887</td>\n",
       "      <td>0.61889</td>\n",
       "      <td>0.608413</td>\n",
       "      <td>0.66023</td>\n",
       "      <td>0.688522</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>0.632731</td>\n",
       "      <td>0.671505</td>\n",
       "      <td>0.693959</td>\n",
       "      <td>0.615879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609053</td>\n",
       "      <td>0.655797</td>\n",
       "      <td>0.661822</td>\n",
       "      <td>0.650944</td>\n",
       "      <td>0.639491</td>\n",
       "      <td>0.742812</td>\n",
       "      <td>0.638124</td>\n",
       "      <td>0.603214</td>\n",
       "      <td>0.695171</td>\n",
       "      <td>0.644681</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.630923</td>\n",
       "      <td>0.61683</td>\n",
       "      <td>0.613294</td>\n",
       "      <td>0.708122</td>\n",
       "      <td>0.627109</td>\n",
       "      <td>0.677749</td>\n",
       "      <td>0.663295</td>\n",
       "      <td>0.722946</td>\n",
       "      <td>0.656109</td>\n",
       "      <td>0.70606</td>\n",
       "      <td>0.594043</td>\n",
       "      <td>0.714787</td>\n",
       "      <td>0.64917</td>\n",
       "      <td>0.69578</td>\n",
       "      <td>0.584778</td>\n",
       "      <td>0.671911</td>\n",
       "      <td>0.639369</td>\n",
       "      <td>0.631751</td>\n",
       "      <td>0.660609</td>\n",
       "      <td>0.803211</td>\n",
       "      <td>0.647094</td>\n",
       "      <td>0.655498</td>\n",
       "      <td>0.597235</td>\n",
       "      <td>0.595519</td>\n",
       "      <td>0.658665</td>\n",
       "      <td>0.63409</td>\n",
       "      <td>0.659733</td>\n",
       "      <td>0.628294</td>\n",
       "      <td>0.650178</td>\n",
       "      <td>0.627496</td>\n",
       "      <td>0.629953</td>\n",
       "      <td>0.580309</td>\n",
       "      <td>0.642258</td>\n",
       "      <td>0.718597</td>\n",
       "      <td>0.621616</td>\n",
       "      <td>0.728676</td>\n",
       "      <td>0.643696</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.685429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.628662</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.649361</td>\n",
       "      <td>0.638067</td>\n",
       "      <td>0.67087</td>\n",
       "      <td>0.691461</td>\n",
       "      <td>0.581975</td>\n",
       "      <td>0.712678</td>\n",
       "      <td>0.646011</td>\n",
       "      <td>0.687956</td>\n",
       "      <td>0.676363</td>\n",
       "      <td>0.651101</td>\n",
       "      <td>0.650226</td>\n",
       "      <td>0.598797</td>\n",
       "      <td>0.646375</td>\n",
       "      <td>0.688472</td>\n",
       "      <td>0.722216</td>\n",
       "      <td>0.701778</td>\n",
       "      <td>0.625013</td>\n",
       "      <td>0.58307</td>\n",
       "      <td>0.570164</td>\n",
       "      <td>0.667782</td>\n",
       "      <td>0.684272</td>\n",
       "      <td>0.663152</td>\n",
       "      <td>0.695074</td>\n",
       "      <td>0.603667</td>\n",
       "      <td>0.648337</td>\n",
       "      <td>0.659915</td>\n",
       "      <td>0.664154</td>\n",
       "      <td>0.689477</td>\n",
       "      <td>0.581169</td>\n",
       "      <td>0.612416</td>\n",
       "      <td>0.634349</td>\n",
       "      <td>0.670135</td>\n",
       "      <td>0.756971</td>\n",
       "      <td>0.64836</td>\n",
       "      <td>0.650773</td>\n",
       "      <td>0.734441</td>\n",
       "      <td>0.639042</td>\n",
       "      <td>0.621887</td>\n",
       "      <td>0.61889</td>\n",
       "      <td>0.608413</td>\n",
       "      <td>0.66023</td>\n",
       "      <td>0.688522</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>0.632731</td>\n",
       "      <td>0.671505</td>\n",
       "      <td>0.693959</td>\n",
       "      <td>0.615879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609053</td>\n",
       "      <td>0.655797</td>\n",
       "      <td>0.661822</td>\n",
       "      <td>0.650944</td>\n",
       "      <td>0.639491</td>\n",
       "      <td>0.742812</td>\n",
       "      <td>0.638124</td>\n",
       "      <td>0.603214</td>\n",
       "      <td>0.695171</td>\n",
       "      <td>0.644681</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.630923</td>\n",
       "      <td>0.61683</td>\n",
       "      <td>0.613294</td>\n",
       "      <td>0.708122</td>\n",
       "      <td>0.627109</td>\n",
       "      <td>0.677749</td>\n",
       "      <td>0.663295</td>\n",
       "      <td>0.722946</td>\n",
       "      <td>0.656109</td>\n",
       "      <td>0.70606</td>\n",
       "      <td>0.594043</td>\n",
       "      <td>0.714787</td>\n",
       "      <td>0.64917</td>\n",
       "      <td>0.69578</td>\n",
       "      <td>0.584778</td>\n",
       "      <td>0.671911</td>\n",
       "      <td>0.639369</td>\n",
       "      <td>0.631751</td>\n",
       "      <td>0.660609</td>\n",
       "      <td>0.803211</td>\n",
       "      <td>0.647094</td>\n",
       "      <td>0.655498</td>\n",
       "      <td>0.597235</td>\n",
       "      <td>0.595519</td>\n",
       "      <td>0.658665</td>\n",
       "      <td>0.63409</td>\n",
       "      <td>0.659733</td>\n",
       "      <td>0.628294</td>\n",
       "      <td>0.650178</td>\n",
       "      <td>0.627496</td>\n",
       "      <td>0.629953</td>\n",
       "      <td>0.580309</td>\n",
       "      <td>0.642258</td>\n",
       "      <td>0.718597</td>\n",
       "      <td>0.621616</td>\n",
       "      <td>0.728676</td>\n",
       "      <td>0.643696</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.685429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.628662</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.649361</td>\n",
       "      <td>0.638067</td>\n",
       "      <td>0.67087</td>\n",
       "      <td>0.691461</td>\n",
       "      <td>0.581975</td>\n",
       "      <td>0.712678</td>\n",
       "      <td>0.646011</td>\n",
       "      <td>0.687956</td>\n",
       "      <td>0.676363</td>\n",
       "      <td>0.651101</td>\n",
       "      <td>0.650226</td>\n",
       "      <td>0.598797</td>\n",
       "      <td>0.646375</td>\n",
       "      <td>0.688472</td>\n",
       "      <td>0.722216</td>\n",
       "      <td>0.701778</td>\n",
       "      <td>0.625013</td>\n",
       "      <td>0.58307</td>\n",
       "      <td>0.570164</td>\n",
       "      <td>0.667782</td>\n",
       "      <td>0.684272</td>\n",
       "      <td>0.663152</td>\n",
       "      <td>0.695074</td>\n",
       "      <td>0.603667</td>\n",
       "      <td>0.648337</td>\n",
       "      <td>0.659915</td>\n",
       "      <td>0.664154</td>\n",
       "      <td>0.689477</td>\n",
       "      <td>0.581169</td>\n",
       "      <td>0.612416</td>\n",
       "      <td>0.634349</td>\n",
       "      <td>0.670135</td>\n",
       "      <td>0.756971</td>\n",
       "      <td>0.64836</td>\n",
       "      <td>0.650773</td>\n",
       "      <td>0.734441</td>\n",
       "      <td>0.639042</td>\n",
       "      <td>0.621887</td>\n",
       "      <td>0.61889</td>\n",
       "      <td>0.608413</td>\n",
       "      <td>0.66023</td>\n",
       "      <td>0.688522</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>0.632731</td>\n",
       "      <td>0.671505</td>\n",
       "      <td>0.693959</td>\n",
       "      <td>0.615879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609053</td>\n",
       "      <td>0.655797</td>\n",
       "      <td>0.661822</td>\n",
       "      <td>0.650944</td>\n",
       "      <td>0.639491</td>\n",
       "      <td>0.742812</td>\n",
       "      <td>0.638124</td>\n",
       "      <td>0.603214</td>\n",
       "      <td>0.695171</td>\n",
       "      <td>0.644681</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.630923</td>\n",
       "      <td>0.61683</td>\n",
       "      <td>0.613294</td>\n",
       "      <td>0.708122</td>\n",
       "      <td>0.627109</td>\n",
       "      <td>0.677749</td>\n",
       "      <td>0.663295</td>\n",
       "      <td>0.722946</td>\n",
       "      <td>0.656109</td>\n",
       "      <td>0.70606</td>\n",
       "      <td>0.594043</td>\n",
       "      <td>0.714787</td>\n",
       "      <td>0.64917</td>\n",
       "      <td>0.69578</td>\n",
       "      <td>0.584778</td>\n",
       "      <td>0.671911</td>\n",
       "      <td>0.639369</td>\n",
       "      <td>0.631751</td>\n",
       "      <td>0.660609</td>\n",
       "      <td>0.803211</td>\n",
       "      <td>0.647094</td>\n",
       "      <td>0.655498</td>\n",
       "      <td>0.597235</td>\n",
       "      <td>0.595519</td>\n",
       "      <td>0.658665</td>\n",
       "      <td>0.63409</td>\n",
       "      <td>0.659733</td>\n",
       "      <td>0.628294</td>\n",
       "      <td>0.650178</td>\n",
       "      <td>0.627496</td>\n",
       "      <td>0.629953</td>\n",
       "      <td>0.580309</td>\n",
       "      <td>0.642258</td>\n",
       "      <td>0.718597</td>\n",
       "      <td>0.621616</td>\n",
       "      <td>0.728676</td>\n",
       "      <td>0.643696</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.685429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.628662</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.649361</td>\n",
       "      <td>0.638067</td>\n",
       "      <td>0.67087</td>\n",
       "      <td>0.691461</td>\n",
       "      <td>0.581975</td>\n",
       "      <td>0.712678</td>\n",
       "      <td>0.646011</td>\n",
       "      <td>0.687956</td>\n",
       "      <td>0.676363</td>\n",
       "      <td>0.651101</td>\n",
       "      <td>0.650226</td>\n",
       "      <td>0.598797</td>\n",
       "      <td>0.646375</td>\n",
       "      <td>0.688472</td>\n",
       "      <td>0.722216</td>\n",
       "      <td>0.701778</td>\n",
       "      <td>0.625013</td>\n",
       "      <td>0.58307</td>\n",
       "      <td>0.570164</td>\n",
       "      <td>0.667782</td>\n",
       "      <td>0.684272</td>\n",
       "      <td>0.663152</td>\n",
       "      <td>0.695074</td>\n",
       "      <td>0.603667</td>\n",
       "      <td>0.648337</td>\n",
       "      <td>0.659915</td>\n",
       "      <td>0.664154</td>\n",
       "      <td>0.689477</td>\n",
       "      <td>0.581169</td>\n",
       "      <td>0.612416</td>\n",
       "      <td>0.634349</td>\n",
       "      <td>0.670135</td>\n",
       "      <td>0.756971</td>\n",
       "      <td>0.64836</td>\n",
       "      <td>0.650773</td>\n",
       "      <td>0.734441</td>\n",
       "      <td>0.639042</td>\n",
       "      <td>0.621887</td>\n",
       "      <td>0.61889</td>\n",
       "      <td>0.608413</td>\n",
       "      <td>0.66023</td>\n",
       "      <td>0.688522</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>0.632731</td>\n",
       "      <td>0.671505</td>\n",
       "      <td>0.693959</td>\n",
       "      <td>0.615879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609053</td>\n",
       "      <td>0.655797</td>\n",
       "      <td>0.661822</td>\n",
       "      <td>0.650944</td>\n",
       "      <td>0.639491</td>\n",
       "      <td>0.742812</td>\n",
       "      <td>0.638124</td>\n",
       "      <td>0.603214</td>\n",
       "      <td>0.695171</td>\n",
       "      <td>0.644681</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.630923</td>\n",
       "      <td>0.61683</td>\n",
       "      <td>0.613294</td>\n",
       "      <td>0.708122</td>\n",
       "      <td>0.627109</td>\n",
       "      <td>0.677749</td>\n",
       "      <td>0.663295</td>\n",
       "      <td>0.722946</td>\n",
       "      <td>0.656109</td>\n",
       "      <td>0.70606</td>\n",
       "      <td>0.594043</td>\n",
       "      <td>0.714787</td>\n",
       "      <td>0.64917</td>\n",
       "      <td>0.69578</td>\n",
       "      <td>0.584778</td>\n",
       "      <td>0.671911</td>\n",
       "      <td>0.639369</td>\n",
       "      <td>0.631751</td>\n",
       "      <td>0.660609</td>\n",
       "      <td>0.803211</td>\n",
       "      <td>0.647094</td>\n",
       "      <td>0.655498</td>\n",
       "      <td>0.597235</td>\n",
       "      <td>0.595519</td>\n",
       "      <td>0.658665</td>\n",
       "      <td>0.63409</td>\n",
       "      <td>0.659733</td>\n",
       "      <td>0.628294</td>\n",
       "      <td>0.650178</td>\n",
       "      <td>0.627496</td>\n",
       "      <td>0.629953</td>\n",
       "      <td>0.580309</td>\n",
       "      <td>0.642258</td>\n",
       "      <td>0.718597</td>\n",
       "      <td>0.621616</td>\n",
       "      <td>0.728676</td>\n",
       "      <td>0.643696</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.685429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.628662</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.649361</td>\n",
       "      <td>0.638067</td>\n",
       "      <td>0.67087</td>\n",
       "      <td>0.691461</td>\n",
       "      <td>0.581975</td>\n",
       "      <td>0.712678</td>\n",
       "      <td>0.646011</td>\n",
       "      <td>0.687956</td>\n",
       "      <td>0.676363</td>\n",
       "      <td>0.651101</td>\n",
       "      <td>0.650226</td>\n",
       "      <td>0.598797</td>\n",
       "      <td>0.646375</td>\n",
       "      <td>0.688472</td>\n",
       "      <td>0.722216</td>\n",
       "      <td>0.701778</td>\n",
       "      <td>0.625013</td>\n",
       "      <td>0.58307</td>\n",
       "      <td>0.570164</td>\n",
       "      <td>0.667782</td>\n",
       "      <td>0.684272</td>\n",
       "      <td>0.663152</td>\n",
       "      <td>0.695074</td>\n",
       "      <td>0.603667</td>\n",
       "      <td>0.648337</td>\n",
       "      <td>0.659915</td>\n",
       "      <td>0.664154</td>\n",
       "      <td>0.689477</td>\n",
       "      <td>0.581169</td>\n",
       "      <td>0.612416</td>\n",
       "      <td>0.634349</td>\n",
       "      <td>0.670135</td>\n",
       "      <td>0.756971</td>\n",
       "      <td>0.64836</td>\n",
       "      <td>0.650773</td>\n",
       "      <td>0.734441</td>\n",
       "      <td>0.639042</td>\n",
       "      <td>0.621887</td>\n",
       "      <td>0.61889</td>\n",
       "      <td>0.608413</td>\n",
       "      <td>0.66023</td>\n",
       "      <td>0.688522</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>0.632731</td>\n",
       "      <td>0.671505</td>\n",
       "      <td>0.693959</td>\n",
       "      <td>0.615879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609053</td>\n",
       "      <td>0.655797</td>\n",
       "      <td>0.661822</td>\n",
       "      <td>0.650944</td>\n",
       "      <td>0.639491</td>\n",
       "      <td>0.742812</td>\n",
       "      <td>0.638124</td>\n",
       "      <td>0.603214</td>\n",
       "      <td>0.695171</td>\n",
       "      <td>0.644681</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.630923</td>\n",
       "      <td>0.61683</td>\n",
       "      <td>0.613294</td>\n",
       "      <td>0.708122</td>\n",
       "      <td>0.627109</td>\n",
       "      <td>0.677749</td>\n",
       "      <td>0.663295</td>\n",
       "      <td>0.722946</td>\n",
       "      <td>0.656109</td>\n",
       "      <td>0.70606</td>\n",
       "      <td>0.594043</td>\n",
       "      <td>0.714787</td>\n",
       "      <td>0.64917</td>\n",
       "      <td>0.69578</td>\n",
       "      <td>0.584778</td>\n",
       "      <td>0.671911</td>\n",
       "      <td>0.639369</td>\n",
       "      <td>0.631751</td>\n",
       "      <td>0.660609</td>\n",
       "      <td>0.803211</td>\n",
       "      <td>0.647094</td>\n",
       "      <td>0.655498</td>\n",
       "      <td>0.597235</td>\n",
       "      <td>0.595519</td>\n",
       "      <td>0.658665</td>\n",
       "      <td>0.63409</td>\n",
       "      <td>0.659733</td>\n",
       "      <td>0.628294</td>\n",
       "      <td>0.650178</td>\n",
       "      <td>0.627496</td>\n",
       "      <td>0.629953</td>\n",
       "      <td>0.580309</td>\n",
       "      <td>0.642258</td>\n",
       "      <td>0.718597</td>\n",
       "      <td>0.621616</td>\n",
       "      <td>0.728676</td>\n",
       "      <td>0.643696</td>\n",
       "      <td>0.660123</td>\n",
       "      <td>0.685429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id    pred_1    pred_2    pred_3    pred_4   pred_5    pred_6  \\\n",
       "0  00eed32682bb  0.628662  0.644628  0.649361  0.638067  0.67087  0.691461   \n",
       "1  010ebe33f668  0.628662  0.644628  0.649361  0.638067  0.67087  0.691461   \n",
       "2  02fa521e1838  0.628662  0.644628  0.649361  0.638067  0.67087  0.691461   \n",
       "3  040e15f562a2  0.628662  0.644628  0.649361  0.638067  0.67087  0.691461   \n",
       "4  046e85c7cc7f  0.628662  0.644628  0.649361  0.638067  0.67087  0.691461   \n",
       "\n",
       "     pred_7    pred_8    pred_9   pred_10   pred_11   pred_12   pred_13  \\\n",
       "0  0.581975  0.712678  0.646011  0.687956  0.676363  0.651101  0.650226   \n",
       "1  0.581975  0.712678  0.646011  0.687956  0.676363  0.651101  0.650226   \n",
       "2  0.581975  0.712678  0.646011  0.687956  0.676363  0.651101  0.650226   \n",
       "3  0.581975  0.712678  0.646011  0.687956  0.676363  0.651101  0.650226   \n",
       "4  0.581975  0.712678  0.646011  0.687956  0.676363  0.651101  0.650226   \n",
       "\n",
       "    pred_14   pred_15   pred_16   pred_17   pred_18   pred_19  pred_20  \\\n",
       "0  0.598797  0.646375  0.688472  0.722216  0.701778  0.625013  0.58307   \n",
       "1  0.598797  0.646375  0.688472  0.722216  0.701778  0.625013  0.58307   \n",
       "2  0.598797  0.646375  0.688472  0.722216  0.701778  0.625013  0.58307   \n",
       "3  0.598797  0.646375  0.688472  0.722216  0.701778  0.625013  0.58307   \n",
       "4  0.598797  0.646375  0.688472  0.722216  0.701778  0.625013  0.58307   \n",
       "\n",
       "    pred_21   pred_22   pred_23   pred_24   pred_25   pred_26   pred_27  \\\n",
       "0  0.570164  0.667782  0.684272  0.663152  0.695074  0.603667  0.648337   \n",
       "1  0.570164  0.667782  0.684272  0.663152  0.695074  0.603667  0.648337   \n",
       "2  0.570164  0.667782  0.684272  0.663152  0.695074  0.603667  0.648337   \n",
       "3  0.570164  0.667782  0.684272  0.663152  0.695074  0.603667  0.648337   \n",
       "4  0.570164  0.667782  0.684272  0.663152  0.695074  0.603667  0.648337   \n",
       "\n",
       "    pred_28   pred_29   pred_30   pred_31   pred_32   pred_33   pred_34  \\\n",
       "0  0.659915  0.664154  0.689477  0.581169  0.612416  0.634349  0.670135   \n",
       "1  0.659915  0.664154  0.689477  0.581169  0.612416  0.634349  0.670135   \n",
       "2  0.659915  0.664154  0.689477  0.581169  0.612416  0.634349  0.670135   \n",
       "3  0.659915  0.664154  0.689477  0.581169  0.612416  0.634349  0.670135   \n",
       "4  0.659915  0.664154  0.689477  0.581169  0.612416  0.634349  0.670135   \n",
       "\n",
       "    pred_35  pred_36   pred_37   pred_38   pred_39   pred_40  pred_41  \\\n",
       "0  0.756971  0.64836  0.650773  0.734441  0.639042  0.621887  0.61889   \n",
       "1  0.756971  0.64836  0.650773  0.734441  0.639042  0.621887  0.61889   \n",
       "2  0.756971  0.64836  0.650773  0.734441  0.639042  0.621887  0.61889   \n",
       "3  0.756971  0.64836  0.650773  0.734441  0.639042  0.621887  0.61889   \n",
       "4  0.756971  0.64836  0.650773  0.734441  0.639042  0.621887  0.61889   \n",
       "\n",
       "    pred_42  pred_43   pred_44   pred_45   pred_46   pred_47   pred_48  \\\n",
       "0  0.608413  0.66023  0.688522  0.617128  0.632731  0.671505  0.693959   \n",
       "1  0.608413  0.66023  0.688522  0.617128  0.632731  0.671505  0.693959   \n",
       "2  0.608413  0.66023  0.688522  0.617128  0.632731  0.671505  0.693959   \n",
       "3  0.608413  0.66023  0.688522  0.617128  0.632731  0.671505  0.693959   \n",
       "4  0.608413  0.66023  0.688522  0.617128  0.632731  0.671505  0.693959   \n",
       "\n",
       "    pred_49  ...   pred_51   pred_52   pred_53   pred_54   pred_55   pred_56  \\\n",
       "0  0.615879  ...  0.609053  0.655797  0.661822  0.650944  0.639491  0.742812   \n",
       "1  0.615879  ...  0.609053  0.655797  0.661822  0.650944  0.639491  0.742812   \n",
       "2  0.615879  ...  0.609053  0.655797  0.661822  0.650944  0.639491  0.742812   \n",
       "3  0.615879  ...  0.609053  0.655797  0.661822  0.650944  0.639491  0.742812   \n",
       "4  0.615879  ...  0.609053  0.655797  0.661822  0.650944  0.639491  0.742812   \n",
       "\n",
       "    pred_57   pred_58   pred_59   pred_60   pred_61   pred_62  pred_63  \\\n",
       "0  0.638124  0.603214  0.695171  0.644681  0.627615  0.630923  0.61683   \n",
       "1  0.638124  0.603214  0.695171  0.644681  0.627615  0.630923  0.61683   \n",
       "2  0.638124  0.603214  0.695171  0.644681  0.627615  0.630923  0.61683   \n",
       "3  0.638124  0.603214  0.695171  0.644681  0.627615  0.630923  0.61683   \n",
       "4  0.638124  0.603214  0.695171  0.644681  0.627615  0.630923  0.61683   \n",
       "\n",
       "    pred_64   pred_65   pred_66   pred_67   pred_68   pred_69   pred_70  \\\n",
       "0  0.613294  0.708122  0.627109  0.677749  0.663295  0.722946  0.656109   \n",
       "1  0.613294  0.708122  0.627109  0.677749  0.663295  0.722946  0.656109   \n",
       "2  0.613294  0.708122  0.627109  0.677749  0.663295  0.722946  0.656109   \n",
       "3  0.613294  0.708122  0.627109  0.677749  0.663295  0.722946  0.656109   \n",
       "4  0.613294  0.708122  0.627109  0.677749  0.663295  0.722946  0.656109   \n",
       "\n",
       "   pred_71   pred_72   pred_73  pred_74  pred_75   pred_76   pred_77  \\\n",
       "0  0.70606  0.594043  0.714787  0.64917  0.69578  0.584778  0.671911   \n",
       "1  0.70606  0.594043  0.714787  0.64917  0.69578  0.584778  0.671911   \n",
       "2  0.70606  0.594043  0.714787  0.64917  0.69578  0.584778  0.671911   \n",
       "3  0.70606  0.594043  0.714787  0.64917  0.69578  0.584778  0.671911   \n",
       "4  0.70606  0.594043  0.714787  0.64917  0.69578  0.584778  0.671911   \n",
       "\n",
       "    pred_78   pred_79   pred_80   pred_81   pred_82   pred_83   pred_84  \\\n",
       "0  0.639369  0.631751  0.660609  0.803211  0.647094  0.655498  0.597235   \n",
       "1  0.639369  0.631751  0.660609  0.803211  0.647094  0.655498  0.597235   \n",
       "2  0.639369  0.631751  0.660609  0.803211  0.647094  0.655498  0.597235   \n",
       "3  0.639369  0.631751  0.660609  0.803211  0.647094  0.655498  0.597235   \n",
       "4  0.639369  0.631751  0.660609  0.803211  0.647094  0.655498  0.597235   \n",
       "\n",
       "    pred_85   pred_86  pred_87   pred_88   pred_89   pred_90   pred_91  \\\n",
       "0  0.595519  0.658665  0.63409  0.659733  0.628294  0.650178  0.627496   \n",
       "1  0.595519  0.658665  0.63409  0.659733  0.628294  0.650178  0.627496   \n",
       "2  0.595519  0.658665  0.63409  0.659733  0.628294  0.650178  0.627496   \n",
       "3  0.595519  0.658665  0.63409  0.659733  0.628294  0.650178  0.627496   \n",
       "4  0.595519  0.658665  0.63409  0.659733  0.628294  0.650178  0.627496   \n",
       "\n",
       "    pred_92   pred_93   pred_94   pred_95   pred_96   pred_97   pred_98  \\\n",
       "0  0.629953  0.580309  0.642258  0.718597  0.621616  0.728676  0.643696   \n",
       "1  0.629953  0.580309  0.642258  0.718597  0.621616  0.728676  0.643696   \n",
       "2  0.629953  0.580309  0.642258  0.718597  0.621616  0.728676  0.643696   \n",
       "3  0.629953  0.580309  0.642258  0.718597  0.621616  0.728676  0.643696   \n",
       "4  0.629953  0.580309  0.642258  0.718597  0.621616  0.728676  0.643696   \n",
       "\n",
       "    pred_99  pred_100  \n",
       "0  0.660123  0.685429  \n",
       "1  0.660123  0.685429  \n",
       "2  0.660123  0.685429  \n",
       "3  0.660123  0.685429  \n",
       "4  0.660123  0.685429  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43d2352e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T15:17:06.978485Z",
     "iopub.status.busy": "2023-08-10T15:17:06.977589Z",
     "iopub.status.idle": "2023-08-10T15:17:07.000759Z",
     "shell.execute_reply": "2023-08-10T15:17:06.999753Z"
    },
    "papermill": {
     "duration": 0.075194,
     "end_time": "2023-08-10T15:17:07.003385",
     "exception": false,
     "start_time": "2023-08-10T15:17:06.928191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.345518</td>\n",
       "      <td>0.654482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.345518</td>\n",
       "      <td>0.654482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.345518</td>\n",
       "      <td>0.654482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.345518</td>\n",
       "      <td>0.654482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.345518</td>\n",
       "      <td>0.654482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id   class_0   class_1\n",
       "0  00eed32682bb  0.345518  0.654482\n",
       "1  010ebe33f668  0.345518  0.654482\n",
       "2  02fa521e1838  0.345518  0.654482\n",
       "3  040e15f562a2  0.345518  0.654482\n",
       "4  046e85c7cc7f  0.345518  0.654482"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "submission[\"class_1\"] = df_preds.drop(\"Id\", axis=1).mean(axis=1).tolist()\n",
    "submission[\"class_0\"] = (1 - submission[\"class_1\"]).tolist()\n",
    "display(submission.head())\n",
    "submission.to_csv(f\"submission.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2125e9",
   "metadata": {
    "papermill": {
     "duration": 0.045492,
     "end_time": "2023-08-10T15:17:07.097584",
     "exception": false,
     "start_time": "2023-08-10T15:17:07.052092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 442.834856,
   "end_time": "2023-08-10T15:17:09.069588",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-10T15:09:46.234732",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
