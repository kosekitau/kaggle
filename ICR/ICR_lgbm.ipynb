{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "coHPLE0D8gql"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 100)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "from pytz import timezone\n",
        "from datetime import datetime, timedelta, date\n",
        "import json\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import fbeta_score, make_scorer\n",
        "\n",
        "# フォルダ移動\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/projects/ICR - Identifying Age-Related Conditions/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"data/train.csv\")\n",
        "df_test = pd.read_csv(\"data/test.csv\")\n",
        "df_sub = pd.read_csv(\"data/sample_submission.csv\")\n",
        "df_greeks = pd.read_csv(\"data/greeks.csv\")\n",
        "\n",
        "# FE\n",
        "df_train[\"EJ\"] = df_train[\"EJ\"].replace({'A':0, 'B':1})\n",
        "df_test[\"EJ\"] = df_test[\"EJ\"].replace({'A':0, 'B':1})\n",
        "\n",
        "target = [\"target\"]\n",
        "ex_columns = ['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n",
        "       'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n",
        "       'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
        "       'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n",
        "       'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL'] # All\n",
        "use_columns = ex_columns + target\n",
        "\n",
        "display(df_train.head())\n",
        "print(ex_columns)\n",
        "print(len(ex_columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "I6ruQVgk9J1P",
        "outputId": "0e3f3fbe-0b96-462b-d463-20ae3f3ed9c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             Id        AB          AF          AH         AM        AR  \\\n",
              "0  000ff2bfdfe9  0.209377  3109.03329   85.200147  22.394407  8.138688   \n",
              "1  007255e47698  0.145282   978.76416   85.200147  36.968889  8.138688   \n",
              "2  013f2bd269f5  0.470030  2635.10654   85.200147  32.360553  8.138688   \n",
              "3  043ac50845d5  0.252107  3819.65177  120.201618  77.112203  8.138688   \n",
              "4  044fb8a146ec  0.380297  3733.04844   85.200147  14.103738  8.138688   \n",
              "\n",
              "         AX        AY         AZ          BC         BD        BN          BP  \\\n",
              "0  0.699861  0.025578   9.812214    5.555634  4126.58731  22.5984  175.638726   \n",
              "1  3.632190  0.025578  13.517790    1.229900  5496.92824  19.4205  155.868030   \n",
              "2  6.732840  0.025578  12.824570    1.229900  5135.78024  26.4825  128.988531   \n",
              "3  3.685344  0.025578  11.053708    1.229900  4169.67738  23.6577  237.282264   \n",
              "4  3.942255  0.054810   3.396778  102.151980  5728.73412  24.0108  324.546318   \n",
              "\n",
              "           BQ           BR          BZ         CB        CC        CD   \\\n",
              "0  152.707705   823.928241  257.432377  47.223358  0.563481  23.387600   \n",
              "1   14.754720    51.216883  257.432377  30.284345  0.484710  50.628208   \n",
              "2  219.320160   482.141594  257.432377  32.563713  0.495852  85.955376   \n",
              "3   11.050410   661.518640  257.432377  15.201914  0.717882  88.159360   \n",
              "4  149.717165  6074.859475  257.432377  82.213495  0.536467  72.644264   \n",
              "\n",
              "          CF        CH        CL        CR         CS        CU        CW   \\\n",
              "0   4.851915  0.023482  1.050225  0.069225  13.784111  1.302012  36.205956   \n",
              "1   6.085041  0.031442  1.113875  1.117800  28.310953  1.357182  37.476568   \n",
              "2   5.376488  0.036218  1.050225  0.700350  39.364743  1.009611  21.459644   \n",
              "3   2.347652  0.029054  1.400300  0.636075  41.116960  0.722727  21.530392   \n",
              "4  30.537722  0.025472  1.050225  0.693150  31.724726  0.827550  34.415360   \n",
              "\n",
              "         DA          DE       DF        DH          DI        DL         DN  \\\n",
              "0  69.08340  295.570575  0.23868  0.284232   89.245560  84.31664  29.657104   \n",
              "1  70.79836  178.553100  0.23868  0.363489  110.581815  75.74548  37.532000   \n",
              "2  70.81970  321.426625  0.23868  0.210441  120.056438  65.46984  28.053464   \n",
              "3  47.27586  196.607985  0.23868  0.292431  139.824570  71.57120  24.354856   \n",
              "4  74.06532  200.178160  0.23868  0.207708   97.920120  52.83888  26.019912   \n",
              "\n",
              "         DU       DV         DY        EB        EE            EG        EH  \\\n",
              "0  5.310690  1.74307  23.187704  7.294176  1.987283   1433.166750  0.949104   \n",
              "1  0.005518  1.74307  17.222328  4.926396  0.858603   1111.287150  0.003042   \n",
              "2  1.289739  1.74307  36.861352  7.813674  8.146651   1494.076488  0.377208   \n",
              "3  2.655345  1.74307  52.003884  7.386060  3.813326  15691.552180  0.614484   \n",
              "4  1.144902  1.74307   9.064856  7.350720  3.490846   1403.656300  0.164268   \n",
              "\n",
              "   EJ          EL         EP         EU          FC        FD             FE  \\\n",
              "0   1   30.879420  78.526968   3.828384   13.394640  10.265073   9028.291921   \n",
              "1   0  109.125159  95.415086  52.260480   17.175984   0.296850   6785.003474   \n",
              "2   1  109.125159  78.526968   5.390628  224.207424   8.745201   8338.906181   \n",
              "3   1   31.674357  78.526968  31.323372   59.301984   7.884336  10965.766040   \n",
              "4   1  109.125159  91.994825  51.141336   29.102640   4.274640  16198.049590   \n",
              "\n",
              "          FI        FL        FR        FS         GB          GE  \\\n",
              "0   3.583450  7.298162   1.73855  0.094822  11.339138   72.611063   \n",
              "1  10.358927  0.173229   0.49706  0.568932   9.292698   72.611063   \n",
              "2  11.626917  7.709560   0.97556  1.198821  37.077772   88.609437   \n",
              "3  14.852022  6.122162   0.49706  0.284466  18.529584   82.416803   \n",
              "4  13.666727  8.153058  48.50134  0.121914  16.408728  146.109943   \n",
              "\n",
              "             GF         GH         GI         GL  Class  \n",
              "0   2003.810319  22.136229  69.834944   0.120343      1  \n",
              "1  27981.562750  29.135430  32.131996  21.978000      0  \n",
              "2  13676.957810  28.022851  35.192676   0.196941      0  \n",
              "3   2094.262452  39.948656  90.493248   0.155829      0  \n",
              "4   8524.370502  45.381316  36.262628   0.096614      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-58d5785b-cd98-408b-8a22-62033ca1ba85\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>AB</th>\n",
              "      <th>AF</th>\n",
              "      <th>AH</th>\n",
              "      <th>AM</th>\n",
              "      <th>AR</th>\n",
              "      <th>AX</th>\n",
              "      <th>AY</th>\n",
              "      <th>AZ</th>\n",
              "      <th>BC</th>\n",
              "      <th>BD</th>\n",
              "      <th>BN</th>\n",
              "      <th>BP</th>\n",
              "      <th>BQ</th>\n",
              "      <th>BR</th>\n",
              "      <th>BZ</th>\n",
              "      <th>CB</th>\n",
              "      <th>CC</th>\n",
              "      <th>CD</th>\n",
              "      <th>CF</th>\n",
              "      <th>CH</th>\n",
              "      <th>CL</th>\n",
              "      <th>CR</th>\n",
              "      <th>CS</th>\n",
              "      <th>CU</th>\n",
              "      <th>CW</th>\n",
              "      <th>DA</th>\n",
              "      <th>DE</th>\n",
              "      <th>DF</th>\n",
              "      <th>DH</th>\n",
              "      <th>DI</th>\n",
              "      <th>DL</th>\n",
              "      <th>DN</th>\n",
              "      <th>DU</th>\n",
              "      <th>DV</th>\n",
              "      <th>DY</th>\n",
              "      <th>EB</th>\n",
              "      <th>EE</th>\n",
              "      <th>EG</th>\n",
              "      <th>EH</th>\n",
              "      <th>EJ</th>\n",
              "      <th>EL</th>\n",
              "      <th>EP</th>\n",
              "      <th>EU</th>\n",
              "      <th>FC</th>\n",
              "      <th>FD</th>\n",
              "      <th>FE</th>\n",
              "      <th>FI</th>\n",
              "      <th>FL</th>\n",
              "      <th>FR</th>\n",
              "      <th>FS</th>\n",
              "      <th>GB</th>\n",
              "      <th>GE</th>\n",
              "      <th>GF</th>\n",
              "      <th>GH</th>\n",
              "      <th>GI</th>\n",
              "      <th>GL</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000ff2bfdfe9</td>\n",
              "      <td>0.209377</td>\n",
              "      <td>3109.03329</td>\n",
              "      <td>85.200147</td>\n",
              "      <td>22.394407</td>\n",
              "      <td>8.138688</td>\n",
              "      <td>0.699861</td>\n",
              "      <td>0.025578</td>\n",
              "      <td>9.812214</td>\n",
              "      <td>5.555634</td>\n",
              "      <td>4126.58731</td>\n",
              "      <td>22.5984</td>\n",
              "      <td>175.638726</td>\n",
              "      <td>152.707705</td>\n",
              "      <td>823.928241</td>\n",
              "      <td>257.432377</td>\n",
              "      <td>47.223358</td>\n",
              "      <td>0.563481</td>\n",
              "      <td>23.387600</td>\n",
              "      <td>4.851915</td>\n",
              "      <td>0.023482</td>\n",
              "      <td>1.050225</td>\n",
              "      <td>0.069225</td>\n",
              "      <td>13.784111</td>\n",
              "      <td>1.302012</td>\n",
              "      <td>36.205956</td>\n",
              "      <td>69.08340</td>\n",
              "      <td>295.570575</td>\n",
              "      <td>0.23868</td>\n",
              "      <td>0.284232</td>\n",
              "      <td>89.245560</td>\n",
              "      <td>84.31664</td>\n",
              "      <td>29.657104</td>\n",
              "      <td>5.310690</td>\n",
              "      <td>1.74307</td>\n",
              "      <td>23.187704</td>\n",
              "      <td>7.294176</td>\n",
              "      <td>1.987283</td>\n",
              "      <td>1433.166750</td>\n",
              "      <td>0.949104</td>\n",
              "      <td>1</td>\n",
              "      <td>30.879420</td>\n",
              "      <td>78.526968</td>\n",
              "      <td>3.828384</td>\n",
              "      <td>13.394640</td>\n",
              "      <td>10.265073</td>\n",
              "      <td>9028.291921</td>\n",
              "      <td>3.583450</td>\n",
              "      <td>7.298162</td>\n",
              "      <td>1.73855</td>\n",
              "      <td>0.094822</td>\n",
              "      <td>11.339138</td>\n",
              "      <td>72.611063</td>\n",
              "      <td>2003.810319</td>\n",
              "      <td>22.136229</td>\n",
              "      <td>69.834944</td>\n",
              "      <td>0.120343</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>007255e47698</td>\n",
              "      <td>0.145282</td>\n",
              "      <td>978.76416</td>\n",
              "      <td>85.200147</td>\n",
              "      <td>36.968889</td>\n",
              "      <td>8.138688</td>\n",
              "      <td>3.632190</td>\n",
              "      <td>0.025578</td>\n",
              "      <td>13.517790</td>\n",
              "      <td>1.229900</td>\n",
              "      <td>5496.92824</td>\n",
              "      <td>19.4205</td>\n",
              "      <td>155.868030</td>\n",
              "      <td>14.754720</td>\n",
              "      <td>51.216883</td>\n",
              "      <td>257.432377</td>\n",
              "      <td>30.284345</td>\n",
              "      <td>0.484710</td>\n",
              "      <td>50.628208</td>\n",
              "      <td>6.085041</td>\n",
              "      <td>0.031442</td>\n",
              "      <td>1.113875</td>\n",
              "      <td>1.117800</td>\n",
              "      <td>28.310953</td>\n",
              "      <td>1.357182</td>\n",
              "      <td>37.476568</td>\n",
              "      <td>70.79836</td>\n",
              "      <td>178.553100</td>\n",
              "      <td>0.23868</td>\n",
              "      <td>0.363489</td>\n",
              "      <td>110.581815</td>\n",
              "      <td>75.74548</td>\n",
              "      <td>37.532000</td>\n",
              "      <td>0.005518</td>\n",
              "      <td>1.74307</td>\n",
              "      <td>17.222328</td>\n",
              "      <td>4.926396</td>\n",
              "      <td>0.858603</td>\n",
              "      <td>1111.287150</td>\n",
              "      <td>0.003042</td>\n",
              "      <td>0</td>\n",
              "      <td>109.125159</td>\n",
              "      <td>95.415086</td>\n",
              "      <td>52.260480</td>\n",
              "      <td>17.175984</td>\n",
              "      <td>0.296850</td>\n",
              "      <td>6785.003474</td>\n",
              "      <td>10.358927</td>\n",
              "      <td>0.173229</td>\n",
              "      <td>0.49706</td>\n",
              "      <td>0.568932</td>\n",
              "      <td>9.292698</td>\n",
              "      <td>72.611063</td>\n",
              "      <td>27981.562750</td>\n",
              "      <td>29.135430</td>\n",
              "      <td>32.131996</td>\n",
              "      <td>21.978000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>013f2bd269f5</td>\n",
              "      <td>0.470030</td>\n",
              "      <td>2635.10654</td>\n",
              "      <td>85.200147</td>\n",
              "      <td>32.360553</td>\n",
              "      <td>8.138688</td>\n",
              "      <td>6.732840</td>\n",
              "      <td>0.025578</td>\n",
              "      <td>12.824570</td>\n",
              "      <td>1.229900</td>\n",
              "      <td>5135.78024</td>\n",
              "      <td>26.4825</td>\n",
              "      <td>128.988531</td>\n",
              "      <td>219.320160</td>\n",
              "      <td>482.141594</td>\n",
              "      <td>257.432377</td>\n",
              "      <td>32.563713</td>\n",
              "      <td>0.495852</td>\n",
              "      <td>85.955376</td>\n",
              "      <td>5.376488</td>\n",
              "      <td>0.036218</td>\n",
              "      <td>1.050225</td>\n",
              "      <td>0.700350</td>\n",
              "      <td>39.364743</td>\n",
              "      <td>1.009611</td>\n",
              "      <td>21.459644</td>\n",
              "      <td>70.81970</td>\n",
              "      <td>321.426625</td>\n",
              "      <td>0.23868</td>\n",
              "      <td>0.210441</td>\n",
              "      <td>120.056438</td>\n",
              "      <td>65.46984</td>\n",
              "      <td>28.053464</td>\n",
              "      <td>1.289739</td>\n",
              "      <td>1.74307</td>\n",
              "      <td>36.861352</td>\n",
              "      <td>7.813674</td>\n",
              "      <td>8.146651</td>\n",
              "      <td>1494.076488</td>\n",
              "      <td>0.377208</td>\n",
              "      <td>1</td>\n",
              "      <td>109.125159</td>\n",
              "      <td>78.526968</td>\n",
              "      <td>5.390628</td>\n",
              "      <td>224.207424</td>\n",
              "      <td>8.745201</td>\n",
              "      <td>8338.906181</td>\n",
              "      <td>11.626917</td>\n",
              "      <td>7.709560</td>\n",
              "      <td>0.97556</td>\n",
              "      <td>1.198821</td>\n",
              "      <td>37.077772</td>\n",
              "      <td>88.609437</td>\n",
              "      <td>13676.957810</td>\n",
              "      <td>28.022851</td>\n",
              "      <td>35.192676</td>\n",
              "      <td>0.196941</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>043ac50845d5</td>\n",
              "      <td>0.252107</td>\n",
              "      <td>3819.65177</td>\n",
              "      <td>120.201618</td>\n",
              "      <td>77.112203</td>\n",
              "      <td>8.138688</td>\n",
              "      <td>3.685344</td>\n",
              "      <td>0.025578</td>\n",
              "      <td>11.053708</td>\n",
              "      <td>1.229900</td>\n",
              "      <td>4169.67738</td>\n",
              "      <td>23.6577</td>\n",
              "      <td>237.282264</td>\n",
              "      <td>11.050410</td>\n",
              "      <td>661.518640</td>\n",
              "      <td>257.432377</td>\n",
              "      <td>15.201914</td>\n",
              "      <td>0.717882</td>\n",
              "      <td>88.159360</td>\n",
              "      <td>2.347652</td>\n",
              "      <td>0.029054</td>\n",
              "      <td>1.400300</td>\n",
              "      <td>0.636075</td>\n",
              "      <td>41.116960</td>\n",
              "      <td>0.722727</td>\n",
              "      <td>21.530392</td>\n",
              "      <td>47.27586</td>\n",
              "      <td>196.607985</td>\n",
              "      <td>0.23868</td>\n",
              "      <td>0.292431</td>\n",
              "      <td>139.824570</td>\n",
              "      <td>71.57120</td>\n",
              "      <td>24.354856</td>\n",
              "      <td>2.655345</td>\n",
              "      <td>1.74307</td>\n",
              "      <td>52.003884</td>\n",
              "      <td>7.386060</td>\n",
              "      <td>3.813326</td>\n",
              "      <td>15691.552180</td>\n",
              "      <td>0.614484</td>\n",
              "      <td>1</td>\n",
              "      <td>31.674357</td>\n",
              "      <td>78.526968</td>\n",
              "      <td>31.323372</td>\n",
              "      <td>59.301984</td>\n",
              "      <td>7.884336</td>\n",
              "      <td>10965.766040</td>\n",
              "      <td>14.852022</td>\n",
              "      <td>6.122162</td>\n",
              "      <td>0.49706</td>\n",
              "      <td>0.284466</td>\n",
              "      <td>18.529584</td>\n",
              "      <td>82.416803</td>\n",
              "      <td>2094.262452</td>\n",
              "      <td>39.948656</td>\n",
              "      <td>90.493248</td>\n",
              "      <td>0.155829</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>044fb8a146ec</td>\n",
              "      <td>0.380297</td>\n",
              "      <td>3733.04844</td>\n",
              "      <td>85.200147</td>\n",
              "      <td>14.103738</td>\n",
              "      <td>8.138688</td>\n",
              "      <td>3.942255</td>\n",
              "      <td>0.054810</td>\n",
              "      <td>3.396778</td>\n",
              "      <td>102.151980</td>\n",
              "      <td>5728.73412</td>\n",
              "      <td>24.0108</td>\n",
              "      <td>324.546318</td>\n",
              "      <td>149.717165</td>\n",
              "      <td>6074.859475</td>\n",
              "      <td>257.432377</td>\n",
              "      <td>82.213495</td>\n",
              "      <td>0.536467</td>\n",
              "      <td>72.644264</td>\n",
              "      <td>30.537722</td>\n",
              "      <td>0.025472</td>\n",
              "      <td>1.050225</td>\n",
              "      <td>0.693150</td>\n",
              "      <td>31.724726</td>\n",
              "      <td>0.827550</td>\n",
              "      <td>34.415360</td>\n",
              "      <td>74.06532</td>\n",
              "      <td>200.178160</td>\n",
              "      <td>0.23868</td>\n",
              "      <td>0.207708</td>\n",
              "      <td>97.920120</td>\n",
              "      <td>52.83888</td>\n",
              "      <td>26.019912</td>\n",
              "      <td>1.144902</td>\n",
              "      <td>1.74307</td>\n",
              "      <td>9.064856</td>\n",
              "      <td>7.350720</td>\n",
              "      <td>3.490846</td>\n",
              "      <td>1403.656300</td>\n",
              "      <td>0.164268</td>\n",
              "      <td>1</td>\n",
              "      <td>109.125159</td>\n",
              "      <td>91.994825</td>\n",
              "      <td>51.141336</td>\n",
              "      <td>29.102640</td>\n",
              "      <td>4.274640</td>\n",
              "      <td>16198.049590</td>\n",
              "      <td>13.666727</td>\n",
              "      <td>8.153058</td>\n",
              "      <td>48.50134</td>\n",
              "      <td>0.121914</td>\n",
              "      <td>16.408728</td>\n",
              "      <td>146.109943</td>\n",
              "      <td>8524.370502</td>\n",
              "      <td>45.381316</td>\n",
              "      <td>36.262628</td>\n",
              "      <td>0.096614</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58d5785b-cd98-408b-8a22-62033ca1ba85')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-58d5785b-cd98-408b-8a22-62033ca1ba85 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-58d5785b-cd98-408b-8a22-62033ca1ba85');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f391a725-f145-45d5-8424-460294c2fe49\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f391a725-f145-45d5-8424-460294c2fe49')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f391a725-f145-45d5-8424-460294c2fe49 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN', 'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS', 'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY', 'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI', 'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL']\n",
            "56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNCK2GcGUGJR",
        "outputId": "f14b41da-5d80-4b1a-8af0-bd7093726118"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(617, 58)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"Class\"\n",
        "random_state = 13\n",
        "n_splits = 10\n",
        "n_repeats = 10\n",
        "num_boost_round = 1000\n",
        "stopping_rounds = 50\n",
        "learning_rate = 0.1\n",
        "model_name = \"LightGBM\""
      ],
      "metadata": {
        "id": "-5wXH1Ur9Szh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# コンペの評価指標\n",
        "def balance_logloss(y_true, y_pred):\n",
        "    y_pred = np.stack([1-y_pred, y_pred]).T\n",
        "    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n",
        "    #y_pred / np.sum(y_pred, axis=1)[:, None]\n",
        "    nc = np.bincount(y_true)\n",
        "    logloss = (-1/nc[0]*(np.sum(np.where(y_true==0,1,0) * np.log(y_pred[:,0]))) - 1/nc[1]*(np.sum(np.where(y_true!=0,1,0) * np.log(y_pred[:,1])))) / 2\n",
        "    return logloss\n",
        "\n",
        "metric = balance_logloss"
      ],
      "metadata": {
        "id": "UKgQFPis9gm-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearnの関数に自作評価関数を入力したい場合は、make_scorerで変換をしたものが必要になる\n",
        "# https://scikit-learn.org/stable/modules/model_evaluation.html#defining-your-scoring-strategy-from-metric-functions\n",
        "scoring = make_scorer(metric)"
      ],
      "metadata": {
        "id": "E37biHMWBB1j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 探索したハイパラ\n",
        "params = {'objective': 'binary', 'metric': 'binary_logloss', 'random_state': 13, 'verbose': -1, 'learning_rate': 0.1, 'max_depth': 6,\n",
        "          'num_leaves': 130, 'min_data_in_leaf': 23, 'min_gain_to_split': 0.4770000759757845, 'max_bin': 456,\n",
        "          'subsample': 0.7730309844043276, 'subsample_freq': 2, 'feature_fraction': 0.7135319817465602,\n",
        "          'scale_pos_weight': 57.4385890562176}\n",
        "\n",
        "FEATURES = df_train[ex_columns].columns.tolist()\n",
        "df_importance = pd.DataFrame(index=FEATURES)\n",
        "df_preds = df_sub[[\"Id\"]].copy()\n",
        "scores = []\n",
        "pfi_scores = []\n",
        "\n",
        "y = df_train[target]\n",
        "X = df_train[ex_columns]\n",
        "X_sub = df_test[ex_columns]\n",
        "oof = np.zeros(len(y))\n",
        "cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
        "\n",
        "for i, (train_index, val_index) in enumerate(cv.split(X, y)):\n",
        "    print(\"=\" * 20, f\"START FOLD {i+1}\", \"=\" * 20)\n",
        "    y_train = y.iloc[train_index].to_numpy()\n",
        "    X_train = X.iloc[train_index].to_numpy()\n",
        "    y_val = y.iloc[val_index].to_numpy()\n",
        "    X_val = X.iloc[val_index].to_numpy()\n",
        "\n",
        "    data_train = lgb.Dataset(X_train, y_train)\n",
        "    data_val = lgb.Dataset(X_val, y_val)\n",
        "    evals_result = {}\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_set=data_train,\n",
        "        valid_sets=[data_train, data_val],\n",
        "        num_boost_round=num_boost_round,\n",
        "        #categorical_feature=cat_columns,\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n",
        "            lgb.record_evaluation(evals_result),\n",
        "        ],\n",
        "    )\n",
        "    # 特徴量重要度\n",
        "    df_importance[f\"FOLD_{i+1}\"] = model.feature_importance(importance_type=\"gain\")\n",
        "    # 検証データ\n",
        "    y_pred = model.predict(X_val)\n",
        "    score = metric(y_val, y_pred)\n",
        "    scores.append(score)\n",
        "    oof[val_index] += y_pred / n_repeats\n",
        "    # 提出用データ\n",
        "    pred = model.predict(X_sub)\n",
        "    df_preds[f\"pred_{i+1}\"] = pred\n",
        "\n",
        "    # PFIで重要度計算\n",
        "    # (シャッフル前の精度 - シャッフル後の精度)が計算される、誤差系は重要な特徴量はマイナスに大きくなるはず\n",
        "    # https://github.com/scikit-learn/scikit-learn/blob/7f9bad99d/sklearn/inspection/_permutation_importance.py#L101\n",
        "    pfi = permutation_importance(\n",
        "        estimator=model,\n",
        "        X=X_val,\n",
        "        y=y_val,\n",
        "        scoring=scoring,\n",
        "        n_repeats=5,\n",
        "        n_jobs=-1,\n",
        "        random_state=random_state)\n",
        "    pfi_scores.append(pfi[\"importances_mean\"].tolist())# 結果を入れる"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cug7LpHn9qFh",
        "outputId": "136b855f-bff0-4be9-fc32-84c3f82933dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================== START FOLD 1 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[109]\ttraining's binary_logloss: 0.0127094\tvalid_1's binary_logloss: 0.269505\n",
            "==================== START FOLD 2 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[74]\ttraining's binary_logloss: 0.0298523\tvalid_1's binary_logloss: 0.155846\n",
            "==================== START FOLD 3 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[159]\ttraining's binary_logloss: 0.00871894\tvalid_1's binary_logloss: 0.0591302\n",
            "==================== START FOLD 4 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[111]\ttraining's binary_logloss: 0.0124873\tvalid_1's binary_logloss: 0.111548\n",
            "==================== START FOLD 5 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[82]\ttraining's binary_logloss: 0.0258102\tvalid_1's binary_logloss: 0.284431\n",
            "==================== START FOLD 6 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[101]\ttraining's binary_logloss: 0.0141258\tvalid_1's binary_logloss: 0.209451\n",
            "==================== START FOLD 7 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[153]\ttraining's binary_logloss: 0.00952167\tvalid_1's binary_logloss: 0.0355751\n",
            "==================== START FOLD 8 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[132]\ttraining's binary_logloss: 0.0106145\tvalid_1's binary_logloss: 0.0360082\n",
            "==================== START FOLD 9 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.129387\tvalid_1's binary_logloss: 0.355256\n",
            "==================== START FOLD 10 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[134]\ttraining's binary_logloss: 0.0108879\tvalid_1's binary_logloss: 0.0810224\n",
            "==================== START FOLD 11 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[97]\ttraining's binary_logloss: 0.0146511\tvalid_1's binary_logloss: 0.237298\n",
            "==================== START FOLD 12 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[147]\ttraining's binary_logloss: 0.0100922\tvalid_1's binary_logloss: 0.139941\n",
            "==================== START FOLD 13 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[76]\ttraining's binary_logloss: 0.0305382\tvalid_1's binary_logloss: 0.163223\n",
            "==================== START FOLD 14 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[140]\ttraining's binary_logloss: 0.0103623\tvalid_1's binary_logloss: 0.187252\n",
            "==================== START FOLD 15 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[113]\ttraining's binary_logloss: 0.0120013\tvalid_1's binary_logloss: 0.150003\n",
            "==================== START FOLD 16 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[69]\ttraining's binary_logloss: 0.0395536\tvalid_1's binary_logloss: 0.160846\n",
            "==================== START FOLD 17 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[92]\ttraining's binary_logloss: 0.0188554\tvalid_1's binary_logloss: 0.128968\n",
            "==================== START FOLD 18 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's binary_logloss: 0.0491855\tvalid_1's binary_logloss: 0.146598\n",
            "==================== START FOLD 19 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[90]\ttraining's binary_logloss: 0.0198354\tvalid_1's binary_logloss: 0.21578\n",
            "==================== START FOLD 20 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[132]\ttraining's binary_logloss: 0.0109455\tvalid_1's binary_logloss: 0.0622137\n",
            "==================== START FOLD 21 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[98]\ttraining's binary_logloss: 0.0147844\tvalid_1's binary_logloss: 0.161625\n",
            "==================== START FOLD 22 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[118]\ttraining's binary_logloss: 0.0115236\tvalid_1's binary_logloss: 0.0561964\n",
            "==================== START FOLD 23 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[144]\ttraining's binary_logloss: 0.00990962\tvalid_1's binary_logloss: 0.116738\n",
            "==================== START FOLD 24 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[127]\ttraining's binary_logloss: 0.0110577\tvalid_1's binary_logloss: 0.102951\n",
            "==================== START FOLD 25 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[116]\ttraining's binary_logloss: 0.0117994\tvalid_1's binary_logloss: 0.145454\n",
            "==================== START FOLD 26 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[153]\ttraining's binary_logloss: 0.00934012\tvalid_1's binary_logloss: 0.100177\n",
            "==================== START FOLD 27 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[90]\ttraining's binary_logloss: 0.0186527\tvalid_1's binary_logloss: 0.205223\n",
            "==================== START FOLD 28 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[81]\ttraining's binary_logloss: 0.0257777\tvalid_1's binary_logloss: 0.197155\n",
            "==================== START FOLD 29 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's binary_logloss: 0.0952641\tvalid_1's binary_logloss: 0.287463\n",
            "==================== START FOLD 30 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[100]\ttraining's binary_logloss: 0.0159118\tvalid_1's binary_logloss: 0.223148\n",
            "==================== START FOLD 31 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[82]\ttraining's binary_logloss: 0.0244146\tvalid_1's binary_logloss: 0.300037\n",
            "==================== START FOLD 32 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[104]\ttraining's binary_logloss: 0.0132561\tvalid_1's binary_logloss: 0.223086\n",
            "==================== START FOLD 33 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[131]\ttraining's binary_logloss: 0.0107311\tvalid_1's binary_logloss: 0.160379\n",
            "==================== START FOLD 34 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[163]\ttraining's binary_logloss: 0.0096659\tvalid_1's binary_logloss: 0.0454378\n",
            "==================== START FOLD 35 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's binary_logloss: 0.0920782\tvalid_1's binary_logloss: 0.178472\n",
            "==================== START FOLD 36 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[155]\ttraining's binary_logloss: 0.00948732\tvalid_1's binary_logloss: 0.167386\n",
            "==================== START FOLD 37 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[81]\ttraining's binary_logloss: 0.0266553\tvalid_1's binary_logloss: 0.10354\n",
            "==================== START FOLD 38 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[87]\ttraining's binary_logloss: 0.023013\tvalid_1's binary_logloss: 0.0610404\n",
            "==================== START FOLD 39 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[72]\ttraining's binary_logloss: 0.0363585\tvalid_1's binary_logloss: 0.205693\n",
            "==================== START FOLD 40 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[150]\ttraining's binary_logloss: 0.00941458\tvalid_1's binary_logloss: 0.208957\n",
            "==================== START FOLD 41 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[116]\ttraining's binary_logloss: 0.0117813\tvalid_1's binary_logloss: 0.184239\n",
            "==================== START FOLD 42 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[68]\ttraining's binary_logloss: 0.0446067\tvalid_1's binary_logloss: 0.139926\n",
            "==================== START FOLD 43 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[108]\ttraining's binary_logloss: 0.0128097\tvalid_1's binary_logloss: 0.158227\n",
            "==================== START FOLD 44 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[93]\ttraining's binary_logloss: 0.0189311\tvalid_1's binary_logloss: 0.165049\n",
            "==================== START FOLD 45 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[81]\ttraining's binary_logloss: 0.0222992\tvalid_1's binary_logloss: 0.257666\n",
            "==================== START FOLD 46 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[88]\ttraining's binary_logloss: 0.0188205\tvalid_1's binary_logloss: 0.142134\n",
            "==================== START FOLD 47 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[135]\ttraining's binary_logloss: 0.00974241\tvalid_1's binary_logloss: 0.116904\n",
            "==================== START FOLD 48 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[83]\ttraining's binary_logloss: 0.0244718\tvalid_1's binary_logloss: 0.156711\n",
            "==================== START FOLD 49 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[104]\ttraining's binary_logloss: 0.0133144\tvalid_1's binary_logloss: 0.143903\n",
            "==================== START FOLD 50 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[144]\ttraining's binary_logloss: 0.00993827\tvalid_1's binary_logloss: 0.145073\n",
            "==================== START FOLD 51 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[68]\ttraining's binary_logloss: 0.0413549\tvalid_1's binary_logloss: 0.286538\n",
            "==================== START FOLD 52 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[102]\ttraining's binary_logloss: 0.0153378\tvalid_1's binary_logloss: 0.114973\n",
            "==================== START FOLD 53 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[84]\ttraining's binary_logloss: 0.0228623\tvalid_1's binary_logloss: 0.102393\n",
            "==================== START FOLD 54 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[157]\ttraining's binary_logloss: 0.00876668\tvalid_1's binary_logloss: 0.20024\n",
            "==================== START FOLD 55 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[121]\ttraining's binary_logloss: 0.0117441\tvalid_1's binary_logloss: 0.06019\n",
            "==================== START FOLD 56 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[99]\ttraining's binary_logloss: 0.0152351\tvalid_1's binary_logloss: 0.303308\n",
            "==================== START FOLD 57 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[129]\ttraining's binary_logloss: 0.0111204\tvalid_1's binary_logloss: 0.0727373\n",
            "==================== START FOLD 58 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[127]\ttraining's binary_logloss: 0.0107425\tvalid_1's binary_logloss: 0.0876918\n",
            "==================== START FOLD 59 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[92]\ttraining's binary_logloss: 0.0183513\tvalid_1's binary_logloss: 0.236547\n",
            "==================== START FOLD 60 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[133]\ttraining's binary_logloss: 0.0103045\tvalid_1's binary_logloss: 0.113691\n",
            "==================== START FOLD 61 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[82]\ttraining's binary_logloss: 0.0239165\tvalid_1's binary_logloss: 0.190708\n",
            "==================== START FOLD 62 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[138]\ttraining's binary_logloss: 0.0103532\tvalid_1's binary_logloss: 0.145612\n",
            "==================== START FOLD 63 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[135]\ttraining's binary_logloss: 0.0103309\tvalid_1's binary_logloss: 0.146906\n",
            "==================== START FOLD 64 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[106]\ttraining's binary_logloss: 0.0122289\tvalid_1's binary_logloss: 0.172586\n",
            "==================== START FOLD 65 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[141]\ttraining's binary_logloss: 0.00981169\tvalid_1's binary_logloss: 0.131051\n",
            "==================== START FOLD 66 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[145]\ttraining's binary_logloss: 0.00995998\tvalid_1's binary_logloss: 0.118423\n",
            "==================== START FOLD 67 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[122]\ttraining's binary_logloss: 0.0113301\tvalid_1's binary_logloss: 0.127505\n",
            "==================== START FOLD 68 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[25]\ttraining's binary_logloss: 0.189956\tvalid_1's binary_logloss: 0.308336\n",
            "==================== START FOLD 69 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[130]\ttraining's binary_logloss: 0.0105986\tvalid_1's binary_logloss: 0.126737\n",
            "==================== START FOLD 70 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[129]\ttraining's binary_logloss: 0.0110842\tvalid_1's binary_logloss: 0.121894\n",
            "==================== START FOLD 71 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[103]\ttraining's binary_logloss: 0.0146718\tvalid_1's binary_logloss: 0.211949\n",
            "==================== START FOLD 72 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[79]\ttraining's binary_logloss: 0.0242778\tvalid_1's binary_logloss: 0.293465\n",
            "==================== START FOLD 73 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[141]\ttraining's binary_logloss: 0.0100282\tvalid_1's binary_logloss: 0.155365\n",
            "==================== START FOLD 74 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[122]\ttraining's binary_logloss: 0.0119117\tvalid_1's binary_logloss: 0.152625\n",
            "==================== START FOLD 75 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[145]\ttraining's binary_logloss: 0.00975872\tvalid_1's binary_logloss: 0.21574\n",
            "==================== START FOLD 76 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[71]\ttraining's binary_logloss: 0.0384527\tvalid_1's binary_logloss: 0.0973447\n",
            "==================== START FOLD 77 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[124]\ttraining's binary_logloss: 0.0126673\tvalid_1's binary_logloss: 0.111323\n",
            "==================== START FOLD 78 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[107]\ttraining's binary_logloss: 0.0124733\tvalid_1's binary_logloss: 0.141248\n",
            "==================== START FOLD 79 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[145]\ttraining's binary_logloss: 0.00991408\tvalid_1's binary_logloss: 0.105861\n",
            "==================== START FOLD 80 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[112]\ttraining's binary_logloss: 0.0133804\tvalid_1's binary_logloss: 0.135632\n",
            "==================== START FOLD 81 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[157]\ttraining's binary_logloss: 0.00886349\tvalid_1's binary_logloss: 0.0513723\n",
            "==================== START FOLD 82 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[89]\ttraining's binary_logloss: 0.0192302\tvalid_1's binary_logloss: 0.169049\n",
            "==================== START FOLD 83 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[162]\ttraining's binary_logloss: 0.00893479\tvalid_1's binary_logloss: 0.0377803\n",
            "==================== START FOLD 84 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[136]\ttraining's binary_logloss: 0.00977294\tvalid_1's binary_logloss: 0.157314\n",
            "==================== START FOLD 85 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[126]\ttraining's binary_logloss: 0.0109045\tvalid_1's binary_logloss: 0.139954\n",
            "==================== START FOLD 86 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[83]\ttraining's binary_logloss: 0.0215925\tvalid_1's binary_logloss: 0.205752\n",
            "==================== START FOLD 87 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[134]\ttraining's binary_logloss: 0.0101596\tvalid_1's binary_logloss: 0.0912076\n",
            "==================== START FOLD 88 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[28]\ttraining's binary_logloss: 0.173632\tvalid_1's binary_logloss: 0.285755\n",
            "==================== START FOLD 89 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[130]\ttraining's binary_logloss: 0.00996377\tvalid_1's binary_logloss: 0.118013\n",
            "==================== START FOLD 90 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[102]\ttraining's binary_logloss: 0.0147018\tvalid_1's binary_logloss: 0.183115\n",
            "==================== START FOLD 91 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[100]\ttraining's binary_logloss: 0.0160892\tvalid_1's binary_logloss: 0.127257\n",
            "==================== START FOLD 92 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[137]\ttraining's binary_logloss: 0.0101575\tvalid_1's binary_logloss: 0.123396\n",
            "==================== START FOLD 93 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[90]\ttraining's binary_logloss: 0.0188497\tvalid_1's binary_logloss: 0.0742034\n",
            "==================== START FOLD 94 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[137]\ttraining's binary_logloss: 0.00961453\tvalid_1's binary_logloss: 0.169205\n",
            "==================== START FOLD 95 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[100]\ttraining's binary_logloss: 0.0156157\tvalid_1's binary_logloss: 0.129373\n",
            "==================== START FOLD 96 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[109]\ttraining's binary_logloss: 0.0126542\tvalid_1's binary_logloss: 0.15211\n",
            "==================== START FOLD 97 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's binary_logloss: 0.0501469\tvalid_1's binary_logloss: 0.220483\n",
            "==================== START FOLD 98 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[102]\ttraining's binary_logloss: 0.0151252\tvalid_1's binary_logloss: 0.112581\n",
            "==================== START FOLD 99 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[90]\ttraining's binary_logloss: 0.0182938\tvalid_1's binary_logloss: 0.239882\n",
            "==================== START FOLD 100 ====================\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[17]\ttraining's binary_logloss: 0.203387\tvalid_1's binary_logloss: 0.322203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-6cbbfb0ec40f>:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_preds[f\"pred_{i+1}\"] = pred\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Score mean: {np.mean(scores)}, Fold scores: {scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn4LHSTzBLJu",
        "outputId": "72fecb09-8cef-4a23-fe9f-5d0750af5ce1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score mean: 0.19769865368784567, Fold scores: [0.47178537908005186, 0.12302595509309466, 0.04319966800892261, 0.23233083799986365, 0.2894890916532623, 0.18520558183464997, 0.03419099547381641, 0.03834369332227508, 0.4487460686136461, 0.09708504610903518, 0.47146551851580737, 0.10813739524534809, 0.11487663405587975, 0.13785231950679436, 0.24268526601294288, 0.2656283518075087, 0.16837153949360925, 0.24947304359892417, 0.148485981086041, 0.0684509582087538, 0.1982814312286032, 0.1054346123296667, 0.1138677549383448, 0.1369350354923281, 0.18685922564710794, 0.13028163536787452, 0.4059744969074282, 0.20516878032035438, 0.29077641381422303, 0.2276373784571911, 0.48931780840127603, 0.18458606122763088, 0.17907980970630658, 0.06570505114402397, 0.18613342925779408, 0.21074496963788147, 0.13146244933597614, 0.07456012440862424, 0.37278621046277544, 0.14621325636421761, 0.18501190722193855, 0.18610182093838362, 0.13157528865215307, 0.3220678828701062, 0.4624644215038708, 0.1742556031623915, 0.145258105268612, 0.12704268645859013, 0.1518385312784052, 0.1293726386551255, 0.4409223079207901, 0.13185786583577297, 0.12557913276727029, 0.2582013558492675, 0.11638732433297104, 0.20527480720164634, 0.13818105556240634, 0.1481717745395832, 0.3388169951882168, 0.09265287446161136, 0.14287657438294382, 0.20481971139713448, 0.1870935047964218, 0.3887087978466397, 0.18585163832623247, 0.14520822867501806, 0.17298463309965867, 0.40414468097819833, 0.1497180954061235, 0.12343067418107324, 0.15829056449699502, 0.6020514691261587, 0.1443845220641533, 0.1080470618791251, 0.17826368702191128, 0.1795487900539067, 0.08104698570577866, 0.13892719006098422, 0.14898558837532805, 0.1926915633980576, 0.08886232480380626, 0.17613990099247337, 0.03697324570976147, 0.1618216856878411, 0.1329355521859015, 0.3865927779486122, 0.11090596800375994, 0.3845827985624048, 0.12157163773812948, 0.16896829852183878, 0.10125344947217912, 0.10532981495760485, 0.09893380209638339, 0.19281628349449598, 0.1367574095196054, 0.15747834417282425, 0.3045575097962273, 0.10020448761423481, 0.3167048385713964, 0.45773163882427736]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = pd.DataFrame(pfi_scores, columns=FEATURES).mean()*-1\n",
        "# (シャッフル後のlogloss - シャッフル前のlogloss)\n",
        "fig, ax = plt.subplots(figsize=(14, 5))\n",
        "sns.barplot(x=d.index, y=d.to_numpy(), ax=ax)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "b2j0VIoRBbMm",
        "outputId": "0f02f991-c91c-4d8f-bf68-8964020f4760"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d.sort_values(ascending=False).head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcBqFVl9CDsJ",
        "outputId": "56394e02-9bb7-4b26-e385-29f4a7f7ae21"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DU    0.274526\n",
              "BQ    0.150076\n",
              "AB    0.055261\n",
              "GL    0.027660\n",
              "CR    0.027451\n",
              "CC    0.023404\n",
              "DN    0.018110\n",
              "FI    0.017050\n",
              "DL    0.016516\n",
              "EB    0.015567\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d.sort_values(ascending=False).tail(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-WSeH2hlkAi",
        "outputId": "5d864728-be6c-4e82-c637-31ecd3ad77e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EG    -0.000199\n",
              "DF    -0.000328\n",
              "FC    -0.000346\n",
              "CW    -0.000397\n",
              "FE    -0.000619\n",
              "CS    -0.000654\n",
              "GB    -0.000703\n",
              "AY    -0.000718\n",
              "AZ    -0.000839\n",
              "GF    -0.002432\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gu4VqSoJCVKJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}